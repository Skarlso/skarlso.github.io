<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Kubernetes distributed application deployment with sample Face Recognition App | Ramblings of a cloud engineer</title>
<meta name="keywords" content="">
<meta name="description" content="Intro
Alright folks. Settle in and get comfortable. This is going to be a long, but hopefully, fun ride.
I&rsquo;m going to deploy a distributed application with Kubernetes. I attempted to create an application that I thought resembled a real world app. Obviously I had to cut some corners due to time and energy constraints.
My focus will be on Kubernetes and deployment.
Shall we delve right in?
The Application
TL;DR
">
<meta name="author" content="hannibal">
<link rel="canonical" href="https://skarlso.github.io/2018/03/15/kubernetes-distributed-application/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://skarlso.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://skarlso.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://skarlso.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://skarlso.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://skarlso.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://skarlso.github.io/2018/03/15/kubernetes-distributed-application/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Kubernetes distributed application deployment with sample Face Recognition App">
<meta property="og:description" content="Intro
Alright folks. Settle in and get comfortable. This is going to be a long, but hopefully, fun ride.
I&rsquo;m going to deploy a distributed application with Kubernetes. I attempted to create an application that I thought resembled a real world app. Obviously I had to cut some corners due to time and energy constraints.
My focus will be on Kubernetes and deployment.
Shall we delve right in?
The Application
TL;DR
">
<meta property="og:type" content="article">
<meta property="og:url" content="https://skarlso.github.io/2018/03/15/kubernetes-distributed-application/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2018-03-15T23:01:00+01:00">
<meta property="article:modified_time" content="2018-03-15T23:01:00+01:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kubernetes distributed application deployment with sample Face Recognition App">
<meta name="twitter:description" content="Intro
Alright folks. Settle in and get comfortable. This is going to be a long, but hopefully, fun ride.
I&rsquo;m going to deploy a distributed application with Kubernetes. I attempted to create an application that I thought resembled a real world app. Obviously I had to cut some corners due to time and energy constraints.
My focus will be on Kubernetes and deployment.
Shall we delve right in?
The Application
TL;DR
">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://skarlso.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Kubernetes distributed application deployment with sample Face Recognition App",
      "item": "https://skarlso.github.io/2018/03/15/kubernetes-distributed-application/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Kubernetes distributed application deployment with sample Face Recognition App",
  "name": "Kubernetes distributed application deployment with sample Face Recognition App",
  "description": "Intro Alright folks. Settle in and get comfortable. This is going to be a long, but hopefully, fun ride.\nI\u0026rsquo;m going to deploy a distributed application with Kubernetes. I attempted to create an application that I thought resembled a real world app. Obviously I had to cut some corners due to time and energy constraints.\nMy focus will be on Kubernetes and deployment.\nShall we delve right in?\nThe Application TL;DR ",
  "keywords": [
    
  ],
  "articleBody": "Intro Alright folks. Settle in and get comfortable. This is going to be a long, but hopefully, fun ride.\nI’m going to deploy a distributed application with Kubernetes. I attempted to create an application that I thought resembled a real world app. Obviously I had to cut some corners due to time and energy constraints.\nMy focus will be on Kubernetes and deployment.\nShall we delve right in?\nThe Application TL;DR The application itself consists of six parts. The repository can be found here: Kube Cluster Sample.\nIt’s a face recognition service which identifies images of people, comparing them to known individuals. A simple frontend displays a table of these images whom they belong to. This happens by sending a request to a receiver. The request contains a path to an image. This image can sit on an NFS somewhere. The receiver stores this path in the DB (MySQL) and sends a processing request to a queue. The queue uses: NSQ. The request contains the ID of the saved image.\nAn Image Processing service is constantly monitoring the queue for jobs to do. The processing consists of the following steps: taking the ID; loading the image; and finally, sending the image to a face recognition backend written in Python via gRPC. If the identification is successful, the backend will return the name of the image corresponding to that person. The image_processor then updates the image’s record with the person’s ID and marks the image as “processed successfully”. If identification is unsuccessful, the image will be left as “pending”. If there was a failure during identification, the image will be flagged as “failed”.\nFailed images can be retried with a cron job, for example:\nSo how does this all work? Let’s check it out .\nReceiver The receiver service is the starting point of the process. It’s an API which receives a request in the following format:\ncurl -d '{\"path\":\"/unknown_images/unknown0001.jpg\"}' http://127.0.0.1:8000/image/post In this instance, the receiver stores the path using a shared database cluster. The entity will then receive an ID from the database service. This application is based on the model where unique identification for Entity Objects is provided by the persistence layer. Once the ID is procured, the receiver will send a message to NSQ. At this point in the process, the receiver’s job is done.\nImage Processor Here is where the excitement begins. When Image Processor first runs it creates two Go routines. These are…\nConsume This is an NSQ consumer. It has three integral jobs. Firstly, it listens for messages on the queue. Secondly, when there is a message, it appends the received ID to a thread safe slice of IDs that the second routine processes. And lastly, it signals the second routine that there is work to be do. It does this through sync.Condition.\nProcessImages This routine processes a slice of IDs until the slice is drained completely. Once the slice is drained, the routine suspends instead of sleep-waiting on a channel. The processing of a single ID can be seen in the following linear steps:\nEstablish a gRPC connection to the Face Recognition service (explained under Face Recognition) Retrieve the image record from the database Setup two functions for the Circuit Breaker Function 1: The main function which runs the RPC method call Function 2: A health check for the Ping of the circuit breaker Call Function 1 which sends the path of the image to the face recognition service. This path should be accessible by the face recognition service. Preferably something shared like an NFS If this call fails, update the image record as FAILED PROCESSING If it succeeds, an image name should come back which corresponds to a person in the db. It runs a joined SQL query which gets the corresponding person’s ID Update the Image record in the database with PROCESSED status and the ID of the person that image was identified as This service can be replicated. In other words, more than one can run at the same time.\nCircuit Breaker A system in which replicating resources requires little to no effort, there still can be cases where, for example, the network goes down, or there are communication problems of any kind between two services. I like to implement a little circuit breaker around the gRPC calls for fun.\nThis is how it works:\nAs you can see, once there are 5 unsuccessful calls to the service, the circuit breaker activates, not allowing any more calls to go through. After a configured amount of time, it will send a Ping call to the service to see if it’s back up. If that still errors out, it will increase the timeout. If not, it opens the circuit, allowing traffic to proceed.\nFront-End This is only a simple table view with Go’s own html/template used to render a list of images.\nFace Recognition Here is where the identification magic happens. I decided to make this a gRPC based service for the sole purpose of its flexibility. I started writing it in Go but decided that a Python implementation would be much sorter. In fact, excluding the gRPC code, the recognition part is approximately 7 lines of Python code. I’m using this fantastic library which contains all the C bindings to OpenCV. Face Recognition. Having an API contract here means that I can change the implementation anytime as long as it adheres to the contract.\nPlease note that there exist a great Go library OpenCV. I was about to use it but they had yet to write the C bindings for that part of OpenCV. It’s called GoCV. Check them out! They have some pretty amazing things, like real-time camera feed processing that only needs a couple of lines of code.\nThe python library is simple in nature. Have a set of images of people you know. I have a folder with a couple of images named, hannibal_1.jpg, hannibal_2.jpg, gergely_1.jpg, john_doe.jpg. In the database I have two tables named, person, person_images. They look like this:\n+----+----------+ | id | name | +----+----------+ | 1 | Gergely | | 2 | John Doe | | 3 | Hannibal | +----+----------+ +----+----------------+-----------+ | id | image_name | person_id | +----+----------------+-----------+ | 1 | hannibal_1.jpg | 3 | | 2 | hannibal_2.jpg | 3 | +----+----------------+-----------+ The face recognition library returns the name of the image from the known people which matches the person on the unknown image. After that, a simple joined query -like this- will return the person in question.\nselect person.name, person.id from person inner join person_images as pi on person.id = pi.person_id where image_name = 'hannibal_2.jpg'; The gRPC call returns the ID of the person which is then used to update the image’s ‘person` column.\nNSQ NSQ is a nice little Go based queue. It can be scaled and has a minimal footprint on the system. It also has a lookup service that consumers use to receive messages, and a daemon that senders use when sending messages.\nNSQ’s philosophy is that the daemon should run with the sender application. That way, the sender will send to the localhost only. But the daemon is connected to the lookup service, and that’s how they achieve a global queue.\nThis means that there are as many NSQ daemons deployed as there are senders. Because the daemon has a minuscule resource requirement, it won’t interfere with the requirements of the main application.\nConfiguration In order to be as flexible as possible, as well as making use of Kubernetes’s ConfigSet, I’m using .env files in development to store configurations like the location of the database service, or NSQ’s lookup address. In production- and that means the Kubernetes’s environment- I’ll use environment properties.\nConclusion for the Application And that’s all there is to the architecture of the application we are about to deploy. All of its components are changeable and coupled only through the database, a queue and gRPC. This is imperative when deploying a distributed application due to how updating mechanics work. I will cover that part in the Deployment section.\nDeployment with Kubernetes Basics What is Kubernetes?\nI’m going to cover some of the basics here. I won’t go too much into detail- that would require a whole book like this one: Kubernetes Up And Running. Also, if you’re daring enough, you can have a look through this documentation: Kubernetes Documentation.\nKubernetes is a containerized service and application manager. It scales easily, employs a swarm of containers, and most importantly, it’s highly configurable via yaml based template files. People often compare Kubernetes to Docker swarm, but Kubernetes does way more than that! For example: it’s container agnostic. You could use LXC with Kubernetes and it would work the same way as you using it with Docker. It provides a layer above managing a cluster of deployed services and applications. How? Let’s take a quick look at the building blocks of Kubernetes.\nIn Kubernetes, you’ll describe a desired state of the application and Kubernetes will do what it can to reach that state. States could be something such as deployed; paused; replicated twice; and so on and so forth.\nOne of the basics of Kubernetes is that it uses Labels and Annotations for all of its components. Services, Deployments, ReplicaSets, DaemonSets, everything is labelled. Consider the following scenario. In order to identify what pod belongs to what application, a label is used called app: myapp. Let’s assume you have two containers of this application deployed; if you would remove the label app from one of the containers, Kubernetes would only detect one and thus would launch a new instance of myapp.\nKubernetes Cluster For Kuberenetes to work, a Kubernetes cluster needs to be present. Setting that up might be a tad painful, but luckily, help is on hand. Minikube sets up a cluster for us locally with one Node. And AWS has a beta service running in the form of a Kubernetes cluster in which the only thing you need to do is request nodes and define your deployments. The Kubernetes cluster components are documented here: Kubernetes Cluster Components.\nNodes A Node is a worker machine. It can be anything- from a vm to a physical machine- including all sorts of cloud provided vms.\nPods Pods are a logically grouped collection of containers, meaning one Pod can potentially house a multitude of containers. A Pod gets its own DNS and virtual IP address after it has been created so Kubernetes can load balancer traffic to it. You rarely need to deal with containers directly. Even when debugging, (like looking at logs), you usually invoke kubectl logs deployment/your-app -f instead of looking at a specific container. Although it is possible with -c container_name. The -f does a tail on the log.\nDeployments When creating any kind of resource in Kubernetes, it will use a Deployment in the background. A deployment describes a desired state of the current application. It’s an object you can use to update Pods or a Service to be in a different state, do an update, or rollout new version of your app. You don’t directly control a ReplicaSet, (as described later), but control the deployment object which creates and manages a ReplicaSet.\nServices By default a Pod will get an IP address. However, since Pods are a volatile thing in Kubernetes, you’ll need something more permanent. A queue, mysql, or an internal API, a frontend; these need to be long running and behind a static, unchanging IP or preferably a DNS record.\nFor this purpose, Kubernetes has Services for which you can define modes of accessibility. Load Balanced, simple IP or internal DNS.\nHow does Kubernetes know if a service is running correctly? You can configure Health Checks and Availability Checks. A Health Check will check whether a container is running, but that doesn’t mean that your service is running. For that, you have the availability check which pings a different endpoint in your application.\nSince Services are pretty important, I recommend that you read up on them later here: Services. Advanced warning though, this document is quite dense. Twenty four A4 pages of networking, services and discovery. It’s also vital to decide whether you want to seriously employ Kubernetes in production.\nDNS / Service Discovery If you create a service in the cluster, that service will get a DNS record in Kubernetes provided by special Kubernetes deployments called kube-proxy and kube-dns. These two provide service discover inside a cluster. If you have a mysql service running and set clusterIP: none, then everyone in the cluster can reach that service by pinging mysql.default.svc.cluster.local. Where:\nmysql – is the name of the service default – is the namespace name svc – is services cluster.local – is a local cluster domain The domain can be changed via a custom definition. To access a service outside the cluster, a DNS provider has to be used, and Nginx (for example), to bind an IP address to a record. The public IP address of a service can be queried with the following commands:\nNodePort – kubectl get -o jsonpath=\"{.spec.ports[0].nodePort}\" services mysql LoadBalancer – kubectl get -o jsonpath=\"{.spec.ports[0].LoadBalancer}\" services mysql Template Files Like Docker Compose, TerraForm or other service management tools, Kubernetes also provides infrastructure describing templates. What that means is that you rarely need to do anything by hand.\nFor example, consider the following yaml template which describes an nginx Deployment:\napiVersion: apps/v1 kind: Deployment #(1) metadata: #(2) name: nginx-deployment labels: #(3) app: nginx spec: #(4) replicas: 3 #(5) selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: #(6) - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 This is a simple deployment in which we do the following:\n(1) Define the type of the template with kind (2) Add metadata that will identify this deployment and every resource that it would create with a label (3) (4) Then comes the spec which describes the desired state (5) For the nginx app, have 3 replicas (6) This is the template definition for the containers that this Pod will contain nginx named container nginx:1.7.9 image (docker in this case) exposed ports ReplicaSet A ReplicaSet is a low level replication manager. It ensures that the correct number of replicates are running for a application. However, Deployments are at a higher level and should always manage ReplicaSets. You rarely need to use ReplicaSets directly unless you have a fringe case in which you want to control the specifics of replication.\nDaemonSet Remember how I said Kubernetes is using Labels all the time? A DaemonSet is a controller that ensures that at daemonized application is always running on a node with a certain label.\nFor example: you want all the nodes labelled with logger or mission_critical to run an logger / auditing service daemon. Then you create a DaemonSet and give it a node selector called logger or mission_critical. Kubernetes will look for a node that has that label. Always ensure that it will have an instance of that daemon running on it. Thus everyone running on that node will have access to that daemon locally.\nIn case of my application, the NSQ daemon could be a DaemonSet. Make sure it’s up on a node which has the receiver component running by labelling a node with receiver and specifying a DaemonSet with a receiver application selector.\nThe DaemonSet has all the benefits of the ReplicaSet. It’s scalable and Kubernetes manages it; which means, all life cycle events are handled by Kube ensuring it never dies, and when it does, it will be immediately replaced.\nScaling It’s trivial to scale in Kubernetes. The ReplicaSets take care of the number of instances of a Pod to run- as seen in the nginx deployment with the setting replicas:3. It’s up to us to write our application in a way that allows Kubernetes to run multiple copies of it.\nOf course the settings are vast. You can specify which replicates must run on what Nodes, or on various waiting times as to how long to wait for an instance to come up. You can read more on this subject here: Horizontal Scaling and here: Interactive Scaling with Kubernetes and of course the details of a ReplicaSet which controls all the scaling made possible in Kubernetes.\nConclusion for Kubernetes It’s a convenient tool to handle container orchestration. Its unit of work are Pods and it has a layered architecture. The top level layer is Deployments through which you handle all other resources. It’s highly configurable. It provides an API for all calls you make, so potentially, instead of running kubectl you can also write your own logic to send information to the Kubernetes API.\nIt provides support for all major cloud providers natively by now and it’s completely open source. Feel free to contribute! And check the code if you would like to have a deeper understanding on how it works: Kubernetes on Github.\nMinikube I’m going to use Minikube. Minikube is a local Kubernetes cluster simulator. It’s not great in simulating multiple nodes though, but for starting out and local play without any costs, it’s great. It uses a VM that can be fine tuned if necessary using VirtualBox and the likes.\nAll of the kube template files that I’ll be using can be found here: Kube files.\nNOTE If, later on, you would like to play with scaling but notice that the replicates are always in Pending state, remember that minikube employs a single node only. It might not allow multiple replicas on the same node, or just plainly ran out of resources to use. You can check available resources with the following command:\nkubectl get nodes -o yaml Building the containers Kubernetes supports most of the containers out there. I’m going to use Docker. For all the services I’ve built, there is a Dockerfile included in the repository. I encourage you to study them. Most of them are simple. For the go services, I’m using a multi stage build that has been recently introduced. The Go services are Alpine Linux based. The Face Recognition service is Python. NSQ and MySQL are using their own containers.\nContext Kubernetes uses namespaces. If you don’t specify any, it will use the default namespace. I’m going to permanently set a context to avoid polluting the default namespace. You do that like this:\n❯ kubectl config set-context kube-face-cluster --namespace=face Context \"kube-face-cluster\" created. You have to also start using the context once it’s created, like so:\n❯ kubectl config use-context kube-face-cluster Switched to context \"kube-face-cluster\". After this, all kubectl commands will use the namespace face.\nDeploying the Application Overview of Pods and Services:\nMySQL The first Service I’m going to deploy is my database.\nI’m using the Kubernetes example located here Kube MySQL which fits my needs. Please note that this file is using a plain password for MYSQL_PASSWORD. I’m going to employ a vault as described here Kubernetes Secrets.\nI’ve created a secret locally as described in that document using a secret yaml:\napiVersion: v1 kind: Secret metadata: name: kube-face-secret type: Opaque data: mysql_password: base64codehere I created the base64 code via the following command:\necho -n \"ubersecurepassword\" | base64 And, this is what you’ll see in my deployment yaml file:\n... - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: kube-face-secret key: mysql_password ... Another thing worth mentioning: It’s using a volume to persist the database. The volume definition is as follows:\n... volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql ... volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pv-claim ... presistentVolumeClain is key here. This tells Kubernetes that this resource requires a persistent volume. How it’s provided is abstracted away from the user. You can be sure that Kubernetes will provide a volume that will always be there. It is similar to Pods. To read up on the details, check out this document: Kubernetes Persistent Volumes.\nDeploying the mysql Service is done with the following command:\nkubectl apply -f mysql.yaml apply vs create. In short, apply is considered a declarative object configuration command while create is imperative. What this means for now is that ‘create’ is usually for a one of tasks, like running something or creating a deployment. While, when using apply, the user doesn’t define the action to be taken. That will be defined by Kubernetes based on the current status of the cluster. Thus, when there is no service called mysql and I’m calling apply -f mysql.yaml it will create the service. When running again, Kubernetes won’t do anything. But if I would run create again it will throw an error saying the service is already created.\nFor more information, check out the following docs: Kubernetes Object Management, Imperative Configuration, Declarative Configuration.\nTo see progress information, run:\n# Describes the whole process kubectl describe deployment mysql # Shows only the pod kubectl get pods -l app=mysql Output should be similar to this:\n... Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: NewReplicaSet: mysql-55cd6b9f47 (1/1 replicas created) ... Or in case of get pods:\nNAME READY STATUS RESTARTS AGE mysql-78dbbd9c49-k6sdv 1/1 Running 0 18s To test the instance, run the following snippet:\nkubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -pyourpasswordhere GOTCHA: If you change the password now, it’s not enough to re-apply your yaml file to update the container. Since the DB is persisted, the password will not be changed. You have to delete the whole deployment with kubectl delete -f mysql.yaml.\nYou should see the following when running a show databases.\nIf you don't see a command prompt, try pressing enter. mysql\u003e mysql\u003e mysql\u003e show databases; +--------------------+ | Database | +--------------------+ | information_schema | | kube | | mysql | | performance_schema | +--------------------+ 4 rows in set (0.00 sec) mysql\u003e exit Bye You’ll also notice that I’ve mounted a file located here: Database Setup SQL into the container. MySQL container automatically executes these. That file will bootstrap some data and the schema I’m going to use.\nThe volume definition is as follows:\nvolumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql - name: bootstrap-script mountPath: /docker-entrypoint-initdb.d/database_setup.sql volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pv-claim - name: bootstrap-script hostPath: path: /Users/hannibal/golang/src/github.com/Skarlso/kube-cluster-sample/database_setup.sql type: File To check if the bootstrap script was successful, run this:\n~/golang/src/github.com/Skarlso/kube-cluster-sample/kube_files master* ❯ kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -uroot -pyourpasswordhere kube If you don't see a command prompt, try pressing enter. mysql\u003e show tables; +----------------+ | Tables_in_kube | +----------------+ | images | | person | | person_images | +----------------+ 3 rows in set (0.00 sec) mysql\u003e This concludes the database service setup. Logs for this service can be viewed with the following command:\nkubectl logs deployment/mysql -f NSQ Lookup The NSQ Lookup will run as an internal service. It doesn’t need access from the outside, so I’m setting clusterIP: None which will tell Kubernetes that this service is a headless service. This means that it won’t be load balanced, and it won’t be a single IP service. The DNS will be based upon service selectors.\nOur NSQ Lookup selector is:\nselector: matchLabels: app: nsqlookup Thus, the internal DNS will look like this: nsqlookup.default.svc.cluster.local.\nHeadless services are described in detail here: Headless Service.\nBasically it’s the same as MySQL, just with slight modifications. As stated earlier, I’m using NSQ’s own Docker Image called nsqio/nsq. All nsq commands are there, so nsqd will also use this image just with a different command. For nsqlookupd, the command is:\ncommand: [\"/nsqlookupd\"] args: [\"--broadcast-address=nsqlookup.default.svc.cluster.local\"] What’s the --broadcast-address for, you might ask? By default, nsqlookup will use the hostname as broadcast address. When the consumer runs a callback it will try to connect to something like: http://nsqlookup-234kf-asdf:4161/lookup?topics=image. Please note that nsqlookup-234kf-asdf is the hostname of the container. By setting the broadcast-address to the internal DNS, the callback will be: http://nsqlookup.default.svc.cluster.local:4161/lookup?topic=images. Which will work as expected.\nNSQ Lookup also requires two ports forwarded: One for broadcasting and one for nsqd callback. These are exposed in the Dockerfile, and then utilized in the Kubernetes template. Like this:\nIn the container template:\nports: - containerPort: 4160 hostPort: 4160 - containerPort: 4161 hostPort: 4161 In the service template:\nspec: ports: - name: tcp protocol: TCP port: 4160 targetPort: 4160 - name: http protocol: TCP port: 4161 targetPort: 4161 Names are required by Kubernetes.\nTo create this service, I’m using the same command as before:\nkubectl apply -f nsqlookup.yaml This concludes nsqlookupd. Two of the major players are in the sack!\nReceiver This is a more complex one. The receiver will do three things:\nCreate some deployments; Create the nsq daemon; Expose the service to the public. Deployments The first deployment it creates is its own. The receiver’s container is skarlso/kube-receiver-alpine.\nNsq Daemon The receiver starts an nsq daemon. As stated earlier, the receiver runs an nsqd with it-self. It does this so talking to it can happen locally and not over the network. By making the receiver do this, they will end up on the same node.\nNSQ daemon also needs some adjustments and parameters.\nports: - containerPort: 4150 hostPort: 4150 - containerPort: 4151 hostPort: 4151 env: - name: NSQLOOKUP_ADDRESS value: nsqlookup.default.svc.cluster.local - name: NSQ_BROADCAST_ADDRESS value: nsqd.default.svc.cluster.local command: [\"/nsqd\"] args: [\"--lookupd-tcp-address=$(NSQLOOKUP_ADDRESS):4160\", \"--broadcast-address=$(NSQ_BROADCAST_ADDRESS)\"] You can see that the lookup-tcp-address and the broadcast-address are set. Lookup tcp address is the DNS for the nsqlookupd service. And the broadcast address is necessary, just like with nsqlookupd, so the callbacks are working properly.\nPublic facing Now, this is the first time I’m deploying a public facing service. There are two options. I could use a LoadBalancer since this API will be under heavy load. And if this would be deployed anywhere in production, then it should be using one.\nI’m doing this locally though- with one node- so something called a NodePort is enough. A NodePort exposes a service on each node’s IP at a static port. If not specified, it will assign a random port on the host between 30000-32767. But it can also be configured to be a specific port, using nodePort in the template file. To reach this service, use :. If more than one node is configured, a LoadBalancer can multiplex them to a single IP.\nFor further information, check out this document: Publishing Services.\nPutting this all together, we’ll get a receiver-service for which the template for is as follows:\napiVersion: v1 kind: Service metadata: name: receiver-service spec: ports: - protocol: TCP port: 8000 targetPort: 8000 selector: app: receiver type: NodePort For a fixed nodePort on 8000 a definition of nodePort must be provided:\napiVersion: v1 kind: Service metadata: name: receiver-service spec: ports: - protocol: TCP port: 8000 targetPort: 8000 selector: app: receiver type: NodePort nodePort: 8000 Image processor The Image Processor is where I’m handling passing off images to be identified. It should have access to nsqlookupd, mysql and the gRPC endpoint of the face recognition service. This is actually quite a boring service. In fact, it’s not even a service at all. It doesn’t expose anything, and thus it’s the first deployment only component. For brevity, here is the whole template:\n--- apiVersion: apps/v1 kind: Deployment metadata: name: image-processor-deployment spec: selector: matchLabels: app: image-processor replicas: 1 template: metadata: labels: app: image-processor spec: containers: - name: image-processor image: skarlso/kube-processor-alpine:latest env: - name: MYSQL_CONNECTION value: \"mysql.default.svc.cluster.local\" - name: MYSQL_USERPASSWORD valueFrom: secretKeyRef: name: kube-face-secret key: mysql_userpassword - name: MYSQL_PORT # TIL: If this is 3306 without \" kubectl throws an error. value: \"3306\" - name: MYSQL_DBNAME value: kube - name: NSQ_LOOKUP_ADDRESS value: \"nsqlookup.default.svc.cluster.local:4161\" - name: GRPC_ADDRESS value: \"face-recog.default.svc.cluster.local:50051\" The only interesting points in this file are the multitude of environment properties that are used to configure the application. Note the nsqlookupd address and the grpc address.\nTo create this deployment, run:\nkubectl apply -f image_processor.yaml Face - Recognition The face recognition service does have a service. It’s a simple one. Only needed by image-processor. It’s template is as follows:\napiVersion: v1 kind: Service metadata: name: face-recog spec: ports: - protocol: TCP port: 50051 targetPort: 50051 selector: app: face-recog clusterIP: None The more interesting part is that it requires two volumes. The two volumes are known_people and unknown_people. Can you guess what they will contain? Yep, images. The known_people volume contains all the images associated to the known people in the database. The unknown_people volume will contain all new images. And that’s the path we will need to use when sending images from the receiver; that is wherever the mount point points too, which in my case is /unknown_people. Basically, the path needs to be one that the face recognition service can access.\nNow, with Kubernetes and Docker, this is easy. It can be a mounted S3 or some kind of nfs or a local mount from host to guest. The possibilities are endless (around a dozen or so). I’m going to use a local mount for the sake of simplicity.\nMounting a volume is done in two parts. Firstly, the Dockerfile has to specify volumes:\nVOLUME [ \"/unknown_people\", \"/known_people\" ] Secondly, the Kubernetes template needs add volumeMounts as seen in the MySQL service; the difference being hostPath instead of claimed volume:\nvolumeMounts: - name: known-people-storage mountPath: /known_people - name: unknown-people-storage mountPath: /unknown_people volumes: - name: known-people-storage hostPath: path: /Users/hannibal/Temp/known_people type: Directory - name: unknown-people-storage hostPath: path: /Users/hannibal/Temp/ type: Directory We also need to set the known_people folder config setting for the face recognition service. This is done via an environment property:\nenv: - name: KNOWN_PEOPLE value: \"/known_people\" Then the Python code will look up images, like this:\nknown_people = os.getenv('KNOWN_PEOPLE', 'known_people') print(\"Known people images location is: %s\" % known_people) images = self.image_files_in_folder(known_people) Where image_files_in_folder is:\ndef image_files_in_folder(self, folder): return [os.path.join(folder, f) for f in os.listdir(folder) if re.match(r'.*\\.(jpg|jpeg|png)', f, flags=re.I)] Neat.\nNow, if the receiver receives a request (and sends it off further down the line) similar to the one below…\ncurl -d '{\"path\":\"/unknown_people/unknown220.jpg\"}' http://192.168.99.100:30251/image/post …it will look for an image called unknown220.jpg under /unknown_people, locate an image in the known_folder that corresponds to the person in the unknown image and return the name of the image that matches.\nLooking at logs, you should see something like this:\n# Receiver ❯ curl -d '{\"path\":\"/unknown_people/unknown219.jpg\"}' http://192.168.99.100:30251/image/post got path: {Path:/unknown_people/unknown219.jpg} image saved with id: 4 image sent to nsq # Image Processor 2018/03/26 18:11:21 INF 1 [images/ch] querying nsqlookupd http://nsqlookup.default.svc.cluster.local:4161/lookup?topic=images 2018/03/26 18:11:59 Got a message: 4 2018/03/26 18:11:59 Processing image id: 4 2018/03/26 18:12:00 got person: Hannibal 2018/03/26 18:12:00 updating record with person id 2018/03/26 18:12:00 done And that concludes all of the services that we need to deploy.\nFrontend Lastly, there is a small web-app which displays the information in the db for convenience. This is also a public facing service with the same parameters as the receiver.\nIt looks like this:\nRecap We are now at the point in which I’ve deployed a bunch of services. A recap off the commands I’ve used so far:\nkubectl apply -f mysql.yaml kubectl apply -f nsqlookup.yaml kubectl apply -f receiver.yaml kubectl apply -f image_processor.yaml kubectl apply -f face_recognition.yaml kubectl apply -f frontend.yaml These could be in any order since the application does not allocate connections on start. (Except for image_processor’s NSQ consumer. But that re-tries.)\nQuery-ing kube for running pods with kubectl get pods should show something like this if there were no errors:\n❯ kubectl get pods NAME READY STATUS RESTARTS AGE face-recog-6bf449c6f-qg5tr 1/1 Running 0 1m image-processor-deployment-6467468c9d-cvx6m 1/1 Running 0 31s mysql-7d667c75f4-bwghw 1/1 Running 0 36s nsqd-584954c44c-299dz 1/1 Running 0 26s nsqlookup-7f5bdfcb87-jkdl7 1/1 Running 0 11s receiver-deployment-5cb4797598-sf5ds 1/1 Running 0 26s Running minikube service list:\n❯ minikube service list |-------------|----------------------|-----------------------------| | NAMESPACE | NAME | URL | |-------------|----------------------|-----------------------------| | default | face-recog | No node port | | default | kubernetes | No node port | | default | mysql | No node port | | default | nsqd | No node port | | default | nsqlookup | No node port | | default | receiver-service | http://192.168.99.100:30251 | | kube-system | kube-dns | No node port | | kube-system | kubernetes-dashboard | http://192.168.99.100:30000 | |-------------|----------------------|-----------------------------| Rolling update What happens during a rolling update?\nAs it happens during software development, change is requested/needed to some parts of the system. So what happens to our cluster if I change one of its components without breaking the others whilst also maintaining backwards compatibility with no disruption to user experience? Thankfully Kubernetes can help with that.\nWhat I don’t like is that the API only handles one image at a time. Unfortunately there is no bulk upload option.\nCode Currently, we have the following code segment dealing with a single image:\n// PostImage handles a post of an image. Saves it to the database // and sends it to NSQ for further processing. func PostImage(w http.ResponseWriter, r *http.Request) { ... } func main() { router := mux.NewRouter() router.HandleFunc(\"/image/post\", PostImage).Methods(\"POST\") log.Fatal(http.ListenAndServe(\":8000\", router)) } We have two options: Add a new endpoint with /images/post and make the client use that, or modify the existing one.\nThe new client code has the advantage in that it can fall back to submitting the old way if the new endpoint isn’t available. The old client code, however, doesn’t have this advantage so we can’t change the way our code works right now. Consider this: You have 90 servers and you do a slow paced rolling update that will take out servers one step at a time whilst doing an update. If an update lasts around a minute, the whole process will take around one and a half hours to complete, (not counting any parallel updates).\nDuring this time, some of your servers will run the new code and some will run the old one. Calls are load balanced, thus you have no control over which servers will be hit. If a client is trying to do a call the new way but hits an old server, the client will fail. The client can try and fallback, but since you eliminated the old version it will not succeed unless it, by mere chance, hits a server with the new code (assuming no sticky sessions are set).\nAlso, once all your servers are updated, an old client will not be able to use your service any longer.\nNow, you can argue that you don’t want to keep old versions of your code forever. And that’s true in a sense. That’s why we are going to modify the old code to simply call the new one with some slight augmentations. This way, once all clients have been migrated, the code can simply be deleted without any problems.\nNew Endpoint Let’s add a new route method:\n... router.HandleFunc(\"/images/post\", PostImages).Methods(\"POST\") ... Updating the old one to call the new one with a modified body looks like this:\n// PostImage handles a post of an image. Saves it to the database // and sends it to NSQ for further processing. func PostImage(w http.ResponseWriter, r *http.Request) { var p Path err := json.NewDecoder(r.Body).Decode(\u0026p) if err != nil { fmt.Fprintf(w, \"got error while decoding body: %s\", err) return } fmt.Fprintf(w, \"got path: %+v\\n\", p) var ps Paths paths := make([]Path, 0) paths = append(paths, p) ps.Paths = paths var pathsJSON bytes.Buffer err = json.NewEncoder(\u0026pathsJSON).Encode(ps) if err != nil { fmt.Fprintf(w, \"failed to encode paths: %s\", err) return } r.Body = ioutil.NopCloser(\u0026pathsJSON) r.ContentLength = int64(pathsJSON.Len()) PostImages(w, r) } Well, the naming could be better, but you should get the basic idea. I’m modifying the incoming single path by wrapping it into the new format and sending it over to the new endpoint handler. And that’s it! There are a few more modifications. To check them out, take a look at this PR: Rolling Update Bulk Image Path PR.\nNow, the receiver can be called in two ways:\n# Single Path: curl -d '{\"path\":\"unknown4456.jpg\"}' http://127.0.0.1:8000/image/post # Multiple Paths: curl -d '{\"paths\":[{\"path\":\"unknown4456.jpg\"}]}' http://127.0.0.1:8000/images/post Here, the client is curl. Normally, if the client is a service, I would modify it that in case the new end-point throws a 404 it would try the old one next.\nFor brevity, I’m not modifying NSQ and the others to handle bulk image processing; they will still receive it one by one. I’ll leave that up to you as homework ;)\nNew Image To perform a rolling update, I must create a new image first from the receiver service.\ndocker build -t skarlso/kube-receiver-alpine:v1.1 . Once this is complete, we can begin rolling out the change.\nRolling update In Kubernetes, you can configure your rolling update in multiple ways:\nManual Update If I was using a container version in my config file called v1.0, then doing an update is simply calling:\nkubectl rolling-update receiver --image:skarlso/kube-receiver-alpine:v1.1 If there is a problem during the rollout we can always rollback.\nkubectl rolling-update receiver --rollback It will set back the previous version. No fuss, no muss.\nApply a new configuration file The problem with by-hand updates is that they aren’t in source control.\nConsider this: Something has changed, A couple of servers got updated by hand to do a quick “patch fix”, but nobody witnessed it and it wasn’t documented. A new person comes along and does a change to the template and applies the template to the cluster. All the servers are updated, and then all of a sudden there is a service outage.\nLong story short, the servers which got updated are written over because the template doesn’t reflect what has been done manually.\nThe recommended way is to change the template in order to use the new version, and than apply the template with the apply command.\nKubernetes recommends that a Deployment with ReplicaSets should handle a rollout. This means there must be at least two replicates present for a rolling update. If less than two replicates are present then the update won’t work (unless maxUnavailable is set to 1). I increase the replica count in yaml. I also set the new image version for the receiver container.\nreplicas: 2 ... spec: containers: - name: receiver image: skarlso/kube-receiver-alpine:v1.1 ... Looking at the progress, this is what you should see :\n❯ kubectl rollout status deployment/receiver-deployment Waiting for rollout to finish: 1 out of 2 new replicas have been updated... You can add in additional rollout configuration settings by specifying the strategy part of the template like this:\nstrategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 Additional information on rolling update can be found in the below documents: Deployment Rolling Update, Updating a Deployment, Manage Deployments, Rolling Update using ReplicaController.\nNOTE MINIKUBE USERS: Since we are doing this on a local machine with one node and 1 replica of an application, we have to set maxUnavailable to 1; otherwise Kubernetes won’t allow the update to happen, and the new version will remain in Pending state. That’s because we aren’t allowing for a services to exist with no running containers; which basically means service outage.\nScaling Scaling is dead easy with Kubernetes. Since it’s managing the whole cluster, you basically just need to put a number into the template of the desired replicas to use.\nThis has been a great post so far, but it’s getting too long. I’m planning on writing a follow-up where I will be truly scaling things up on AWS with multiple nodes and replicas; plus deploying a Kubernetes cluster with Kops. So stay tuned!\nCleanup kubectl delete deployments --all kubectl delete services -all Final Words And that’s it ladies and gentlemen. We wrote, deployed, updated and scaled (well, not yet really) a distributed application with Kubernetes.\nIf you have any questions, please feel free to chat in the comments below. I’m happy to answer.\nI hope you’ve enjoyed reading this. I know it’s quite long; I was thinking of splitting it up multiple posts, but having a cohesive, one page guide is useful and makes it easy to find, save, and print.\nThank you for reading, Gergely.\n",
  "wordCount" : "6618",
  "inLanguage": "en",
  "datePublished": "2018-03-15T23:01:00+01:00",
  "dateModified": "2018-03-15T23:01:00+01:00",
  "author":{
    "@type": "Person",
    "name": "hannibal"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://skarlso.github.io/2018/03/15/kubernetes-distributed-application/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Ramblings of a cloud engineer",
    "logo": {
      "@type": "ImageObject",
      "url": "https://skarlso.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://skarlso.github.io/" accesskey="h" title="Ramblings of a cloud engineer (Alt + H)">Ramblings of a cloud engineer</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://skarlso.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://skarlso.github.io/search/" title="Search">
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://skarlso.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://skarlso.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://skarlso.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://skarlso.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Kubernetes distributed application deployment with sample Face Recognition App
    </h1>
    <div class="post-meta"><span title='2018-03-15 23:01:00 +0100 +0100'>March 15, 2018</span>&nbsp;·&nbsp;32 min&nbsp;·&nbsp;hannibal

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#intro" aria-label="Intro">Intro</a></li>
                <li>
                    <a href="#the-application" aria-label="The Application">The Application</a><ul>
                        
                <li>
                    <a href="#tldr" aria-label="TL;DR">TL;DR</a></li>
                <li>
                    <a href="#receiver" aria-label="Receiver">Receiver</a></li>
                <li>
                    <a href="#image-processor" aria-label="Image Processor">Image Processor</a><ul>
                        
                <li>
                    <a href="#consume" aria-label="Consume">Consume</a></li>
                <li>
                    <a href="#processimages" aria-label="ProcessImages">ProcessImages</a></li>
                <li>
                    <a href="#circuit-breaker" aria-label="Circuit Breaker">Circuit Breaker</a></li></ul>
                </li>
                <li>
                    <a href="#front-end" aria-label="Front-End">Front-End</a></li>
                <li>
                    <a href="#face-recognition" aria-label="Face Recognition">Face Recognition</a></li>
                <li>
                    <a href="#nsq" aria-label="NSQ">NSQ</a></li>
                <li>
                    <a href="#configuration" aria-label="Configuration">Configuration</a></li>
                <li>
                    <a href="#conclusion-for-the-application" aria-label="Conclusion for the Application">Conclusion for the Application</a></li></ul>
                </li>
                <li>
                    <a href="#deployment-with-kubernetes" aria-label="Deployment with Kubernetes">Deployment with Kubernetes</a><ul>
                        
                <li>
                    <a href="#basics" aria-label="Basics">Basics</a><ul>
                        
                <li>
                    <a href="#kubernetes-cluster" aria-label="Kubernetes Cluster">Kubernetes Cluster</a></li>
                <li>
                    <a href="#nodes" aria-label="Nodes">Nodes</a></li>
                <li>
                    <a href="#pods" aria-label="Pods">Pods</a></li>
                <li>
                    <a href="#deployments" aria-label="Deployments">Deployments</a></li>
                <li>
                    <a href="#services" aria-label="Services">Services</a></li>
                <li>
                    <a href="#dns--service-discovery" aria-label="DNS / Service Discovery">DNS / Service Discovery</a></li>
                <li>
                    <a href="#template-files" aria-label="Template Files">Template Files</a></li>
                <li>
                    <a href="#replicaset" aria-label="ReplicaSet">ReplicaSet</a></li>
                <li>
                    <a href="#daemonset" aria-label="DaemonSet">DaemonSet</a></li>
                <li>
                    <a href="#scaling" aria-label="Scaling">Scaling</a></li>
                <li>
                    <a href="#conclusion-for-kubernetes" aria-label="Conclusion for Kubernetes">Conclusion for Kubernetes</a></li></ul>
                </li>
                <li>
                    <a href="#minikube" aria-label="Minikube">Minikube</a></li>
                <li>
                    <a href="#building-the-containers" aria-label="Building the containers">Building the containers</a></li>
                <li>
                    <a href="#context" aria-label="Context">Context</a></li>
                <li>
                    <a href="#deploying-the-application" aria-label="Deploying the Application">Deploying the Application</a><ul>
                        
                <li>
                    <a href="#mysql" aria-label="MySQL">MySQL</a></li>
                <li>
                    <a href="#nsq-lookup" aria-label="NSQ Lookup">NSQ Lookup</a></li>
                <li>
                    <a href="#receiver-1" aria-label="Receiver">Receiver</a><ul>
                        
                <li>
                    <a href="#deployments-1" aria-label="Deployments">Deployments</a></li>
                <li>
                    <a href="#nsq-daemon" aria-label="Nsq Daemon">Nsq Daemon</a></li>
                <li>
                    <a href="#public-facing" aria-label="Public facing">Public facing</a></li></ul>
                </li>
                <li>
                    <a href="#image-processor-1" aria-label="Image processor">Image processor</a></li>
                <li>
                    <a href="#face---recognition" aria-label="Face - Recognition">Face - Recognition</a></li>
                <li>
                    <a href="#frontend" aria-label="Frontend">Frontend</a></li>
                <li>
                    <a href="#recap" aria-label="Recap">Recap</a></li>
                <li>
                    <a href="#rolling-update" aria-label="Rolling update">Rolling update</a><ul>
                        
                <li>
                    <a href="#code" aria-label="Code">Code</a></li>
                <li>
                    <a href="#new-endpoint" aria-label="New Endpoint">New Endpoint</a></li>
                <li>
                    <a href="#new-image" aria-label="New Image">New Image</a></li>
                <li>
                    <a href="#rolling-update-1" aria-label="Rolling update">Rolling update</a><ul>
                        
                <li>
                    <a href="#manual-update" aria-label="Manual Update">Manual Update</a></li>
                <li>
                    <a href="#apply-a-new-configuration-file" aria-label="Apply a new configuration file">Apply a new configuration file</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#scaling-1" aria-label="Scaling">Scaling</a></li>
                <li>
                    <a href="#cleanup" aria-label="Cleanup">Cleanup</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#final-words" aria-label="Final Words">Final Words</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="intro">Intro<a hidden class="anchor" aria-hidden="true" href="#intro">#</a></h1>
<p>Alright folks. Settle in and get comfortable. This is going to be a long, but hopefully, fun ride.</p>
<p>I&rsquo;m going to deploy a distributed application with <a href="https://kubernetes.io/">Kubernetes</a>. I attempted to create an application that I thought resembled a real world app. Obviously I had to cut some corners due to time and energy constraints.</p>
<p>My focus will be on Kubernetes and deployment.</p>
<p>Shall we delve right in?</p>
<h1 id="the-application">The Application<a hidden class="anchor" aria-hidden="true" href="#the-application">#</a></h1>
<h2 id="tldr">TL;DR<a hidden class="anchor" aria-hidden="true" href="#tldr">#</a></h2>
<p><img alt="kube overview" loading="lazy" src="/img/kube_overview.png"></p>
<p>The application itself consists of six parts. The repository can be found here: <a href="https://github.com/Skarlso/kube-cluster-sample">Kube Cluster Sample</a>.</p>
<p>It’s a face recognition service which identifies images of people, comparing them to known individuals. A simple frontend displays a table of these images whom they belong to. This happens by sending a request to a <a href="https://github.com/Skarlso/kube-cluster-sample/tree/master/receiver">receiver</a>. The request contains a path to an image. This image can sit on an NFS somewhere. The receiver stores this path in the DB (MySQL) and sends a processing request to a queue. The queue uses: <a href="http://nsq.io/">NSQ</a>. The request contains the ID of the saved image.</p>
<p>An <a href="https://github.com/Skarlso/kube-cluster-sample/tree/master/image_processor">Image Processing</a> service is constantly monitoring the queue for jobs to do. The processing consists of the following steps: taking the ID; loading the image; and finally,  sending the image to a <a href="https://github.com/Skarlso/kube-cluster-sample/tree/master/face_recognition">face recognition</a> backend written in Python via <a href="https://grpc.io/">gRPC</a>. If the identification is successful, the backend will return the name of the image corresponding to that person. The image_processor then updates the image’s record with the person’s ID and marks the image as “processed successfully”. If identification is unsuccessful, the image will be left as “pending”. If there was a failure during identification, the image will be flagged as “failed”.</p>
<p>Failed images can be retried  with a cron job, for example:</p>
<p>So how does this all work? Let&rsquo;s check it out .</p>
<h2 id="receiver">Receiver<a hidden class="anchor" aria-hidden="true" href="#receiver">#</a></h2>
<p>The receiver service is the starting point of the process. It&rsquo;s an API which receives a request in the following format:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -d <span style="color:#e6db74">&#39;{&#34;path&#34;:&#34;/unknown_images/unknown0001.jpg&#34;}&#39;</span> http://127.0.0.1:8000/image/post
</span></span></code></pre></div><p>In this instance, the receiver stores the path using a shared database cluster. The entity will then receive an ID from the database service. This application is based on the model where unique identification for Entity Objects is provided by the persistence layer. Once the ID is procured, the receiver will send a message to NSQ. At this point in the process, the receiver&rsquo;s job is done.</p>
<h2 id="image-processor">Image Processor<a hidden class="anchor" aria-hidden="true" href="#image-processor">#</a></h2>
<p>Here is where the excitement begins. When Image Processor first runs it creates two Go routines. These are&hellip;</p>
<h3 id="consume">Consume<a hidden class="anchor" aria-hidden="true" href="#consume">#</a></h3>
<p>This is an NSQ consumer. It has three integral jobs. Firstly, it listens for messages on the queue. Secondly, when there is a message, it appends the received ID to a thread safe slice of IDs that the second routine processes. And lastly, it signals the second routine that there is work to be do. It does this through <a href="https://golang.org/pkg/sync/#Cond">sync.Condition</a>.</p>
<h3 id="processimages">ProcessImages<a hidden class="anchor" aria-hidden="true" href="#processimages">#</a></h3>
<p>This routine processes a slice of IDs until the slice is drained completely. Once the slice is drained, the routine suspends instead of sleep-waiting on a channel. The processing of a single ID can be seen in the following linear steps:</p>
<ul>
<li>Establish a gRPC connection to the Face Recognition service (explained under Face Recognition)</li>
<li>Retrieve the image record from the database</li>
<li>Setup two functions for the <a href="/2018/03/15/kubernetes-distributed-application/#circuit-breaker">Circuit Breaker</a>
<ul>
<li>Function 1: The main function which runs  the RPC method call</li>
<li>Function 2: A health check for the Ping of the circuit breaker</li>
</ul>
</li>
<li>Call Function 1 which sends the path of the image to the face recognition service. This path should be accessible by the face recognition service. Preferably something shared like an NFS</li>
<li>If this call fails, update the image record as FAILED PROCESSING</li>
<li>If it succeeds, an image name should come back which corresponds to a person in the db. It runs a joined SQL query which gets the corresponding person&rsquo;s ID</li>
<li>Update the Image record in the database with PROCESSED status and the ID of the person that image was identified as</li>
</ul>
<p>This service can be replicated. In other words, more than one can run at the same time.</p>
<h3 id="circuit-breaker">Circuit Breaker<a hidden class="anchor" aria-hidden="true" href="#circuit-breaker">#</a></h3>
<p>A  system in which replicating resources requires little to no effort, there still can be cases where, for example, the network goes down, or there are communication problems of any kind between two services. I like to implement a little circuit breaker around the gRPC calls for fun.</p>
<p>This is how it works:</p>
<p><img alt="kube circuit" loading="lazy" src="/img/kube_circuit1.png"></p>
<p>As you can see, once there are 5 unsuccessful calls to the service, the circuit breaker activates, not allowing any more calls to go through. After a configured amount of time, it will send a Ping call to the service to see if it&rsquo;s back up. If that still errors out, it will increase the timeout. If not, it opens the circuit, allowing traffic to proceed.</p>
<h2 id="front-end">Front-End<a hidden class="anchor" aria-hidden="true" href="#front-end">#</a></h2>
<p>This is only a simple table view with Go&rsquo;s own html/template used to render a list of images.</p>
<h2 id="face-recognition">Face Recognition<a hidden class="anchor" aria-hidden="true" href="#face-recognition">#</a></h2>
<p>Here is where the identification magic happens. I decided to make this a gRPC based service for the  sole purpose of its flexibility. I started writing it in Go but decided that a Python implementation would be much sorter. In fact, excluding the gRPC code, the recognition part is approximately 7 lines of Python code. I&rsquo;m using this fantastic library which contains all the C bindings to OpenCV. <a href="https://github.com/ageitgey/face_recognition">Face Recognition</a>. Having an API contract here means that I can change the implementation anytime as long as it adheres to the contract.</p>
<p>Please note that there exist a great Go library OpenCV. I was about to use it but they had yet to write the C bindings for that part of OpenCV. It&rsquo;s called <a href="https://gocv.io/">GoCV</a>. Check them out! They have some pretty amazing things, like real-time camera feed processing that only needs a couple of lines of code.</p>
<p>The python library is simple in nature. Have a set of images of people you know. I have a folder with a couple of images named, <code>hannibal_1.jpg, hannibal_2.jpg, gergely_1.jpg, john_doe.jpg</code>. In the database I have two tables named, <code>person, person_images</code>. They look like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>+----+----------+
</span></span><span style="display:flex;"><span>| id | name     |
</span></span><span style="display:flex;"><span>+----+----------+
</span></span><span style="display:flex;"><span>|  <span style="color:#ae81ff">1</span> | Gergely  |
</span></span><span style="display:flex;"><span>|  <span style="color:#ae81ff">2</span> | John Doe |
</span></span><span style="display:flex;"><span>|  <span style="color:#ae81ff">3</span> | Hannibal |
</span></span><span style="display:flex;"><span>+----+----------+
</span></span><span style="display:flex;"><span>+----+----------------+-----------+
</span></span><span style="display:flex;"><span>| id | image_name     | person_id |
</span></span><span style="display:flex;"><span>+----+----------------+-----------+
</span></span><span style="display:flex;"><span>|  <span style="color:#ae81ff">1</span> | hannibal_1.jpg |         <span style="color:#ae81ff">3</span> |
</span></span><span style="display:flex;"><span>|  <span style="color:#ae81ff">2</span> | hannibal_2.jpg |         <span style="color:#ae81ff">3</span> |
</span></span><span style="display:flex;"><span>+----+----------------+-----------+
</span></span></code></pre></div><p>The face recognition library returns the name of the image from the known people which matches the person on the unknown image. After that, a simple joined query -like this- will return the person in question.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sql" data-lang="sql"><span style="display:flex;"><span><span style="color:#66d9ef">select</span> person.name, person.id <span style="color:#66d9ef">from</span> person <span style="color:#66d9ef">inner</span> <span style="color:#66d9ef">join</span> person_images <span style="color:#66d9ef">as</span> pi <span style="color:#66d9ef">on</span> person.id <span style="color:#f92672">=</span> pi.person_id <span style="color:#66d9ef">where</span> image_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;hannibal_2.jpg&#39;</span>;
</span></span></code></pre></div><p>The gRPC call returns the ID of the person which is then used to update the image&rsquo;s ‘person` column.</p>
<h2 id="nsq">NSQ<a hidden class="anchor" aria-hidden="true" href="#nsq">#</a></h2>
<p>NSQ is a nice little Go based queue. It can be scaled and has a minimal footprint on the system. It also has a lookup service that consumers use to receive messages, and a daemon that senders use when sending messages.</p>
<p>NSQ&rsquo;s philosophy is that the daemon should run with the sender application. That way, the sender will send to the localhost only. But the daemon is connected to the lookup service, and that&rsquo;s how they achieve a global queue.</p>
<p>This means that there are as many NSQ daemons deployed as there are senders. Because the daemon has a minuscule resource requirement, it won&rsquo;t interfere with the requirements of the main application.</p>
<h2 id="configuration">Configuration<a hidden class="anchor" aria-hidden="true" href="#configuration">#</a></h2>
<p>In order to be as flexible as possible, as well as making use of Kubernetes&rsquo;s ConfigSet, I&rsquo;m using .env files in development to store configurations like the location of the database service, or NSQ&rsquo;s lookup address. In production- and that means the Kubernetes’s environment- I&rsquo;ll use environment properties.</p>
<h2 id="conclusion-for-the-application">Conclusion for the Application<a hidden class="anchor" aria-hidden="true" href="#conclusion-for-the-application">#</a></h2>
<p>And that&rsquo;s all there is to the architecture of the application we are about to deploy. All of its components are changeable and coupled only through the database, a queue and gRPC. This is imperative when deploying a distributed application due to how updating mechanics work. I will cover that part in the Deployment section.</p>
<h1 id="deployment-with-kubernetes">Deployment with Kubernetes<a hidden class="anchor" aria-hidden="true" href="#deployment-with-kubernetes">#</a></h1>
<h2 id="basics">Basics<a hidden class="anchor" aria-hidden="true" href="#basics">#</a></h2>
<p>What <strong>is</strong> Kubernetes?</p>
<p>I&rsquo;m going to cover some of the basics here. I won&rsquo;t go too much into detail-  that would require a whole book like this one: <a href="http://shop.oreilly.com/product/0636920043874.do">Kubernetes Up And Running</a>. Also, if you’re daring enough, you can have a look through this documentation: <a href="https://kubernetes.io/docs/">Kubernetes Documentation</a>.</p>
<p>Kubernetes is a containerized service and application manager. It scales easily, employs a swarm of containers, and most importantly, it&rsquo;s highly configurable via yaml based template files. People often compare Kubernetes to Docker swarm, but Kubernetes does way more than that! For example: it&rsquo;s container agnostic. You could use LXC with Kubernetes and it would work the same way as you using it with Docker. It provides a layer above managing a cluster of deployed services and applications. How? Let&rsquo;s take a quick look at the building blocks of Kubernetes.</p>
<p>In Kubernetes, you’ll describe a desired state of the application and Kubernetes will do what it can to reach that state. States could be something such as deployed; paused; replicated twice; and so on and so forth.</p>
<p>One of the basics of Kubernetes is that it uses Labels and Annotations for all of its components. Services, Deployments, ReplicaSets, DaemonSets, everything is labelled. Consider the following scenario. In order to identify what pod belongs to what application, a label is used called <code>app: myapp</code>. Let’s assume you have two containers of this application deployed; if you would remove the label <code>app</code> from one of the containers, Kubernetes would only detect one and thus would launch a new instance of <code>myapp</code>.</p>
<h3 id="kubernetes-cluster">Kubernetes Cluster<a hidden class="anchor" aria-hidden="true" href="#kubernetes-cluster">#</a></h3>
<p>For Kuberenetes to work, a Kubernetes cluster needs to be present. Setting that up might be a tad painful, but luckily, help is on hand. Minikube sets up a cluster for us locally with one Node. And AWS has a beta service running in the form of a Kubernetes cluster in which the only thing you need to do is request nodes and define your deployments. The Kubernetes cluster components are documented here: <a href="https://kubernetes.io/docs/concepts/overview/components/">Kubernetes Cluster Components</a>.</p>
<h3 id="nodes">Nodes<a hidden class="anchor" aria-hidden="true" href="#nodes">#</a></h3>
<p>A Node is a worker machine. It can be anything- from a vm to a physical machine- including all sorts of cloud provided vms.</p>
<h3 id="pods">Pods<a hidden class="anchor" aria-hidden="true" href="#pods">#</a></h3>
<p>Pods are a logically grouped collection of containers, meaning one Pod can potentially house a multitude of containers. A Pod gets its own DNS and virtual IP address after it has been created so Kubernetes can load balancer traffic to it. You rarely need to deal with containers directly. Even when debugging, (like looking at logs), you usually invoke <code>kubectl logs deployment/your-app -f</code> instead of looking at a specific container. Although it is possible with <code>-c container_name</code>. The <code>-f</code> does a tail on the log.</p>
<h3 id="deployments">Deployments<a hidden class="anchor" aria-hidden="true" href="#deployments">#</a></h3>
<p>When creating any kind of resource in Kubernetes, it will use a Deployment in the background. A deployment describes a desired state of the current application. It&rsquo;s an object you can use to update Pods or a Service to be in a different state, do an update, or rollout new version of your app. You don&rsquo;t directly control a ReplicaSet, (as described later), but control the deployment object which creates and manages a ReplicaSet.</p>
<h3 id="services">Services<a hidden class="anchor" aria-hidden="true" href="#services">#</a></h3>
<p>By default a Pod will get an IP address. However, since Pods are a volatile thing in Kubernetes, you&rsquo;ll need something more permanent. A queue, mysql, or an internal API, a frontend; these need to be long running and behind a static, unchanging IP or preferably a DNS record.</p>
<p>For this purpose, Kubernetes has Services for which you can define modes of accessibility. Load Balanced, simple IP or internal DNS.</p>
<p>How does Kubernetes know if a service is running correctly? You can configure Health Checks and Availability Checks. A Health Check will check whether a container is running, but that doesn&rsquo;t mean that your service is running. For that, you have the availability check which pings a different endpoint in your application.</p>
<p>Since Services are pretty important, I recommend that you read up on them later here: <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services</a>. Advanced  warning though, this document is quite dense. Twenty four A4 pages of networking, services and discovery. It&rsquo;s also vital to decide whether you want to seriously employ Kubernetes in production.</p>
<h3 id="dns--service-discovery">DNS / Service Discovery<a hidden class="anchor" aria-hidden="true" href="#dns--service-discovery">#</a></h3>
<p>If you create a service in the cluster, that service will get a DNS record in Kubernetes provided by special Kubernetes deployments called kube-proxy and kube-dns. These two provide service discover inside a cluster. If you have a mysql service running and set <code>clusterIP: none</code>, then everyone in the cluster can reach that service by pinging <code>mysql.default.svc.cluster.local</code>. Where:</p>
<ul>
<li><code>mysql</code> &ndash; is the name of the service</li>
<li><code>default</code> &ndash; is the namespace name</li>
<li><code>svc</code> &ndash; is services</li>
<li><code>cluster.local</code> &ndash; is a local cluster domain</li>
</ul>
<p>The domain can be changed via a custom definition. To access a service outside the cluster, a DNS provider has to be used, and Nginx (for example), to bind an IP address to a record. The public IP address of a service can be queried with the following commands:</p>
<ul>
<li>NodePort &ndash; <code>kubectl get -o jsonpath=&quot;{.spec.ports[0].nodePort}&quot; services mysql</code></li>
<li>LoadBalancer &ndash; <code>kubectl get -o jsonpath=&quot;{.spec.ports[0].LoadBalancer}&quot; services mysql</code></li>
</ul>
<h3 id="template-files">Template Files<a hidden class="anchor" aria-hidden="true" href="#template-files">#</a></h3>
<p>Like Docker Compose, TerraForm or other service management tools, Kubernetes also provides infrastructure describing templates. What that means is that you rarely need  to do anything by hand.</p>
<p>For example, consider the following yaml template which describes an nginx Deployment:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span> <span style="color:#75715e">#(1)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>: <span style="color:#75715e">#(2)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nginx-deployment</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">labels</span>: <span style="color:#75715e">#(3)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">nginx</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>: <span style="color:#75715e">#(4)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">3</span> <span style="color:#75715e">#(5)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">matchLabels</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">nginx</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">template</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">app</span>: <span style="color:#ae81ff">nginx</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">containers</span>: <span style="color:#75715e">#(6)</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nginx</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">nginx:1.7.9</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">80</span>
</span></span></code></pre></div><p>This is a simple deployment in which we do the following:</p>
<ul>
<li>(1) Define the type of the template with kind</li>
<li>(2) Add metadata that will identify this deployment and every resource that it would create with a label (3)</li>
<li>(4) Then comes the spec which describes the desired state
<ul>
<li>(5) For the nginx app, have 3 replicas</li>
<li>(6) This is the template definition for the containers that this Pod will contain
<ul>
<li>nginx named container</li>
<li>nginx:1.7.9 image (docker in this case)</li>
<li>exposed ports</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="replicaset">ReplicaSet<a hidden class="anchor" aria-hidden="true" href="#replicaset">#</a></h3>
<p>A ReplicaSet is a low level replication manager. It ensures that the correct number of replicates are running for a application. However, Deployments are at a higher level and should always manage ReplicaSets. You rarely need to use ReplicaSets directly unless you have a fringe case in which you want to control the specifics of replication.</p>
<h3 id="daemonset">DaemonSet<a hidden class="anchor" aria-hidden="true" href="#daemonset">#</a></h3>
<p>Remember how I said Kubernetes is using Labels all the time? A DaemonSet is a controller that ensures that at daemonized application is always running on a node with a certain label.</p>
<p>For example: you want all the nodes labelled with <code>logger</code> or <code>mission_critical</code> to run an logger / auditing service daemon. Then you create a DaemonSet and give it a node selector called <code>logger</code> or <code>mission_critical</code>. Kubernetes will look for a node that has that label. Always ensure that it will have an instance of that daemon running on it. Thus everyone running on that node will have access to that daemon locally.</p>
<p>In case of my application, the NSQ daemon could be a DaemonSet. Make sure it&rsquo;s up on a node which has the receiver component running by labelling a node with <code>receiver</code> and specifying a DaemonSet with a <code>receiver</code> application selector.</p>
<p>The DaemonSet has all the benefits of the ReplicaSet. It&rsquo;s scalable and Kubernetes manages it; which means, all life cycle events are handled by Kube ensuring it never dies, and when it does,  it will be immediately replaced.</p>
<h3 id="scaling">Scaling<a hidden class="anchor" aria-hidden="true" href="#scaling">#</a></h3>
<p>It&rsquo;s trivial to scale in Kubernetes. The ReplicaSets take care of the number of instances of a Pod to run- as seen in the nginx deployment with the setting <code>replicas:3</code>. It&rsquo;s up to us to write our application in a way that allows Kubernetes to run multiple copies of it.</p>
<p>Of course the settings are vast. You can specify which replicates must run on what Nodes, or on various waiting times as to how long to wait for an instance to come up. You can read more on this subject here: <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Scaling</a> and here: <a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/scale-interactive/">Interactive Scaling with Kubernetes</a> and of course the details of a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a> which controls all the scaling made possible in Kubernetes.</p>
<h3 id="conclusion-for-kubernetes">Conclusion for Kubernetes<a hidden class="anchor" aria-hidden="true" href="#conclusion-for-kubernetes">#</a></h3>
<p>It&rsquo;s a convenient tool to handle container orchestration. Its unit of work are Pods and it has a layered architecture. The top level layer is Deployments through which you handle all other resources. It&rsquo;s highly configurable. It provides an API for all calls you make, so potentially, instead of running <code>kubectl</code> you can also write your own logic to send information to the Kubernetes API.</p>
<p>It provides support for all major cloud providers natively by now and it&rsquo;s completely open source. Feel free to contribute! And check the code if you would like to have a deeper understanding on how it works: <a href="https://github.com/kubernetes/kubernetes">Kubernetes on Github</a>.</p>
<h2 id="minikube">Minikube<a hidden class="anchor" aria-hidden="true" href="#minikube">#</a></h2>
<p>I&rsquo;m going to use <a href="https://github.com/kubernetes/minikube/">Minikube</a>. Minikube is a local Kubernetes cluster simulator. It&rsquo;s not great in simulating multiple nodes though, but for starting out and local play without any costs, it&rsquo;s great. It uses a VM that can be fine tuned if necessary using VirtualBox and the likes.</p>
<p>All of the kube template files that I&rsquo;ll be using can be found here: <a href="https://github.com/Skarlso/kube-cluster-sample/tree/master/kube_files">Kube files</a>.</p>
<p><strong>NOTE</strong> If, later on, you would like to play with scaling but notice that the replicates are always in <code>Pending</code> state, remember that minikube employs a single node only. It might not allow multiple replicas on the same node, or just plainly ran out of resources to use. You can check available resources with the following command:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl get nodes -o yaml
</span></span></code></pre></div><h2 id="building-the-containers">Building the containers<a hidden class="anchor" aria-hidden="true" href="#building-the-containers">#</a></h2>
<p>Kubernetes supports most of the containers out there. I&rsquo;m going to use Docker. For all the services I&rsquo;ve built, there is a Dockerfile included in the repository. I encourage you to study them. Most of them are simple. For the go services, I&rsquo;m using a multi stage build that has been  recently introduced. The Go services are Alpine Linux based. The Face Recognition service is Python. NSQ and MySQL are using their own containers.</p>
<h2 id="context">Context<a hidden class="anchor" aria-hidden="true" href="#context">#</a></h2>
<p>Kubernetes uses namespaces. If you don&rsquo;t specify any, it will use the <code>default</code> namespace. I&rsquo;m going to permanently set a context to avoid polluting the default namespace. You do that like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>❯ kubectl config set-context kube-face-cluster --namespace<span style="color:#f92672">=</span>face
</span></span><span style="display:flex;"><span>Context <span style="color:#e6db74">&#34;kube-face-cluster&#34;</span> created.
</span></span></code></pre></div><p>You have to also start using the context once it&rsquo;s created, like so:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>❯ kubectl config use-context kube-face-cluster
</span></span><span style="display:flex;"><span>Switched to context <span style="color:#e6db74">&#34;kube-face-cluster&#34;</span>.
</span></span></code></pre></div><p>After this, all <code>kubectl</code> commands will use the namespace <code>face</code>.</p>
<h2 id="deploying-the-application">Deploying the Application<a hidden class="anchor" aria-hidden="true" href="#deploying-the-application">#</a></h2>
<p>Overview of Pods and Services:</p>
<p><img alt="kube deployed" loading="lazy" src="/img/kube_deployed.png"></p>
<h3 id="mysql">MySQL<a hidden class="anchor" aria-hidden="true" href="#mysql">#</a></h3>
<p>The first Service I&rsquo;m going to deploy is my database.</p>
<p>I&rsquo;m using the Kubernetes example located here <a href="https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/#deploy-mysql">Kube MySQL</a> which fits my needs. Please note that this file is using a plain password for MYSQL_PASSWORD. I&rsquo;m going to employ a vault as described here <a href="https://kubernetes.io/docs/concepts/configuration/secret/">Kubernetes Secrets</a>.</p>
<p>I&rsquo;ve created a secret locally as described in that document using a secret yaml:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Secret</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">kube-face-secret</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">type</span>: <span style="color:#ae81ff">Opaque</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">data</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">mysql_password</span>: <span style="color:#ae81ff">base64codehere</span>
</span></span></code></pre></div><p>I created the  base64 code via the following command:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>echo -n <span style="color:#e6db74">&#34;ubersecurepassword&#34;</span> | base64
</span></span></code></pre></div><p>And, this is what you&rsquo;ll see in my deployment yaml file:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">name</span>: <span style="color:#ae81ff">MYSQL_ROOT_PASSWORD</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">valueFrom</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">secretKeyRef</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">name</span>: <span style="color:#ae81ff">kube-face-secret</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">key</span>: <span style="color:#ae81ff">mysql_password</span>
</span></span><span style="display:flex;"><span>...
</span></span></code></pre></div><p>Another thing worth mentioning: It&rsquo;s using a volume to persist the database. The volume definition is as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">mysql-persistent-storage</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/var/lib/mysql</span>
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">mysql-persistent-storage</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">persistentVolumeClaim</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">claimName</span>: <span style="color:#ae81ff">mysql-pv-claim</span>
</span></span><span style="display:flex;"><span>...
</span></span></code></pre></div><p><code>presistentVolumeClain</code> is key here. This tells Kubernetes that this resource requires a persistent volume. How it&rsquo;s provided is abstracted away from the user. You can be sure that Kubernetes will provide a volume that will always be there. It is similar to Pods. To read up on the details, check out this document: <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes">Kubernetes Persistent Volumes</a>.</p>
<p>Deploying the mysql Service is done with the following command:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl apply -f mysql.yaml
</span></span></code></pre></div><p><code>apply</code> vs <code>create</code>. In short, <code>apply</code> is considered a declarative object configuration command while <code>create</code> is imperative. What this means for now is that ‘create’ is usually for a one of tasks, like running something or creating a deployment. While, when using apply, the user doesn&rsquo;t define the action to be taken. That will be defined by Kubernetes based on the current status of the cluster. Thus, when there is no service called <code>mysql</code> and I&rsquo;m calling <code>apply -f mysql.yaml</code> it will create the service. When running again, Kubernetes won&rsquo;t do anything. But if I would run <code>create</code> again it will throw an error saying the service is already created.</p>
<p>For more information, check out the following docs: <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/">Kubernetes Object Management</a>, <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/">Imperative Configuration</a>, <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/">Declarative Configuration</a>.</p>
<p>To see progress information, run:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Describes the whole process</span>
</span></span><span style="display:flex;"><span>kubectl describe deployment mysql
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Shows only the pod</span>
</span></span><span style="display:flex;"><span>kubectl get pods -l app<span style="color:#f92672">=</span>mysql
</span></span></code></pre></div><p>Output should be similar to this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>  Type           Status  Reason
</span></span><span style="display:flex;"><span>  ----           ------  ------
</span></span><span style="display:flex;"><span>  Available      True    MinimumReplicasAvailable
</span></span><span style="display:flex;"><span>  Progressing    True    NewReplicaSetAvailable
</span></span><span style="display:flex;"><span>OldReplicaSets:  &lt;none&gt;
</span></span><span style="display:flex;"><span>NewReplicaSet:   mysql-55cd6b9f47 <span style="color:#f92672">(</span>1/1 replicas created<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>...
</span></span></code></pre></div><p>Or in case of <code>get pods</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>NAME                     READY     STATUS    RESTARTS   AGE
</span></span><span style="display:flex;"><span>mysql-78dbbd9c49-k6sdv   1/1       Running   <span style="color:#ae81ff">0</span>          18s
</span></span></code></pre></div><p>To test the instance, run the following snippet:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl run -it --rm --image<span style="color:#f92672">=</span>mysql:5.6 --restart<span style="color:#f92672">=</span>Never mysql-client -- mysql -h mysql -pyourpasswordhere
</span></span></code></pre></div><p><strong>GOTCHA</strong>: If you change the password now, it&rsquo;s not enough to re-apply your yaml file to update the container. Since the DB is persisted, the password will not be changed. You have to delete the whole deployment with <code>kubectl delete -f mysql.yaml</code>.</p>
<p>You should see the following when running a <code>show databases</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>If you don<span style="color:#960050;background-color:#1e0010">&#39;</span>t see a command prompt, try pressing enter.
</span></span><span style="display:flex;"><span>mysql&gt;
</span></span><span style="display:flex;"><span>mysql&gt;
</span></span><span style="display:flex;"><span>mysql&gt; show databases;
</span></span><span style="display:flex;"><span>+--------------------+
</span></span><span style="display:flex;"><span>| Database           |
</span></span><span style="display:flex;"><span>+--------------------+
</span></span><span style="display:flex;"><span>| information_schema |
</span></span><span style="display:flex;"><span>| kube               |
</span></span><span style="display:flex;"><span>| mysql              |
</span></span><span style="display:flex;"><span>| performance_schema |
</span></span><span style="display:flex;"><span>+--------------------+
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span> rows in set <span style="color:#f92672">(</span>0.00 sec<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mysql&gt; exit
</span></span><span style="display:flex;"><span>Bye
</span></span></code></pre></div><p>You&rsquo;ll also notice that I’ve mounted a file located here: <a href="https://github.com/Skarlso/kube-cluster-sample/blob/master/database_setup.sql">Database Setup SQL</a> into the container. MySQL container automatically executes these. That file will bootstrap some data and the schema I&rsquo;m going to use.</p>
<p>The volume definition is as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>  <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">mysql-persistent-storage</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/var/lib/mysql</span>
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">bootstrap-script</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/docker-entrypoint-initdb.d/database_setup.sql</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">name</span>: <span style="color:#ae81ff">mysql-persistent-storage</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">persistentVolumeClaim</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">claimName</span>: <span style="color:#ae81ff">mysql-pv-claim</span>
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">name</span>: <span style="color:#ae81ff">bootstrap-script</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">hostPath</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/Users/hannibal/golang/src/github.com/Skarlso/kube-cluster-sample/database_setup.sql</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">type</span>: <span style="color:#ae81ff">File</span>
</span></span></code></pre></div><p>To check if the bootstrap script was successful, run this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>~/golang/src/github.com/Skarlso/kube-cluster-sample/kube_files master*
</span></span><span style="display:flex;"><span>❯ kubectl run -it --rm --image<span style="color:#f92672">=</span>mysql:5.6 --restart<span style="color:#f92672">=</span>Never mysql-client -- mysql -h mysql -uroot -pyourpasswordhere kube
</span></span><span style="display:flex;"><span>If you don<span style="color:#960050;background-color:#1e0010">&#39;</span>t see a command prompt, try pressing enter.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mysql&gt; show tables;
</span></span><span style="display:flex;"><span>+----------------+
</span></span><span style="display:flex;"><span>| Tables_in_kube |
</span></span><span style="display:flex;"><span>+----------------+
</span></span><span style="display:flex;"><span>| images         |
</span></span><span style="display:flex;"><span>| person         |
</span></span><span style="display:flex;"><span>| person_images  |
</span></span><span style="display:flex;"><span>+----------------+
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span> rows in set <span style="color:#f92672">(</span>0.00 sec<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mysql&gt;
</span></span></code></pre></div><p>This concludes the database service setup. Logs for this service can be viewed with the following command:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl logs deployment/mysql -f
</span></span></code></pre></div><h3 id="nsq-lookup">NSQ Lookup<a hidden class="anchor" aria-hidden="true" href="#nsq-lookup">#</a></h3>
<p>The NSQ Lookup will run as an internal service. It doesn&rsquo;t need access from the outside, so I&rsquo;m setting <code>clusterIP: None</code> which will tell Kubernetes that this service is a headless service. This means that it won&rsquo;t be load balanced, and it won&rsquo;t be a single IP service. The DNS will be based upon service selectors.</p>
<p>Our NSQ Lookup selector is:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">matchLabels</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">nsqlookup</span>
</span></span></code></pre></div><p>Thus, the internal DNS will look like this: <code>nsqlookup.default.svc.cluster.local</code>.</p>
<p>Headless services are described in detail here: <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services">Headless Service</a>.</p>
<p>Basically it&rsquo;s the same as MySQL, just with slight modifications. As stated earlier, I&rsquo;m using NSQ&rsquo;s own Docker Image called <code>nsqio/nsq</code>. All nsq commands are there, so nsqd will also use this image just with a different command. For nsqlookupd, the command is:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">command</span>: [<span style="color:#e6db74">&#34;/nsqlookupd&#34;</span>]
</span></span><span style="display:flex;"><span><span style="color:#f92672">args</span>: [<span style="color:#e6db74">&#34;--broadcast-address=nsqlookup.default.svc.cluster.local&#34;</span>]
</span></span></code></pre></div><p>What&rsquo;s the <code>--broadcast-address</code> for, you might ask? By default, nsqlookup will use the <code>hostname</code> as broadcast address. When the consumer runs a callback it will try to connect to something like: <code>http://nsqlookup-234kf-asdf:4161/lookup?topics=image</code>. Please note that <code>nsqlookup-234kf-asdf</code> is the hostname of the container. By setting the broadcast-address to the internal DNS, the callback will be: <code>http://nsqlookup.default.svc.cluster.local:4161/lookup?topic=images</code>. Which will work as expected.</p>
<p>NSQ Lookup also requires two ports forwarded: One for broadcasting and one for nsqd callback. These are exposed in the Dockerfile, and then utilized in the Kubernetes template. Like this:</p>
<p>In the container template:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>        <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">4160</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">hostPort</span>: <span style="color:#ae81ff">4160</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">4161</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">hostPort</span>: <span style="color:#ae81ff">4161</span>
</span></span></code></pre></div><p>In the service template:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">tcp</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">port</span>: <span style="color:#ae81ff">4160</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">4160</span>
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">http</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">port</span>: <span style="color:#ae81ff">4161</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">4161</span>
</span></span></code></pre></div><p>Names are required by Kubernetes.</p>
<p>To create this service, I&rsquo;m using the same command as before:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl apply -f nsqlookup.yaml
</span></span></code></pre></div><p>This concludes nsqlookupd. Two of the major players are in the sack!</p>
<h3 id="receiver-1">Receiver<a hidden class="anchor" aria-hidden="true" href="#receiver-1">#</a></h3>
<p>This is a more complex one. The receiver will do three things:</p>
<ul>
<li>Create some deployments;</li>
<li>Create the nsq daemon;</li>
<li>Expose the service to the public.</li>
</ul>
<h4 id="deployments-1">Deployments<a hidden class="anchor" aria-hidden="true" href="#deployments-1">#</a></h4>
<p>The first deployment it creates is its own. The receiver’s container is <code>skarlso/kube-receiver-alpine</code>.</p>
<h4 id="nsq-daemon">Nsq Daemon<a hidden class="anchor" aria-hidden="true" href="#nsq-daemon">#</a></h4>
<p>The receiver starts an nsq daemon. As stated earlier, the receiver runs an nsqd with it-self. It does this so talking to it can happen locally and not over the network. By making the receiver do this, they will end up on the same node.</p>
<p>NSQ daemon also needs some adjustments and parameters.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>        <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">4150</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">hostPort</span>: <span style="color:#ae81ff">4150</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">4151</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">hostPort</span>: <span style="color:#ae81ff">4151</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">env</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">NSQLOOKUP_ADDRESS</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">value</span>: <span style="color:#ae81ff">nsqlookup.default.svc.cluster.local</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">NSQ_BROADCAST_ADDRESS</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">value</span>: <span style="color:#ae81ff">nsqd.default.svc.cluster.local</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">command</span>: [<span style="color:#e6db74">&#34;/nsqd&#34;</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">args</span>: [<span style="color:#e6db74">&#34;--lookupd-tcp-address=$(NSQLOOKUP_ADDRESS):4160&#34;</span>, <span style="color:#e6db74">&#34;--broadcast-address=$(NSQ_BROADCAST_ADDRESS)&#34;</span>]
</span></span></code></pre></div><p>You can see that the lookup-tcp-address and the broadcast-address are set. Lookup tcp address is the DNS for the nsqlookupd service. And the broadcast address is necessary, just like with nsqlookupd, so the callbacks are working properly.</p>
<h4 id="public-facing">Public facing<a hidden class="anchor" aria-hidden="true" href="#public-facing">#</a></h4>
<p>Now, this is the first time I&rsquo;m deploying a public facing service. There are two options. I could use a LoadBalancer since this API will be under heavy load. And if this would be deployed anywhere in production, then it should be using one.</p>
<p>I&rsquo;m doing this locally though- with one node- so something called a <code>NodePort</code> is enough. A <code>NodePort</code> exposes a service on each node&rsquo;s IP at a static port. If not specified, it will assign a random port on the host between 30000-32767. But it can also be configured to be a specific port, using <code>nodePort</code> in the template file. To reach this service, use <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code>. If more than one node is configured, a LoadBalancer can multiplex them to a single IP.</p>
<p>For further information, check out this document: <a href="https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services---service-types">Publishing Services</a>.</p>
<p>Putting this all together, we&rsquo;ll get a receiver-service for which the template for is as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">receiver-service</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">port</span>: <span style="color:#ae81ff">8000</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">8000</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">receiver</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">type</span>: <span style="color:#ae81ff">NodePort</span>
</span></span></code></pre></div><p>For a fixed nodePort on 8000 a definition of <code>nodePort</code> must be provided:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">receiver-service</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">port</span>: <span style="color:#ae81ff">8000</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">8000</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">receiver</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">type</span>: <span style="color:#ae81ff">NodePort</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">nodePort</span>: <span style="color:#ae81ff">8000</span>
</span></span></code></pre></div><h3 id="image-processor-1">Image processor<a hidden class="anchor" aria-hidden="true" href="#image-processor-1">#</a></h3>
<p>The Image Processor is where I&rsquo;m handling passing off images to be identified. It should have access to nsqlookupd, mysql and the gRPC endpoint of the face recognition service. This is actually quite a boring service. In fact, it&rsquo;s not even a service at all. It doesn&rsquo;t expose anything, and thus it&rsquo;s the first deployment only component. For brevity, here is the whole template:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">image-processor-deployment</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">matchLabels</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">image-processor</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">template</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">app</span>: <span style="color:#ae81ff">image-processor</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">image-processor</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">skarlso/kube-processor-alpine:latest</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">env</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">MYSQL_CONNECTION</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#34;mysql.default.svc.cluster.local&#34;</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">MYSQL_USERPASSWORD</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">valueFrom</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">secretKeyRef</span>:
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">name</span>: <span style="color:#ae81ff">kube-face-secret</span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">key</span>: <span style="color:#ae81ff">mysql_userpassword</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">MYSQL_PORT</span>
</span></span><span style="display:flex;"><span>          <span style="color:#75715e"># TIL: If this is 3306 without &#34; kubectl throws an error.</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#34;3306&#34;</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">MYSQL_DBNAME</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">value</span>: <span style="color:#ae81ff">kube</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">NSQ_LOOKUP_ADDRESS</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#34;nsqlookup.default.svc.cluster.local:4161&#34;</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">GRPC_ADDRESS</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#34;face-recog.default.svc.cluster.local:50051&#34;</span>
</span></span></code></pre></div><p>The only interesting points in this file are the multitude of environment properties that are used to configure the application. Note the nsqlookupd address and the grpc address.</p>
<p>To create this deployment, run:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl apply -f image_processor.yaml
</span></span></code></pre></div><h3 id="face---recognition">Face - Recognition<a hidden class="anchor" aria-hidden="true" href="#face---recognition">#</a></h3>
<p>The face recognition service does have a service. It&rsquo;s a simple one. Only needed by image-processor. It&rsquo;s template is as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">face-recog</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">port</span>: <span style="color:#ae81ff">50051</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">50051</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">face-recog</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">clusterIP</span>: <span style="color:#ae81ff">None</span>
</span></span></code></pre></div><p>The more interesting part is that it requires two volumes. The two volumes are <code>known_people</code> and <code>unknown_people</code>. Can you guess what they will contain? Yep, images. The <code>known_people</code> volume contains all the images associated to the known people in the database. The <code>unknown_people</code> volume will contain all new images. And that&rsquo;s the path we will need to use when sending images from the receiver; that is wherever the mount point points too, which in my case is <code>/unknown_people</code>. Basically, the path needs to be one that the face recognition service can access.</p>
<p>Now, with Kubernetes and Docker, this is easy. It can be a mounted S3 or some kind of nfs or a local mount from host to guest. The possibilities are endless (around a dozen or so). I&rsquo;m going to use a local mount for the sake of simplicity.</p>
<p>Mounting a volume is done in two parts. Firstly, the Dockerfile has to specify volumes:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Dockerfile" data-lang="Dockerfile"><span style="display:flex;"><span><span style="color:#66d9ef">VOLUME</span> [ <span style="color:#e6db74">&#34;/unknown_people&#34;</span>, <span style="color:#e6db74">&#34;/known_people&#34;</span> ]<span style="color:#960050;background-color:#1e0010">
</span></span></span></code></pre></div><p>Secondly, the Kubernetes template needs add <code>volumeMounts</code> as seen in the MySQL service; the difference being <code>hostPath</code> instead of claimed volume:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>        <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">known-people-storage</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/known_people</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">unknown-people-storage</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/unknown_people</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">known-people-storage</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">hostPath</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/Users/hannibal/Temp/known_people</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">type</span>: <span style="color:#ae81ff">Directory</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">unknown-people-storage</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">hostPath</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/Users/hannibal/Temp/</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">type</span>: <span style="color:#ae81ff">Directory</span>
</span></span></code></pre></div><p>We also need to set the <code>known_people</code> folder config setting for the face recognition service. This is done via an environment property:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>        <span style="color:#f92672">env</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">KNOWN_PEOPLE</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#34;/known_people&#34;</span>
</span></span></code></pre></div><p>Then the Python code will look up images, like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>        known_people <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#39;KNOWN_PEOPLE&#39;</span>, <span style="color:#e6db74">&#39;known_people&#39;</span>)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;Known people images location is: </span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> known_people)
</span></span><span style="display:flex;"><span>        images <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>image_files_in_folder(known_people)
</span></span></code></pre></div><p>Where <code>image_files_in_folder</code> is:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">image_files_in_folder</span>(self, folder):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> [os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(folder, f) <span style="color:#66d9ef">for</span> f <span style="color:#f92672">in</span> os<span style="color:#f92672">.</span>listdir(folder) <span style="color:#66d9ef">if</span> re<span style="color:#f92672">.</span><span style="color:#66d9ef">match</span>(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;.*\.(jpg|jpeg|png)&#39;</span>, f, flags<span style="color:#f92672">=</span>re<span style="color:#f92672">.</span>I)]
</span></span></code></pre></div><p>Neat.</p>
<p>Now, if the receiver receives a request (and sends it off further down the line) similar to the one below&hellip;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -d <span style="color:#e6db74">&#39;{&#34;path&#34;:&#34;/unknown_people/unknown220.jpg&#34;}&#39;</span> http://192.168.99.100:30251/image/post
</span></span></code></pre></div><p>&hellip;it will look for an image called unknown220.jpg under <code>/unknown_people</code>, locate an image in the known_folder that corresponds to the person in the unknown image and return the name of the image that matches.</p>
<p>Looking at logs, you should see something like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Receiver</span>
</span></span><span style="display:flex;"><span>❯ curl -d <span style="color:#e6db74">&#39;{&#34;path&#34;:&#34;/unknown_people/unknown219.jpg&#34;}&#39;</span> http://192.168.99.100:30251/image/post
</span></span><span style="display:flex;"><span>got path: <span style="color:#f92672">{</span>Path:/unknown_people/unknown219.jpg<span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>image saved with id: <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>image sent to nsq
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Image Processor</span>
</span></span><span style="display:flex;"><span>2018/03/26 18:11:21 INF    <span style="color:#ae81ff">1</span> <span style="color:#f92672">[</span>images/ch<span style="color:#f92672">]</span> querying nsqlookupd http://nsqlookup.default.svc.cluster.local:4161/lookup?topic<span style="color:#f92672">=</span>images
</span></span><span style="display:flex;"><span>2018/03/26 18:11:59 Got a message: <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>2018/03/26 18:11:59 Processing image id:  <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>2018/03/26 18:12:00 got person:  Hannibal
</span></span><span style="display:flex;"><span>2018/03/26 18:12:00 updating record with person id
</span></span><span style="display:flex;"><span>2018/03/26 18:12:00 <span style="color:#66d9ef">done</span>
</span></span></code></pre></div><p>And that concludes all of the services that we need to deploy.</p>
<h3 id="frontend">Frontend<a hidden class="anchor" aria-hidden="true" href="#frontend">#</a></h3>
<p>Lastly, there is a small web-app which displays the information in the db for convenience. This is also a public facing service with the same parameters as the receiver.</p>
<p>It looks like this:</p>
<p><img alt="frontend" loading="lazy" src="/img/kube-frontend.png"></p>
<h3 id="recap">Recap<a hidden class="anchor" aria-hidden="true" href="#recap">#</a></h3>
<p>We are now at the point in which I’ve deployed a bunch of services. A recap off the commands I’ve used so far:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl apply -f mysql.yaml
</span></span><span style="display:flex;"><span>kubectl apply -f nsqlookup.yaml
</span></span><span style="display:flex;"><span>kubectl apply -f receiver.yaml
</span></span><span style="display:flex;"><span>kubectl apply -f image_processor.yaml
</span></span><span style="display:flex;"><span>kubectl apply -f face_recognition.yaml
</span></span><span style="display:flex;"><span>kubectl apply -f frontend.yaml
</span></span></code></pre></div><p>These could be in any order since the application does not allocate connections on start. (Except for image_processor&rsquo;s NSQ consumer. But that re-tries.)</p>
<p>Query-ing kube for running pods with <code>kubectl get pods</code> should show something like this if there were no errors:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>❯ kubectl get pods
</span></span><span style="display:flex;"><span>NAME                                          READY     STATUS    RESTARTS   AGE
</span></span><span style="display:flex;"><span>face-recog-6bf449c6f-qg5tr                    1/1       Running   <span style="color:#ae81ff">0</span>          1m
</span></span><span style="display:flex;"><span>image-processor-deployment-6467468c9d-cvx6m   1/1       Running   <span style="color:#ae81ff">0</span>          31s
</span></span><span style="display:flex;"><span>mysql-7d667c75f4-bwghw                        1/1       Running   <span style="color:#ae81ff">0</span>          36s
</span></span><span style="display:flex;"><span>nsqd-584954c44c-299dz                         1/1       Running   <span style="color:#ae81ff">0</span>          26s
</span></span><span style="display:flex;"><span>nsqlookup-7f5bdfcb87-jkdl7                    1/1       Running   <span style="color:#ae81ff">0</span>          11s
</span></span><span style="display:flex;"><span>receiver-deployment-5cb4797598-sf5ds          1/1       Running   <span style="color:#ae81ff">0</span>          26s
</span></span></code></pre></div><p>Running <code>minikube service list</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>❯ minikube service list
</span></span><span style="display:flex;"><span>|-------------|----------------------|-----------------------------|
</span></span><span style="display:flex;"><span>|  NAMESPACE  |         NAME         |             URL             |
</span></span><span style="display:flex;"><span>|-------------|----------------------|-----------------------------|
</span></span><span style="display:flex;"><span>| default     | face-recog           | No node port                |
</span></span><span style="display:flex;"><span>| default     | kubernetes           | No node port                |
</span></span><span style="display:flex;"><span>| default     | mysql                | No node port                |
</span></span><span style="display:flex;"><span>| default     | nsqd                 | No node port                |
</span></span><span style="display:flex;"><span>| default     | nsqlookup            | No node port                |
</span></span><span style="display:flex;"><span>| default     | receiver-service     | http://192.168.99.100:30251 |
</span></span><span style="display:flex;"><span>| kube-system | kube-dns             | No node port                |
</span></span><span style="display:flex;"><span>| kube-system | kubernetes-dashboard | http://192.168.99.100:30000 |
</span></span><span style="display:flex;"><span>|-------------|----------------------|-----------------------------|
</span></span></code></pre></div><h3 id="rolling-update">Rolling update<a hidden class="anchor" aria-hidden="true" href="#rolling-update">#</a></h3>
<p>What happens during a rolling update?</p>
<p><img alt="kube rotate" loading="lazy" src="/img/kube_rotate.png"></p>
<p>As it happens during software development, change is requested/needed to some parts of the system. So what happens to our cluster if I change one of its components without breaking the others whilst also maintaining backwards compatibility with no disruption to user experience? Thankfully Kubernetes can help with that.</p>
<p>What I don&rsquo;t like is that the API only handles one image at a time. Unfortunately there is no bulk upload option.</p>
<h4 id="code">Code<a hidden class="anchor" aria-hidden="true" href="#code">#</a></h4>
<p>Currently, we have the following code segment dealing with a single image:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-go" data-lang="go"><span style="display:flex;"><span><span style="color:#75715e">// PostImage handles a post of an image. Saves it to the database
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// and sends it to NSQ for further processing.
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">func</span> <span style="color:#a6e22e">PostImage</span>(<span style="color:#a6e22e">w</span> <span style="color:#a6e22e">http</span>.<span style="color:#a6e22e">ResponseWriter</span>, <span style="color:#a6e22e">r</span> <span style="color:#f92672">*</span><span style="color:#a6e22e">http</span>.<span style="color:#a6e22e">Request</span>) {
</span></span><span style="display:flex;"><span><span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">func</span> <span style="color:#a6e22e">main</span>() {
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">router</span> <span style="color:#f92672">:=</span> <span style="color:#a6e22e">mux</span>.<span style="color:#a6e22e">NewRouter</span>()
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">router</span>.<span style="color:#a6e22e">HandleFunc</span>(<span style="color:#e6db74">&#34;/image/post&#34;</span>, <span style="color:#a6e22e">PostImage</span>).<span style="color:#a6e22e">Methods</span>(<span style="color:#e6db74">&#34;POST&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">log</span>.<span style="color:#a6e22e">Fatal</span>(<span style="color:#a6e22e">http</span>.<span style="color:#a6e22e">ListenAndServe</span>(<span style="color:#e6db74">&#34;:8000&#34;</span>, <span style="color:#a6e22e">router</span>))
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>We have two options: Add a new endpoint with <code>/images/post</code> and make the client use that, or modify the existing one.</p>
<p>The new client code has the advantage in that it can fall back to submitting the old way if the new endpoint isn&rsquo;t available. The old client code, however, doesn&rsquo;t have this advantage so we can&rsquo;t change the way our code works right now. Consider this: You have 90 servers and you do a slow paced rolling update that will take out servers one step at a time whilst doing an update. If an update lasts around a minute, the whole process will take around one and a half hours to complete, (not counting any parallel updates).</p>
<p>During this time, some of your servers will run the new code and some will run the old one. Calls are load balanced, thus you have no control over which servers will be hit. If a client is trying to do a call the new way but hits an old server, the client will fail. The client can try and fallback, but since you eliminated the old version it will not succeed unless it, by mere chance, hits a server with the new code (assuming no sticky sessions are set).</p>
<p>Also, once all your servers are updated, an old client will not be able to use your service any longer.</p>
<p>Now, you can argue that you don&rsquo;t want to keep old versions of your code forever. And that’s true in a sense. That&rsquo;s why we are going to modify the old code to simply call the new one with some slight augmentations. This way, once all clients have been migrated, the code can simply be deleted without any problems.</p>
<h4 id="new-endpoint">New Endpoint<a hidden class="anchor" aria-hidden="true" href="#new-endpoint">#</a></h4>
<p>Let&rsquo;s add a new route method:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-go" data-lang="go"><span style="display:flex;"><span><span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">router</span>.<span style="color:#a6e22e">HandleFunc</span>(<span style="color:#e6db74">&#34;/images/post&#34;</span>, <span style="color:#a6e22e">PostImages</span>).<span style="color:#a6e22e">Methods</span>(<span style="color:#e6db74">&#34;POST&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#f92672">...</span>
</span></span></code></pre></div><p>Updating the old one to call the new one with a modified body looks like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-go" data-lang="go"><span style="display:flex;"><span><span style="color:#75715e">// PostImage handles a post of an image. Saves it to the database
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// and sends it to NSQ for further processing.
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">func</span> <span style="color:#a6e22e">PostImage</span>(<span style="color:#a6e22e">w</span> <span style="color:#a6e22e">http</span>.<span style="color:#a6e22e">ResponseWriter</span>, <span style="color:#a6e22e">r</span> <span style="color:#f92672">*</span><span style="color:#a6e22e">http</span>.<span style="color:#a6e22e">Request</span>) {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">var</span> <span style="color:#a6e22e">p</span> <span style="color:#a6e22e">Path</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">err</span> <span style="color:#f92672">:=</span> <span style="color:#a6e22e">json</span>.<span style="color:#a6e22e">NewDecoder</span>(<span style="color:#a6e22e">r</span>.<span style="color:#a6e22e">Body</span>).<span style="color:#a6e22e">Decode</span>(<span style="color:#f92672">&amp;</span><span style="color:#a6e22e">p</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> <span style="color:#a6e22e">err</span> <span style="color:#f92672">!=</span> <span style="color:#66d9ef">nil</span> {
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">fmt</span>.<span style="color:#a6e22e">Fprintf</span>(<span style="color:#a6e22e">w</span>, <span style="color:#e6db74">&#34;got error while decoding body: %s&#34;</span>, <span style="color:#a6e22e">err</span>)
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">return</span>
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">fmt</span>.<span style="color:#a6e22e">Fprintf</span>(<span style="color:#a6e22e">w</span>, <span style="color:#e6db74">&#34;got path: %+v\n&#34;</span>, <span style="color:#a6e22e">p</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">var</span> <span style="color:#a6e22e">ps</span> <span style="color:#a6e22e">Paths</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">paths</span> <span style="color:#f92672">:=</span> make([]<span style="color:#a6e22e">Path</span>, <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">paths</span> = append(<span style="color:#a6e22e">paths</span>, <span style="color:#a6e22e">p</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">ps</span>.<span style="color:#a6e22e">Paths</span> = <span style="color:#a6e22e">paths</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">var</span> <span style="color:#a6e22e">pathsJSON</span> <span style="color:#a6e22e">bytes</span>.<span style="color:#a6e22e">Buffer</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">err</span> = <span style="color:#a6e22e">json</span>.<span style="color:#a6e22e">NewEncoder</span>(<span style="color:#f92672">&amp;</span><span style="color:#a6e22e">pathsJSON</span>).<span style="color:#a6e22e">Encode</span>(<span style="color:#a6e22e">ps</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> <span style="color:#a6e22e">err</span> <span style="color:#f92672">!=</span> <span style="color:#66d9ef">nil</span> {
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">fmt</span>.<span style="color:#a6e22e">Fprintf</span>(<span style="color:#a6e22e">w</span>, <span style="color:#e6db74">&#34;failed to encode paths: %s&#34;</span>, <span style="color:#a6e22e">err</span>)
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">return</span>
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">r</span>.<span style="color:#a6e22e">Body</span> = <span style="color:#a6e22e">ioutil</span>.<span style="color:#a6e22e">NopCloser</span>(<span style="color:#f92672">&amp;</span><span style="color:#a6e22e">pathsJSON</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">r</span>.<span style="color:#a6e22e">ContentLength</span> = int64(<span style="color:#a6e22e">pathsJSON</span>.<span style="color:#a6e22e">Len</span>())
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">PostImages</span>(<span style="color:#a6e22e">w</span>, <span style="color:#a6e22e">r</span>)
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Well, the naming could be better, but you should get the basic idea. I&rsquo;m modifying the incoming single path by wrapping it into the new format and sending it over to the new endpoint handler. And that&rsquo;s it! There are a few more modifications. To check them out, take a look at this PR: <a href="https://github.com/Skarlso/kube-cluster-sample/pull/1">Rolling Update Bulk Image Path PR</a>.</p>
<p>Now, the receiver can be called in two ways:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Single Path:</span>
</span></span><span style="display:flex;"><span>curl -d <span style="color:#e6db74">&#39;{&#34;path&#34;:&#34;unknown4456.jpg&#34;}&#39;</span> http://127.0.0.1:8000/image/post
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Multiple Paths:</span>
</span></span><span style="display:flex;"><span>curl -d <span style="color:#e6db74">&#39;{&#34;paths&#34;:[{&#34;path&#34;:&#34;unknown4456.jpg&#34;}]}&#39;</span> http://127.0.0.1:8000/images/post
</span></span></code></pre></div><p>Here, the client is curl. Normally, if the client is a service, I would modify it that in case the new end-point throws a 404 it would try the old one next.</p>
<p>For brevity, I&rsquo;m not modifying NSQ and the others to handle bulk image processing; they will still receive it one by one. I&rsquo;ll leave that up to you as homework ;)</p>
<h4 id="new-image">New Image<a hidden class="anchor" aria-hidden="true" href="#new-image">#</a></h4>
<p>To perform a rolling update, I must create a new image first from the receiver service.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker build -t skarlso/kube-receiver-alpine:v1.1 .
</span></span></code></pre></div><p>Once this is complete, we can begin rolling out the change.</p>
<h4 id="rolling-update-1">Rolling update<a hidden class="anchor" aria-hidden="true" href="#rolling-update-1">#</a></h4>
<p>In Kubernetes, you can configure your rolling update in multiple ways:</p>
<h5 id="manual-update">Manual Update<a hidden class="anchor" aria-hidden="true" href="#manual-update">#</a></h5>
<p>If I was using a container version in my config file called <code>v1.0</code>, then doing an update is simply calling:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl rolling-update receiver --image:skarlso/kube-receiver-alpine:v1.1
</span></span></code></pre></div><p>If there is a problem during the rollout we can always rollback.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl rolling-update receiver --rollback
</span></span></code></pre></div><p>It will set back the previous version. No fuss, no muss.</p>
<h5 id="apply-a-new-configuration-file">Apply a new configuration file<a hidden class="anchor" aria-hidden="true" href="#apply-a-new-configuration-file">#</a></h5>
<p>The problem with by-hand updates is that they aren&rsquo;t in source control.</p>
<p>Consider this: Something has changed, A couple of servers got updated by hand to do a quick “patch fix”, but nobody witnessed it and it wasn’t documented. A new person comes along and does a change to the template and applies the template to the cluster. All the servers are updated, and then all of a sudden there is a service outage.</p>
<p>Long story short, the servers which got updated are written over because the template doesn’t  reflect what has been done manually.</p>
<p>The recommended way is to change the template in order to use the new version, and than apply the template with the <code>apply</code> command.</p>
<p>Kubernetes recommends that a Deployment with ReplicaSets should handle a rollout. This means there must be at least two replicates present for a rolling update. If less than two replicates are present then the update won&rsquo;t work (unless <code>maxUnavailable</code> is set to 1). I increase the replica count in yaml. I also set the new image version for the receiver container.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">receiver</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">skarlso/kube-receiver-alpine:v1.1</span>
</span></span><span style="display:flex;"><span>...
</span></span></code></pre></div><p>Looking at the progress, this is what you should see :</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>❯ kubectl rollout status deployment/receiver-deployment
</span></span><span style="display:flex;"><span>Waiting <span style="color:#66d9ef">for</span> rollout to finish: <span style="color:#ae81ff">1</span> out of <span style="color:#ae81ff">2</span> new replicas have been updated...
</span></span></code></pre></div><p>You can add in additional rollout configuration settings by specifying the <code>strategy</code> part of the template like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>  <span style="color:#f92672">strategy</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">type</span>: <span style="color:#ae81ff">RollingUpdate</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">rollingUpdate</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">maxSurge</span>: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">maxUnavailable</span>: <span style="color:#ae81ff">0</span>
</span></span></code></pre></div><p>Additional information on rolling update can be found in the below documents: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-a-deployment">Deployment Rolling Update</a>, <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment">Updating a Deployment</a>, <a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#updating-your-application-without-a-service-outage">Manage Deployments</a>, <a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/">Rolling Update using ReplicaController</a>.</p>
<p><strong>NOTE MINIKUBE USERS</strong>: Since we are doing this on a local machine with one node and 1 replica of an application, we have to set <code>maxUnavailable</code> to <code>1</code>; otherwise Kubernetes won&rsquo;t allow the update to happen, and the new version will remain in <code>Pending</code> state. That’s because we aren’t allowing for a services to exist with no running containers; which basically means service outage.</p>
<h3 id="scaling-1">Scaling<a hidden class="anchor" aria-hidden="true" href="#scaling-1">#</a></h3>
<p>Scaling is dead easy with Kubernetes. Since it&rsquo;s managing the whole cluster, you basically just need to put a number into the template of the desired replicas to use.</p>
<p>This has been a great post so far, but it&rsquo;s getting too long. I&rsquo;m planning on writing a follow-up where I will be truly scaling things up on AWS with multiple nodes and replicas; plus deploying a Kubernetes cluster with <a href="https://github.com/kubernetes/kops">Kops</a>. So stay tuned!</p>
<h3 id="cleanup">Cleanup<a hidden class="anchor" aria-hidden="true" href="#cleanup">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl delete deployments --all
</span></span><span style="display:flex;"><span>kubectl delete services -all
</span></span></code></pre></div><h1 id="final-words">Final Words<a hidden class="anchor" aria-hidden="true" href="#final-words">#</a></h1>
<p>And that’s it ladies and gentlemen. We wrote, deployed, updated and scaled (well, not yet really) a distributed application with Kubernetes.</p>
<p>If you have any questions, please feel free to chat in the comments below. I&rsquo;m happy to answer.</p>
<p>I hope you’ve enjoyed reading this. I know it&rsquo;s quite long; I was thinking of splitting it up multiple posts, but having a cohesive, one page guide is useful and makes it easy to find, save, and print.</p>
<p>Thank you for reading,
Gergely.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://skarlso.github.io/2018/06/08/fork-updater/">
    <span class="title">« Prev</span>
    <br>
    <span>Keep your git forks updated all the time</span>
  </a>
  <a class="next" href="https://skarlso.github.io/2018/02/06/go-budapest-meetup/">
    <span class="title">Next »</span>
    <br>
    <span>Go Budapest Meetup</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://skarlso.github.io/">Ramblings of a cloud engineer</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
