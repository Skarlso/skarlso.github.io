<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Using a Kubernetes based Cluster for Various Services with auto HTTPS | Ramblings of a cloud engineer</title>
<meta name="keywords" content="">
<meta name="description" content="Intro
Hi folks.
Today, I would like to show you how my infrastructure is deployed and managed. Spoiler alert, I&rsquo;m using Kubernetes to do that.
I know&hellip; What a twist!
Let&rsquo;s get to it.
What

What services am I running exactly? Here is a list I&rsquo;m running at the time of this writing:

Athens Go Proxy
Gitea
The Lounge (IRC bouncer)
Two CronJobs

Fork Updater
IDLE RPG online checker


My WebSite (gergelybrautigam.com)
Monitoring

And it&rsquo;s really simple to add more.">
<meta name="author" content="hannibal">
<link rel="canonical" href="https://skarlso.github.io/2019/09/21/kubernetes-cluster/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://skarlso.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://skarlso.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://skarlso.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://skarlso.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://skarlso.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://skarlso.github.io/2019/09/21/kubernetes-cluster/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Using a Kubernetes based Cluster for Various Services with auto HTTPS">
<meta property="og:description" content="Intro
Hi folks.
Today, I would like to show you how my infrastructure is deployed and managed. Spoiler alert, I&rsquo;m using Kubernetes to do that.
I know&hellip; What a twist!
Let&rsquo;s get to it.
What

What services am I running exactly? Here is a list I&rsquo;m running at the time of this writing:

Athens Go Proxy
Gitea
The Lounge (IRC bouncer)
Two CronJobs

Fork Updater
IDLE RPG online checker


My WebSite (gergelybrautigam.com)
Monitoring

And it&rsquo;s really simple to add more.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://skarlso.github.io/2019/09/21/kubernetes-cluster/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2019-09-21T21:01:00+01:00">
<meta property="article:modified_time" content="2019-09-21T21:01:00+01:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Using a Kubernetes based Cluster for Various Services with auto HTTPS">
<meta name="twitter:description" content="Intro
Hi folks.
Today, I would like to show you how my infrastructure is deployed and managed. Spoiler alert, I&rsquo;m using Kubernetes to do that.
I know&hellip; What a twist!
Let&rsquo;s get to it.
What

What services am I running exactly? Here is a list I&rsquo;m running at the time of this writing:

Athens Go Proxy
Gitea
The Lounge (IRC bouncer)
Two CronJobs

Fork Updater
IDLE RPG online checker


My WebSite (gergelybrautigam.com)
Monitoring

And it&rsquo;s really simple to add more.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://skarlso.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Using a Kubernetes based Cluster for Various Services with auto HTTPS",
      "item": "https://skarlso.github.io/2019/09/21/kubernetes-cluster/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Using a Kubernetes based Cluster for Various Services with auto HTTPS",
  "name": "Using a Kubernetes based Cluster for Various Services with auto HTTPS",
  "description": "Intro Hi folks.\nToday, I would like to show you how my infrastructure is deployed and managed. Spoiler alert, I\u0026rsquo;m using Kubernetes to do that.\nI know\u0026hellip; What a twist!\nLet\u0026rsquo;s get to it.\nWhat What services am I running exactly? Here is a list I\u0026rsquo;m running at the time of this writing:\nAthens Go Proxy Gitea The Lounge (IRC bouncer) Two CronJobs Fork Updater IDLE RPG online checker My WebSite (gergelybrautigam.com) Monitoring And it\u0026rsquo;s really simple to add more.\n",
  "keywords": [
    
  ],
  "articleBody": "Intro Hi folks.\nToday, I would like to show you how my infrastructure is deployed and managed. Spoiler alert, I’m using Kubernetes to do that.\nI know… What a twist!\nLet’s get to it.\nWhat What services am I running exactly? Here is a list I’m running at the time of this writing:\nAthens Go Proxy Gitea The Lounge (IRC bouncer) Two CronJobs Fork Updater IDLE RPG online checker My WebSite (gergelybrautigam.com) Monitoring And it’s really simple to add more.\nWhere My cluster is deployed at DigitalOcean using two droplets each 1vCPU and 2GB RAM.\nWhat Not This isn’t going to be a production grade cluster. What I don’t include in here:\nRBAC for various services and users Since I’m the only user of my cluster I didn’t create any kind of access limits / users or such. You are free to create them though. The only role based auth that’s going on is for Prometheus.\nI’m not using any third party things which require access to the API.\nResource limitation I’m the sole user of my things. I’m not really scaling my gitea up or down based on usage and as such, I did not define things like:\nResource limits Nodes with certain capabilities Affinities and Taints – which means, everything can run anywhere Readiness Liveliness Most of by deploys and services don’t have these except for Athens.\nHow Okay, with that out of the way, let’s get into the hows of things…\nBeginning The most important thing that you need to do in order to use Kubernetes is Containerizing all the things.\nSince Kubernetes is a container orchestration tool, without containers it’s pretty useless.\nAs a driver, I’m going to use Docker. Kubernetes can use anything that’s OCI compatible, which means if you would like to use runc as a container engine, you can do that. I’d like to keep my sanity though.\nExample To show you what I mean… I have a cronjob which is running every month. It gathers all my forks on github and updates them with the latest from their parents. This a small ruby script located here: Fork Updater. How do we run this? It requires two things. First, a token. We pass that currently as an environment property. It could be in a file in a vault or a secret mounted in as a file it doesn’t matter. Currently, it’s an environment property. The second thing is more subtle.\nI’m pushing the changes back into my remote forks. I’m doing this via SSH. So, we need a key in there too. How we’ll get that in there, I’ll talk about later when we are talking about how to set this cron job up. For now though, the container needs to look for a key in a specific location because we don’t want to over-mount /root/.ssh/ and we also don’t want to use an initContainer to copy over an SSH key (because it’s mounted in as a symlink (but that’s a different issue all together)). Also, we certianly do NOT want to have a key in the container.\nTo achieve this, we simply set up a config file for SSH like this one:\nHost github.com IdentityFile /etc/secret/id_rsa /etc/secret will be the destination of the ssh key we create.\nAnd we also need to have a known_hosts file, otherwise git clone will complain. We also bake this into the container. Why? Why not generate that on the fly? Because I want it to fail in case there is something wrong or there is a MIMA going on etc.\nAll this translated into a Dockerfile looks like this:\n# We are using alpine for a minimalistic image FROM alpine:latest RUN apk --no-cache add ca-certificates RUN apk update # Openssh is needed for the SSH command RUN apk --no-cache add ruby vim curl git build-base ruby-dev openssh openssh-client # Setup dependencies for the fork ruby file RUN gem install octokit logger multi_json json --no-ri --no-rdoc RUN mkdir /data WORKDIR /data # Setup some data about the committer RUN git config --global user.name \"Fork Updater\" RUN git config --global user.email \"email@email.com\" RUN mkdir -p /root/.ssh # Get the host config for github.com RUN ssh-keyscan github.com \u003e\u003e /root/.ssh/known_hosts # Setup the SSH config COPY ./config /root/.ssh COPY ./fork_updater.rb . CMD [\"ruby\", \"/data/fork_updater.rb\"] That’s it. Now our updater is containerized and ready to be deployed as a cronjob on a kube cluster. Oh, and we also need to create the SSH key like this:\nkubectl create secret generic ssh-key-secret --from-file=ssh-privatekey=/path/to/.ssh/id_rsa Before we Begin There are two things we will need though to set up for our cluster before we even begin adding the first service. And that’s an ingress with a load-balancer and cert-manager.\nCert-Manager Now, you have the option of installing cert-manager via helm, or via the provided kube config yaml file. I STRONGLY recommend using the config yaml file because the upgrading process with helm is a hell of a lot dirtier / failure prone than simply applying a new yaml file with a different version in it.\nEither way, to install cert-manager follow this simple guide: Cert-manager Install Manual.\nIngress An Ingress is a must. This is the component which manages external access to the services which we will define. Like a proxy before your http server. This component will handle the hostname based routing between our services.\nI’m using nginx ingress, although there are a couple of implementations out there.\nTo install nginx ingress, follow their guide here: Installing Nginx-Ingress.\nFrom Easy to Complicated Alright. Now that we have the prereqs out of the way, let’s get our hands dirty. I’ll start with the easiest of them all, my web site, and then will progress towards the more complicated ones, like Gitea and Athens, which require a lot more fiddling and have more moving parts.\nMy Website The site, located here: Gergely’s Domain; is a really simple, static, Hugo based website. It contains nothing fancy, no real Javascript magic, has a simple list of things I’ve done and who I am.\nIt’s powered / served by an nginx instance running on port 9090 define by a very simple Dockerfile:\nFROM golang:latest as builder RUN apt-get update \u0026\u0026 apt install -y git make vim hugo RUN mkdir -p /opt/website RUN git clone https://github.com/Skarlso/gergelybrautigam /opt/website WORKDIR /opt/website RUN make FROM nginx:latest RUN mkdir -p /var/www/html/gergelybrautigam WORKDIR /var/www/html/gergelybrautigam COPY --from=builder /opt/website/public . COPY 01_gergelybrautigam /etc/nginx/sites-available/ RUN mkdir -p /etc/nginx/sites-enabled/ RUN ln -s /etc/nginx/sites-available/01_gergelybrautigam /etc/nginx/sites-enabled/01_gergelybrautigam Easy as goblin pie. Nginx has a command set like this CMD [\"nginx\", \"-g\", \"daemon off;\"] and exposes port 80.\nThe deployment In order to deploy this in the cluster, I created a deployment as follows:\napiVersion: apps/v1 kind: Deployment metadata: name: gb-deployment namespace: gergely-brautigam labels: app: gergelybrautigam annotations: prometheus.io/scrape: 'true' prometheus.io/port: '9090' spec: replicas: 1 selector: matchLabels: app: gergelybrautigam template: metadata: labels: app: gergelybrautigam annotations: prometheus.io/scrape: 'true' prometheus.io/port: '9090' spec: containers: - name: gergelybrautigam image: skarlso/gergelybrautigam:v0.0.26 ports: - containerPort: 9090 The metadata section defines information about the deployment. It’s name is gb-deployment. The namespace in which this sits is called gergely-brautigam and it has some labels to it so Prometheus monitoring tool can discover the pod.\nIt’s running a single replica, has a bunch of more metadata and template settings, and finally the container spec which defines the image, and the exposed container port on which the application is running.\nNow we need a service to expose this deployment.\nThe service The service is also simple. It looks like this:\nkind: Service apiVersion: v1 metadata: namespace: gergely-brautigam name: gb-service spec: selector: app: gergelybrautigam ports: - port: 80 targetPort: 9090 Again, nothing fancy here, just a simple service exposing a port to a different port on the front-end side. This service will be picked up by our previously created routing facility.\nIngress Now that we have the service we need to expose it to the domain. I have the domain gergelybrautigam.com and I already pointed it at the LoadBalancer’s IP which was created by the nginx ingress controller.\nWe only want one LoadBalancer, but we have multiple hostnames. We can achieve that by creating an Ingress resource in the namespace our service is in like this:\napiVersion: extensions/v1beta1 kind: Ingress metadata: namespace: gergely-brautigam name: gergely-brautigam-ingress annotations: kubernetes.io/ingress.class: \"nginx\" certmanager.k8s.io/cluster-issuer: \"letsencrypt-prod\" certmanager.k8s.io/acme-challenge-type: http01 nginx.ingress.kubernetes.io/rewrite-target: / spec: tls: - hosts: - gergelybrautigam.com secretName: gergelybrautigam-tls rules: - host: gergelybrautigam.com http: paths: - backend: serviceName: gb-service servicePort: 80 Remember, we already have the nginx ingress resource in the default namespace when we installed the controller previously. That is the main entrypoint. We are taking advantage of the rewrite-target annotation. That is our key to success nginx.ingress.kubernetes.io/rewrite-target: /. The rest is basic routing. We’ll have something like this in the other namespace to.\nAnd with that, our website is done and it should be working under HTTPS. Cert-manager should have picked it up and generated a certificate for it. Let’s check.\nRunning k get certs -n gergely-brautigam you should see something like this:\n$ k get certs -n gergely-brautigam NAME READY SECRET AGE gergelybrautigam-tls True gergelybrautigam-tls 86d If there is a problem, just describe the cert resource and look for the generated challenge and if it was successful or not. The challenge contains mostly good error messages.\nIRC bouncer That wasn’t too bad, right? Let’s do something a bit more complex this time. We are going to deploy The lounge irc bouncer.\nIt’s actually quite easy to do but can be daunting to look at at first.\nThe container Lucky for us, the bouncer already provides a container located here: The Lounge Docker Hub.\nWe just need two things. To expose the port 9000 and to give it something called a PersistentVolume. What’s a persistent volume? Well, look it up here: Kubernetes Persistent Volumes.\nTL;DR: We need to preserve data. Containers are ephemeral in nature. Meaning if there is a problem we usually just delete the pod. Which means that all data will be lost. But we need persistence in this case because we’ll have user data and user information which we would like to persist between pods. That’s what a volume is for.\nIt will be mounted into the pod so we can point the bouncer to use that location for data management.\nPVC With that, this is how our PersistentVolumeClaim will look like:\napiVersion: v1 kind: PersistentVolumeClaim metadata: namespace: powerhouse name: do-storage-irc spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: do-block-storage DigitalOcean provides a block storage implementation for this claim so we use that storage class do-block-storage.\nDeployment With that, this is how the deployment will look like:\napiVersion: apps/v1 kind: Deployment metadata: namespace: powerhouse name: irc-app labels: app: irc spec: replicas: 1 selector: matchLabels: app: irc template: metadata: labels: app: irc app.kubernetes.io/name: irc app.kubernetes.io/instance: irc spec: containers: - name: irc image: thelounge/thelounge:3.1.1 ports: - containerPort: 9000 name: irc-http volumeMounts: - mountPath: /var/opt/thelounge subPath: thelounge name: irc-data readOnly: false volumes: - name: irc-data persistentVolumeClaim: claimName: do-storage-irc Short and sweet. The important bits are the labels, those are used by cert-manager and ingress to find the right deployment, and the volumeMounts. We mount into the /var/opt/thelounge folder because that’s the main configuration location. The subPath is important for a correct mounting.\nThe service Alright, with the deployment in place, let’s take a look at the service.\nkind: Service apiVersion: v1 metadata: namespace: powerhouse name: irc labels: app: irc app.kubernetes.io/name: irc app.kubernetes.io/instance: irc spec: selector: app: irc app.kubernetes.io/name: irc app.kubernetes.io/instance: irc ports: - name: http port: 9000 targetPort: irc-http Again, very boring stuff. Boring is good. Boring is predictable. We expose port 9000 to the named targetPort called irc-http which we defined in the above deployment.\nNow, I have a domain in which these things are running, let’s call it powerhouse.com (because I’m tired of example.com). And I have multiple services in this namespace too, so I’ll call the namespace here, powerhouse and put this irc service in there. This also means that the ingress resource for this namespace will contain a couple more routings, because my powerhouse namespace will also contain my gitea and Athens proxy installation.\nWe can, however, take a peak at the ingress resource here and now… because I hate suspense.\napiVersion: extensions/v1beta1 kind: Ingress metadata: namespace: powerhouse name: powerhouse-ingress annotations: kubernetes.io/ingress.class: \"nginx\" certmanager.k8s.io/cluster-issuer: \"letsencrypt-prod\" certmanager.k8s.io/acme-challenge-type: http01 nginx.ingress.kubernetes.io/rewrite-target: / spec: tls: - hosts: - irc.powerhouse.com secretName: irc-powerhouse-tls - hosts: - gitea.powerhouse.com secretName: gitea-powerhouse-tls - hosts: - athens.powerhouse.com secretName: athens-powerhouse-tls rules: - host: irc.powerhouse.com http: paths: - backend: serviceName: irc servicePort: 9000 path: / - host: gitea.powerhouse.com http: paths: - backend: serviceName: gitea servicePort: 3000 path: / - host: athens.powerhouse.com http: paths: - backend: serviceName: athens-service servicePort: 80 path: / We can see that I have multiple paths pointing to three different subdomains with different ports. These ports will be routed to by nginx ingress. Meaning you DO NOT OPEN THESE ON YOUR LOADBALANCER. These will all be accessible from 443/HTTPS. Expect for gitea’s SSH port later on.\nWith these in place, cert-manager should pick it up and provide a certificate for it.\nSide track – debugging If there is a problem and we can’t reach TheLounge we need to debug. I use the following tool to access Kubernetes resources: K9S. It’s a neat CLI tool to look at kube resources in an interactive way and not having to type in a bunch of commands. Never the less, I will also paste those in here.\nTo look at the pods that should have been created, type:\nk get pods -n powerhouse Should see something like this:\nNAME READY STATUS RESTARTS AGE athens-app-857749c59c-lmjnb 1/1 Running 0 6d3h gitea-app-6974fb995b-pn2vv 1/1 Running 0 9d gitea-db-59758fbcd9-4562c 1/1 Running 0 9d irc-app-5f87688f98-dqsvb 1/1 Running 0 9d You can see that my other services are running fine. And there is IRC as well. Now if there would be any kind of problem we could access the Pods information be describing the pod with:\nk describe pod/irc-app-5f87688f98-dqsvb -n powerhouse Which will provide a bunch of information about the pod. But the pod could be absolutely fine, yet our service could be down. (We didn’t define any liveliness or readiness probs after all).\nWe can verify that by taking a peak in the container (also, check if our mounting was successful). Since this is just a container, exec works similar to docker exec.\n$ k exec -it irc-app-5f87688f98-dqsvb -n powerhouse /bin/bash root@irc-app-5f87688f98-dqsvb:/# Should give us a prompt. We can now look at logs, check out the configuration folder etc.\nIn k9s you would simply select the right namespace, select the pod, hit d for describe or s for shell. Done.\nGitea Now, we have IRC running. Let’s try deploying Gitea. This takes a tiny bit more fiddling though.\nRequirements Gitea requires the following things to be present:\nThe gitea app configuration file (this can be done via environment properties though) A DB A PersistentVolume SSH Port for SSH based git clones instead of simple https DB We shall begin with the simplest of them, the DB. At this point we could go with the DigitalOcean managed Postgres installation, but I didn’t want to put that on the bill as well. So I choose to simply put my DB into a container and deploy it within the cluster.\nThis is actually quite simple. The DB will be a separate deployment / application in the same namespace as the Gitea app. It will also contain a network policy, since the DB doesn’t need internet access and the internet shouldn’t be able to access it.\nIn fact the only thing that should be able to access the DB is the Gitea application itself which we will be able to restrict via the usage of… Labels!\nDeployment But first, take a look at the deployment of a Postgres 11 pod:\napiVersion: apps/v1 kind: Deployment metadata: namespace: powerhouse name: gitea-db spec: replicas: 1 selector: matchLabels: app: gitea-db template: metadata: name: gitea-db labels: app: gitea-db spec: containers: - name: postgres image: postgres:11 env: - name: POSTGRES_USER value: gitea - name: POSTGRES_PASSWORD valueFrom: secretKeyRef: name: gitea-db-password key: password - name: POSTGRES_DB value: gitea volumeMounts: - mountPath: /var/lib/postgresql/data subPath: data # important so it gets mounted properly name: git-db-data volumes: - name: git-db-data persistentVolumeClaim: claimName: do-storage-gitea-db Okay, there are a lot of things going on here, but the three things we need to note are the following:\nlabels: app: gitea-db Our network policy will look for this label to identify the pods which fell under its rule.\n- name: POSTGRES_PASSWORD valueFrom: secretKeyRef: name: gitea-db-password key: password The database password will come from a secret.\nvolumeMounts: - mountPath: /var/lib/postgresql/data subPath: data # important so it gets mounted properly name: git-db-data volumes: - name: git-db-data persistentVolumeClaim: claimName: do-storage-gitea-db We also need a persistent volume otherwise the data will be lost on each pod restart.\n--- apiVersion: v1 kind: PersistentVolumeClaim metadata: namespace: powerhouse name: do-storage-gitea-db spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: do-block-storage Service We also need a Service so Gitea will be able to reach it. This isn’t public though so a NodePort is enough with no clusterIP.\nkind: Service apiVersion: v1 metadata: namespace: powerhouse name: gitea-db-service spec: ports: - port: 5432 selector: app: gitea-db clusterIP: None In order to reach this DB we can use a URL like this now from the Gitea app: gitea-db-service.powerhouse.svc.cluster.local:5432.\nNetworkPolicy We want the Gitea app to be able to reach it. Which means in-out to the Gitea app and nothing else.\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: gitea-db-network-policy namespace: powehouse spec: podSelector: matchLabels: app: gitea-db policyTypes: - Ingress - Egress ingress: - from: - podSelector: matchLabels: app: gitea ports: - protocol: TCP port: 5432 egress: - to: - podSelector: matchLabels: app: gitea ports: - protocol: TCP port: 5432 We can test this now by exec-ing into the Pod of the DB deployment and trying to ping google.com for example. It should be denied. Yet later, when we deploy our Gitea app, that should be able to talk to the DB instance.\nSecret Finally, we have a Secret which contains our db password base64 encoded.\napiVersion: v1 kind: Secret metadata: namespace: cronohub name: gitea-db-password type: Opaque data: password: Z2l0ZWE= That says password123. To get it, you can run something like echo -n \"password123\" | base64.\nGitea App ini Huh, with that done, we can go on with the application ini file. This can be configured via environment properties but once you get over a dozen configuration entries, it’s just easier to use an app.ini. My app ini is large, so I won’t post it here. I could mount it in as a file, but that proved to be difficult or not work at all properly because Gitea is running under a different user than root. Also, once the mount happened the fact the gitea was trying to write into it caused problems. Mounting as a different user didn’t work out either, so I’m using an InitContainer to do the job. They are there for that reason. And it was actually a hell of a lot simpler than doing file mounting.\nThe app.ini is defined as a ConfigMap like this:\nkubectl create configmap gitea-app-ini --from-file=app.ini --namespace powerhouse This was done from the folder where my app.ini was residing.\nDeployment Now comes the big gun. The Gitea deployment file. This is how it looks like in all its glory:\napiVersion: apps/v1 kind: Deployment metadata: namespace: cronohub name: gitea-app labels: app: gitea spec: replicas: 1 selector: matchLabels: app: gitea template: metadata: labels: app: gitea app.kubernetes.io/name: gitea app.kubernetes.io/instance: gitea spec: initContainers: - name: init-disk image: busybox:latest command: - /bin/chown - 1000:1000 # we set the gid and uid of the user for gitea. - /data volumeMounts: - name: git-data mountPath: \"/data\" readOnly: false - name: init-app-ini image: busybox:latest command: ['sh', '-c', 'mkdir -p /data/gitea/conf/; cp /data/app.ini /data/gitea/conf'] volumeMounts: - name: git-data mountPath: \"/data\" readOnly: false - name: gitea-app-ini-conf mountPath: /data/app.ini subPath: app.ini readOnly: false containers: - name: gitea image: gitea/gitea:1.9.2 env: - name: DB_PASSWD valueFrom: secretKeyRef: name: gitea-db-password key: password - name: DB_TYPE valueFrom: configMapKeyRef: name: gitea-config-map key: DB_TYPE - name: DB_HOST valueFrom: configMapKeyRef: name: gitea-config-map key: DB_HOST - name: DB_NAME valueFrom: configMapKeyRef: name: gitea-config-map key: DB_NAME - name: DB_USER valueFrom: configMapKeyRef: name: gitea-config-map key: DB_USER ports: - containerPort: 3000 name: gitea-http - containerPort: 22 name: gitea-ssh volumeMounts: - mountPath: /data name: git-data readOnly: false volumes: - name: git-data persistentVolumeClaim: claimName: do-storage-gitea - name: gitea-app-ini-conf configMap: name: gitea-app-ini The important bit is the initContainer section. What’s happening here? We mount the app.ini file to the init container under /data. The awesome part about the initContainer is that the real container will have access to the file system the init container created.\nSo we take that file, fix the permissions on it and copy it to the right location under /data/gitea/conf for the Gitea app to work with.\nDone!\nAnd the configMap is simple:\napiVersion: v1 kind: ConfigMap metadata: namespace: powerhouse name: gitea-config-map data: APP_COLOR: blue APP_MOD: prod DB_TYPE: postgres DB_HOST: \"gitea-db-service.cronohub.svc.cluster.local:5432\" DB_NAME: gitea DB_USER: gitea SSH Normally, Ingress only allows HTTP based traffic control. But what would an ingress be without also regular TCP based routing?\nBut it’s not trivial. Nginx Ingress provides a documentation for this here: Exposing TCP and UDP services. What does that mean in practice?\nYou see we are also exposing port 22 on the container:\n- containerPort: 22 name: gitea-ssh I choose to differentiate my SSH port for Gitea from port 22 because that’s just cumbersome to get done right. Gitea provides an explanation on how to do port 22 forwarding in a docker container with a custom git command which forwards commands to the container itself. This is all just plain too much to worry about.\nI have this in the app.ini:\nSSH_PORT = And then this in the Service definition:\nkind: Service apiVersion: v1 metadata: namespace: powerhouse name: gitea labels: app: gitea app.kubernetes.io/name: gitea app.kubernetes.io/instance: gitea spec: selector: app: gitea app.kubernetes.io/name: gitea app.kubernetes.io/instance: gitea ports: - name: http port: 3000 targetPort: gitea-http - name: ssh port: targetPort: gitea-ssh protocol: TCP And then, we edit the nginx-controller deployment like this:\nkubectl edit deployment.apps/nginx-ingress-controller And add this line --tcp-services-configmap=cronohub/gitea-ssh-service to the container’s args field:\ncontainers: - args: - /nginx-ingress-controller - --default-backend-service=default/nginx-ingress-default-backend - --election-id=ingress-controller-leader - --ingress-class=nginx - --configmap=default/nginx-ingress-controller - --tcp-services-configmap=powerhouse/gitea-ssh-service One more thing is that we have to open that port on the load balancer as well to get traffic to it. To that end, edit the nginx ingress service as well:\nkubectl edit services/nginx-ingress-controller And add the exposed port:\n- name: ssh port: protocol: TCP targetPort: There will probably be a nodePort section in there on the other ports. Ignore that for your change.\nAlso, if you are doing the nginx installation by hand, just add this or save the yaml file from those deployments like this:\nkubectl get service/nginx-ingress-controller -o yaml \u003e nginx-ingress-controller.yaml So you can deploy / modify it later on.\nFinished Gitea And with that, visit gitea.powerhouse.com and it should work including HTTPS and SSH!\nYou can now clone repositories like this: git clone ssh://git@gitea.powerhouse.com:1234/user/awesome_project.git after you created your user.\nUser creation is done by using the gitea admin CLI tool described here: Gitea Documentation.\nIt is important to note that we don’t use latest anywhere. It’s just not good if you are trying to update a service later on. We could set ImagePolicy to AlwaysPull but that’s just not a good thing to do if you have a 2 gig image. Always use version and policy imagePullPolicy: IfNotPresent to save yourself some bandwidth.\nIdle Checker Let’s create a last resource, then we’ll call it a day.\nThe idle RPG is a cool little game that you play by… not playing. At all. If you play, you get penalties. Here is a cool resource to start: Idle RPG. It looks something like this:\n21:56 \u003c@IdleBot\u003e Verily I say unto thee, the Heavens have burst forth, and the blessed hand of God carried ganome 0 days, 03:52:11 toward level 45. 21:56 \u003c@IdleBot\u003e ganome reaches next level in 0 days, 01:49:16. 22:02 \u003c@IdleBot\u003e himuraken, the level 77 Mage Of BitFlips, is now online from nickname himuraken. Next level in 11 days, 10:35:53. 22:14 \u003c@IdleBot\u003e Nechayev, Sundance, and simple [2011/2347] have team battled HeavyPodda, Sixbierehomme, and L [1417/2717] and won! 0 days, 06:14:54 is removed from their clocks. 22:18 \u003c@IdleBot\u003e canton7 saw an episode of Ally McBeal. This terrible calamity has slowed them 0 days, 05:10:53 from level 85. 22:18 \u003c@IdleBot\u003e canton7 reaches next level in 2 days, 00:21:36. 22:26 \u003c@IdleBot\u003e Tor [4/1142] has challenged Brainiac [232/817] in combat and lost! 3 days, 23:06:05 is added to Tor's clock. 22:26 \u003c@IdleBot\u003e Tor reaches next level in 39 days, 23:39:35. It could happen that by some misfortune the bouncer gets restarted and it doesn’t log you back in. Or you simply just lose connection and you don’t re-connect. That is unacceptable because the point is to be present. Otherwise you don’t play. So you need an early warning in case you are offline. Luckily, IdleRPG provides an XML based endpoint to get which contains your status.\nFrom there, I’m using mailgun with a registered domain to send me an email in case my status is offline. For that, here is a small Go program IdleRPG Checker Go Code.\nTo put that into a Docker container, here is a Dockerfile:\nFROM golang:latest as build RUN go get -v github.com/sirupsen/logrus \u0026\u0026 \\ go get -v github.com/mailgun/mailgun-go COPY ./main.go /code/ WORKDIR /code RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o /idlerpg-checker . FROM alpine:latest RUN apk --no-cache add ca-certificates COPY --from=build /idlerpg-checker /idlerpg-checker RUN echo \"v0.0.1\" \u003e\u003e .version ENTRYPOINT [\"/idlerpg-checker\"] And the corresponding cronjob resource definition:\napiVersion: batch/v1beta1 kind: CronJob metadata: name: idle-checker namespace: idle-checker spec: schedule: \"*/20 * * * *\" jobTemplate: spec: template: spec: containers: - name: idle-checker image: skarlso/idle-checker imagePullPolicy: IfNotPresent env: - name: MG_API_KEY valueFrom: secretKeyRef: name: idle-rpg-secret key: MG_API_KEY - name: MG_DOMAIN valueFrom: secretKeyRef: name: idle-rpg-secret key: MG_DOMAIN args: ['-username', 'username', '-email', 'user@powerhouse.com'] restartPolicy: OnFailure Aaaand, the secret for the API key:\napiVersion: v1 kind: Secret metadata: namespace: idle-checker name: idle-rpg-secret type: Opaque data: MG_API_KEY: asdf= MG_DOMAIN: asdf== Done. Huh. This will run every 20 minutes and check if the user with username username is online. If not, it will send an email to the given email address. Your levels are safe.\nClosing words Phew. This has been quite the ride. The post is now really long, so I will split the rest out into a Part 2. That is, Athens and Monitoring.\nThank you for reading this far!\nCheers, Gergely.\n",
  "wordCount" : "4425",
  "inLanguage": "en",
  "datePublished": "2019-09-21T21:01:00+01:00",
  "dateModified": "2019-09-21T21:01:00+01:00",
  "author":{
    "@type": "Person",
    "name": "hannibal"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://skarlso.github.io/2019/09/21/kubernetes-cluster/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Ramblings of a cloud engineer",
    "logo": {
      "@type": "ImageObject",
      "url": "https://skarlso.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://skarlso.github.io/" accesskey="h" title="Ramblings of a cloud engineer (Alt + H)">Ramblings of a cloud engineer</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://skarlso.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://skarlso.github.io/search/" title="Search">
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://skarlso.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://skarlso.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://skarlso.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://skarlso.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Using a Kubernetes based Cluster for Various Services with auto HTTPS
    </h1>
    <div class="post-meta"><span title='2019-09-21 21:01:00 +0100 +0100'>September 21, 2019</span>&nbsp;·&nbsp;21 min&nbsp;·&nbsp;hannibal

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#intro" aria-label="Intro">Intro</a></li>
                <li>
                    <a href="#what" aria-label="What">What</a></li>
                <li>
                    <a href="#where" aria-label="Where">Where</a></li>
                <li>
                    <a href="#what-not" aria-label="What Not">What Not</a><ul>
                        
                <li>
                    <a href="#rbac-for-various-services-and-users" aria-label="RBAC for various services and users">RBAC for various services and users</a></li>
                <li>
                    <a href="#resource-limitation" aria-label="Resource limitation">Resource limitation</a></li>
                <li>
                    <a href="#readiness-liveliness" aria-label="Readiness Liveliness">Readiness Liveliness</a></li></ul>
                </li>
                <li>
                    <a href="#how" aria-label="How">How</a></li>
                <li>
                    <a href="#beginning" aria-label="Beginning">Beginning</a><ul>
                        
                <li>
                    <a href="#example" aria-label="Example">Example</a></li></ul>
                </li>
                <li>
                    <a href="#before-we-begin" aria-label="Before we Begin">Before we Begin</a></li>
                <li>
                    <a href="#cert-manager" aria-label="Cert-Manager">Cert-Manager</a></li>
                <li>
                    <a href="#ingress" aria-label="Ingress">Ingress</a></li>
                <li>
                    <a href="#from-easy-to-complicated" aria-label="From Easy to Complicated">From Easy to Complicated</a><ul>
                        
                <li>
                    <a href="#my-website" aria-label="My Website">My Website</a><ul>
                        
                <li>
                    <a href="#the-deployment" aria-label="The deployment">The deployment</a></li>
                <li>
                    <a href="#the-service" aria-label="The service">The service</a></li>
                <li>
                    <a href="#ingress-1" aria-label="Ingress">Ingress</a></li></ul>
                </li>
                <li>
                    <a href="#irc-bouncer" aria-label="IRC bouncer">IRC bouncer</a><ul>
                        
                <li>
                    <a href="#the-container" aria-label="The container">The container</a></li>
                <li>
                    <a href="#pvc" aria-label="PVC">PVC</a></li>
                <li>
                    <a href="#deployment" aria-label="Deployment">Deployment</a></li>
                <li>
                    <a href="#the-service-1" aria-label="The service">The service</a></li>
                <li>
                    <a href="#side-track----debugging" aria-label="Side track &ndash; debugging">Side track &ndash; debugging</a></li></ul>
                </li>
                <li>
                    <a href="#gitea" aria-label="Gitea">Gitea</a><ul>
                        
                <li>
                    <a href="#requirements" aria-label="Requirements">Requirements</a><ul>
                        
                <li>
                    <a href="#db" aria-label="DB">DB</a><ul>
                        
                <li>
                    <a href="#deployment-1" aria-label="Deployment">Deployment</a></li>
                <li>
                    <a href="#service" aria-label="Service">Service</a></li>
                <li>
                    <a href="#networkpolicy" aria-label="NetworkPolicy">NetworkPolicy</a></li>
                <li>
                    <a href="#secret" aria-label="Secret">Secret</a></li></ul>
                </li>
                <li>
                    <a href="#gitea-app-ini" aria-label="Gitea App ini">Gitea App ini</a></li>
                <li>
                    <a href="#deployment-2" aria-label="Deployment">Deployment</a></li>
                <li>
                    <a href="#ssh" aria-label="SSH">SSH</a></li>
                <li>
                    <a href="#finished-gitea" aria-label="Finished Gitea">Finished Gitea</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#idle-checker" aria-label="Idle Checker">Idle Checker</a></li></ul>
                </li>
                <li>
                    <a href="#closing-words" aria-label="Closing words">Closing words</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="intro">Intro<a hidden class="anchor" aria-hidden="true" href="#intro">#</a></h1>
<p>Hi folks.</p>
<p>Today, I would like to show you how my infrastructure is deployed and managed. Spoiler alert, I&rsquo;m using Kubernetes to do that.</p>
<p>I know&hellip; What a twist!</p>
<p>Let&rsquo;s get to it.</p>
<h1 id="what">What<a hidden class="anchor" aria-hidden="true" href="#what">#</a></h1>
<p><img alt="kube-architecture" loading="lazy" src="/img/hosting/kube-architecture.png"></p>
<p>What services am I running exactly? Here is a list I&rsquo;m running at the time of this writing:</p>
<ul>
<li>Athens Go Proxy</li>
<li>Gitea</li>
<li>The Lounge (IRC bouncer)</li>
<li>Two CronJobs
<ul>
<li>Fork Updater</li>
<li>IDLE RPG online checker</li>
</ul>
</li>
<li>My WebSite (gergelybrautigam.com)</li>
<li>Monitoring</li>
</ul>
<p>And it&rsquo;s really simple to add more.</p>
<h1 id="where">Where<a hidden class="anchor" aria-hidden="true" href="#where">#</a></h1>
<p>My cluster is deployed at DigitalOcean using two droplets each 1vCPU and 2GB RAM.</p>
<p><img alt="kube-on-digitalocean" loading="lazy" src="/img/hosting/kube-on-digitalocean.png"></p>
<h1 id="what-not">What Not<a hidden class="anchor" aria-hidden="true" href="#what-not">#</a></h1>
<p>This isn&rsquo;t going to be a production grade cluster. What I don&rsquo;t include in here:</p>
<h2 id="rbac-for-various-services-and-users">RBAC for various services and users<a hidden class="anchor" aria-hidden="true" href="#rbac-for-various-services-and-users">#</a></h2>
<p>Since I&rsquo;m the only user of my cluster I didn&rsquo;t create any kind of access limits / users or such. You are free to create them though. The only role based auth that&rsquo;s going on is for Prometheus.</p>
<p>I&rsquo;m not using any third party things which require access to the API.</p>
<h2 id="resource-limitation">Resource limitation<a hidden class="anchor" aria-hidden="true" href="#resource-limitation">#</a></h2>
<p>I&rsquo;m the sole user of my things. I&rsquo;m not really scaling my gitea up or down based on usage and as such, I did not define things like:</p>
<ul>
<li>Resource limits</li>
<li>Nodes with certain capabilities</li>
<li>Affinities and Taints &ndash; which means, everything can run anywhere</li>
</ul>
<h2 id="readiness-liveliness">Readiness Liveliness<a hidden class="anchor" aria-hidden="true" href="#readiness-liveliness">#</a></h2>
<p>Most of by deploys and services don&rsquo;t have these except for Athens.</p>
<h1 id="how">How<a hidden class="anchor" aria-hidden="true" href="#how">#</a></h1>
<p>Okay, with that out of the way, let&rsquo;s get into the hows of things&hellip;</p>
<h1 id="beginning">Beginning<a hidden class="anchor" aria-hidden="true" href="#beginning">#</a></h1>
<p>The most important thing that you need to do in order to use Kubernetes is Containerizing all the things.</p>
<p><img alt="containers" loading="lazy" src="/img/hosting/containers.png"></p>
<p>Since Kubernetes is a container orchestration tool, without containers it&rsquo;s pretty useless.</p>
<p>As a driver, I&rsquo;m going to use Docker. Kubernetes can use anything that&rsquo;s OCI compatible, which means if you would like to use runc as a container engine, you can do that. I&rsquo;d like to keep my sanity though.</p>
<h2 id="example">Example<a hidden class="anchor" aria-hidden="true" href="#example">#</a></h2>
<p><img alt="fork-updater" loading="lazy" src="/img/hosting/fork-updater.png"></p>
<p>To show you what I mean&hellip; I have a cronjob which is running every month. It gathers all my forks on github and updates them with the latest from their parents. This a small ruby script located here: <a href="https://gist.github.com/Skarlso/fd5bd5971a78a5fa9760b31683de690e">Fork Updater</a>. How do we run this? It requires two things. First, a token. We pass that currently as an environment property. It could be in a file in a vault or a secret mounted in as a file it doesn&rsquo;t matter. Currently, it&rsquo;s an environment property. The second thing is more subtle.</p>
<p>I&rsquo;m pushing the changes back into my remote forks. I&rsquo;m doing this via SSH. So, we need a key in there too. How we&rsquo;ll get that in there, I&rsquo;ll talk about later when we are talking about how to set this cron job up. For now though, the container needs to look for a key in a specific location because we don&rsquo;t want to over-mount <code>/root/.ssh/</code> and we also don&rsquo;t want to use an initContainer to copy over an SSH key (because it&rsquo;s mounted in as a symlink (but that&rsquo;s a different issue all together)). Also, we certianly do NOT want to have a key in the container.</p>
<p>To achieve this, we simply set up a <code>config</code> file for SSH like this one:</p>
<pre tabindex="0"><code>Host github.com
    IdentityFile /etc/secret/id_rsa
</code></pre><p><code>/etc/secret</code> will be the destination of the ssh key we create.</p>
<p>And we also need to have a known_hosts file, otherwise git clone will complain. We also bake this into the container. Why? Why not generate that on the fly? Because I want it to fail in case there is something wrong or there is a MIMA going on etc.</p>
<p>All this translated into a Dockerfile looks like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Dockerfile" data-lang="Dockerfile"><span style="display:flex;"><span><span style="color:#75715e"># We are using alpine for a minimalistic image</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">FROM</span><span style="color:#e6db74"> alpine:latest</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> apk --no-cache add ca-certificates<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> apk update<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e"># Openssh is needed for the SSH command</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> apk --no-cache add ruby vim curl git build-base ruby-dev openssh openssh-client<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e"># Setup dependencies for the fork ruby file</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> gem install octokit logger multi_json json --no-ri --no-rdoc<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> mkdir /data<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">WORKDIR</span><span style="color:#e6db74"> /data</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e"># Setup some data about the committer</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> git config --global user.name <span style="color:#e6db74">&#34;Fork Updater&#34;</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> git config --global user.email <span style="color:#e6db74">&#34;email@email.com&#34;</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> mkdir -p /root/.ssh<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e"># Get the host config for github.com</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> ssh-keyscan github.com &gt;&gt; /root/.ssh/known_hosts<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e"># Setup the SSH config</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">COPY</span> ./config /root/.ssh<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">COPY</span> ./fork_updater.rb .<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">CMD</span> [<span style="color:#e6db74">&#34;ruby&#34;</span>, <span style="color:#e6db74">&#34;/data/fork_updater.rb&#34;</span>]<span style="color:#960050;background-color:#1e0010">
</span></span></span></code></pre></div><p>That&rsquo;s it. Now our updater is containerized and ready to be deployed as a cronjob on a kube cluster. Oh, and we also need to create the SSH key like this:</p>
<pre tabindex="0"><code>kubectl create secret generic ssh-key-secret --from-file=ssh-privatekey=/path/to/.ssh/id_rsa
</code></pre><h1 id="before-we-begin">Before we Begin<a hidden class="anchor" aria-hidden="true" href="#before-we-begin">#</a></h1>
<p>There are two things we will need though to set up for our cluster before we even begin adding the first service. And that&rsquo;s an ingress with a load-balancer and cert-manager.</p>
<h1 id="cert-manager">Cert-Manager<a hidden class="anchor" aria-hidden="true" href="#cert-manager">#</a></h1>
<p>Now, you have the option of installing cert-manager via helm, or via the provided kube config yaml file. I <strong>STRONGLY</strong> recommend using the config yaml file because the upgrading process with helm is a hell of a lot dirtier / failure prone than simply applying a new yaml file with a different version in it.</p>
<p>Either way, to install cert-manager follow this simple guide: <a href="https://docs.cert-manager.io/en/latest/getting-started/install/kubernetes.html#installing-with-regular-manifests">Cert-manager Install Manual</a>.</p>
<h1 id="ingress">Ingress<a hidden class="anchor" aria-hidden="true" href="#ingress">#</a></h1>
<p>An Ingress is a must. This is the component which manages external access to the services which we will define. Like a proxy before your http server. This component will handle the hostname based routing between our services.</p>
<p>I&rsquo;m using nginx ingress, although there are a couple of implementations out there.</p>
<p>To install nginx ingress, follow their guide here: <a href="https://kubernetes.github.io/ingress-nginx/deploy/">Installing Nginx-Ingress</a>.</p>
<h1 id="from-easy-to-complicated">From Easy to Complicated<a hidden class="anchor" aria-hidden="true" href="#from-easy-to-complicated">#</a></h1>
<p>Alright. Now that we have the prereqs out of the way, let&rsquo;s get our hands dirty. I&rsquo;ll start with the easiest of them all, my web site, and then will progress towards the more complicated ones, like Gitea and Athens, which require a lot more fiddling and have more moving parts.</p>
<h2 id="my-website">My Website<a hidden class="anchor" aria-hidden="true" href="#my-website">#</a></h2>
<p>The site, located here: <a href="https://gergelybrautigam.com">Gergely&rsquo;s Domain</a>; is a really simple, static, <a href="https://gohugo.io">Hugo</a> based website. It contains nothing fancy, no real Javascript magic, has a simple list of things I&rsquo;ve done and who I am.</p>
<p>It&rsquo;s powered / served by an nginx instance running on port 9090 define by a very simple Dockerfile:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Dockerfile" data-lang="Dockerfile"><span style="display:flex;"><span><span style="color:#66d9ef">FROM</span><span style="color:#e6db74"> golang:latest as builder</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> apt-get update <span style="color:#f92672">&amp;&amp;</span> apt install -y git make vim hugo<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> mkdir -p /opt/website<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> git clone https://github.com/Skarlso/gergelybrautigam /opt/website<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">WORKDIR</span><span style="color:#e6db74"> /opt/website</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> make<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">FROM</span><span style="color:#e6db74"> nginx:latest</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> mkdir -p /var/www/html/gergelybrautigam<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">WORKDIR</span><span style="color:#e6db74"> /var/www/html/gergelybrautigam</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">COPY</span> --from<span style="color:#f92672">=</span>builder /opt/website/public .<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">COPY</span> 01_gergelybrautigam /etc/nginx/sites-available/<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> mkdir -p /etc/nginx/sites-enabled/<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> ln -s /etc/nginx/sites-available/01_gergelybrautigam /etc/nginx/sites-enabled/01_gergelybrautigam<span style="color:#960050;background-color:#1e0010">
</span></span></span></code></pre></div><p>Easy as goblin pie. Nginx has a command set like this <code>CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]</code> and exposes port 80.</p>
<h3 id="the-deployment">The deployment<a hidden class="anchor" aria-hidden="true" href="#the-deployment">#</a></h3>
<p>In order to deploy this in the cluster, I created a deployment as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gb-deployment</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">gergely-brautigam</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gergelybrautigam</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">annotations</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">prometheus.io/scrape</span>: <span style="color:#e6db74">&#39;true&#39;</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">prometheus.io/port</span>:   <span style="color:#e6db74">&#39;9090&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">matchLabels</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gergelybrautigam</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">template</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gergelybrautigam</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">annotations</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">prometheus.io/scrape</span>: <span style="color:#e6db74">&#39;true&#39;</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">prometheus.io/port</span>:   <span style="color:#e6db74">&#39;9090&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gergelybrautigam</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">skarlso/gergelybrautigam:v0.0.26</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">9090</span>
</span></span></code></pre></div><p>The metadata section defines information about the deployment. It&rsquo;s name is gb-deployment. The namespace in which this sits is called gergely-brautigam and it has some labels to it so Prometheus monitoring tool can discover the pod.</p>
<p>It&rsquo;s running a single replica, has a bunch of more metadata and template settings, and finally the container spec which defines the image, and the exposed container port on which the application is running.</p>
<p>Now we need a service to expose this deployment.</p>
<h3 id="the-service">The service<a hidden class="anchor" aria-hidden="true" href="#the-service">#</a></h3>
<p>The service is also simple. It looks like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">gergely-brautigam</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gb-service</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gergelybrautigam</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">port</span>: <span style="color:#ae81ff">80</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">9090</span>
</span></span></code></pre></div><p>Again, nothing fancy here, just a simple service exposing a port to a different port on the front-end side. This service will be picked up by our previously created routing facility.</p>
<h3 id="ingress-1">Ingress<a hidden class="anchor" aria-hidden="true" href="#ingress-1">#</a></h3>
<p>Now that we have the service we need to expose it to the domain. I have the domain gergelybrautigam.com and I already pointed it at the LoadBalancer&rsquo;s IP which was created by the nginx ingress controller.</p>
<p>We only want one LoadBalancer, but we have multiple hostnames. We can achieve that by creating an Ingress resource in the namespace our service is in like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">extensions/v1beta1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Ingress</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">gergely-brautigam</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gergely-brautigam-ingress</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">annotations</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">kubernetes.io/ingress.class</span>: <span style="color:#e6db74">&#34;nginx&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">certmanager.k8s.io/cluster-issuer</span>: <span style="color:#e6db74">&#34;letsencrypt-prod&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">certmanager.k8s.io/acme-challenge-type</span>: <span style="color:#ae81ff">http01</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">nginx.ingress.kubernetes.io/rewrite-target</span>: <span style="color:#ae81ff">/</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">tls</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">hosts</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">gergelybrautigam.com</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">secretName</span>: <span style="color:#ae81ff">gergelybrautigam-tls</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">rules</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">host</span>: <span style="color:#ae81ff">gergelybrautigam.com</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">http</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">paths</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">backend</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">serviceName</span>: <span style="color:#ae81ff">gb-service</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">servicePort</span>: <span style="color:#ae81ff">80</span>
</span></span></code></pre></div><p>Remember, we already have the nginx ingress resource in the default namespace when we installed the controller previously. That is the main entrypoint. We are taking advantage of the rewrite-target annotation. That is our key to success <code>nginx.ingress.kubernetes.io/rewrite-target: /</code>. The rest is basic routing. We&rsquo;ll have something like this in the other namespace to.</p>
<p>And with that, our website is done and it should be working under HTTPS. Cert-manager should have picked it up and generated a certificate for it. Let&rsquo;s check.</p>
<p>Running <code>k get certs -n gergely-brautigam</code> you should see something like this:</p>
<pre tabindex="0"><code> $ k get certs -n gergely-brautigam
NAME                   READY   SECRET                 AGE
gergelybrautigam-tls   True    gergelybrautigam-tls   86d
</code></pre><p>If there is a problem, just describe the cert resource and look for the generated challenge and if it was successful or not. The challenge contains mostly good error messages.</p>
<h2 id="irc-bouncer">IRC bouncer<a hidden class="anchor" aria-hidden="true" href="#irc-bouncer">#</a></h2>
<p>That wasn&rsquo;t too bad, right? Let&rsquo;s do something a bit more complex this time. We are going to deploy <a href="https://github.com/thelounge/thelounge">The lounge</a> irc bouncer.</p>
<p>It&rsquo;s actually quite easy to do but can be daunting to look at at first.</p>
<p><img alt="easy" loading="lazy" src="/img/hosting/the-climb.png"></p>
<h3 id="the-container">The container<a hidden class="anchor" aria-hidden="true" href="#the-container">#</a></h3>
<p>Lucky for us, the bouncer already provides a container located here: <a href="https://hub.docker.com/r/thelounge/thelounge/">The Lounge Docker Hub</a>.</p>
<p>We just need two things. To expose the port 9000 and to give it something called a PersistentVolume. What&rsquo;s a persistent volume? Well, look it up here: <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Kubernetes Persistent Volumes</a>.</p>
<p>TL;DR: We need to preserve data. Containers are ephemeral in nature. Meaning if there is a problem we usually just delete the pod. Which means that all data will be lost. But we need persistence in this case because we&rsquo;ll have user data and user information which we would like to persist between pods. That&rsquo;s what a volume is for.</p>
<p>It will be mounted into the pod so we can point the bouncer to use that location for data management.</p>
<h3 id="pvc">PVC<a hidden class="anchor" aria-hidden="true" href="#pvc">#</a></h3>
<p>With that, this is how our PersistentVolumeClaim will look like:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">PersistentVolumeClaim</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">powerhouse</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">do-storage-irc</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">accessModes</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#ae81ff">ReadWriteOnce</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">resources</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">requests</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">storage</span>: <span style="color:#ae81ff">5Gi</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">storageClassName</span>: <span style="color:#ae81ff">do-block-storage</span>
</span></span></code></pre></div><p>DigitalOcean provides a block storage implementation for this claim so we use that storage class <code>do-block-storage</code>.</p>
<h3 id="deployment">Deployment<a hidden class="anchor" aria-hidden="true" href="#deployment">#</a></h3>
<p>With that, this is how the deployment will look like:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">powerhouse</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">irc-app</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">irc</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">matchLabels</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">irc</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">template</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">app</span>: <span style="color:#ae81ff">irc</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">irc</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">app.kubernetes.io/instance</span>: <span style="color:#ae81ff">irc</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">irc</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">thelounge/thelounge:3.1.1</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">9000</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">name</span>: <span style="color:#ae81ff">irc-http</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/var/opt/thelounge</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">subPath</span>: <span style="color:#ae81ff">thelounge</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">name</span>: <span style="color:#ae81ff">irc-data</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">readOnly</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">irc-data</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">persistentVolumeClaim</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">claimName</span>: <span style="color:#ae81ff">do-storage-irc</span>
</span></span></code></pre></div><p>Short and sweet. The important bits are the labels, those are used by cert-manager and ingress to find the right deployment, and the <code>volumeMounts</code>. We mount into the /var/opt/thelounge folder because that&rsquo;s the main configuration location. The subPath is important for a correct mounting.</p>
<h3 id="the-service-1">The service<a hidden class="anchor" aria-hidden="true" href="#the-service-1">#</a></h3>
<p>Alright, with the deployment in place, let&rsquo;s take a look at the service.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">powerhouse</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">irc</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">irc</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">irc</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/instance</span>: <span style="color:#ae81ff">irc</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">irc</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">irc</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/instance</span>: <span style="color:#ae81ff">irc</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">http</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">port</span>: <span style="color:#ae81ff">9000</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">irc-http</span>
</span></span></code></pre></div><p>Again, very boring stuff. Boring is good. Boring is predictable. We expose port 9000 to the named targetPort called irc-http which we defined in the above deployment.</p>
<p>Now, I have a domain in which these things are running, let&rsquo;s call it <code>powerhouse.com</code> (because I&rsquo;m tired of example.com). And I have multiple services in this namespace too, so I&rsquo;ll call the namespace here, powerhouse and put this irc service in there. This also means that the ingress resource for this namespace will contain a couple more routings, because my powerhouse namespace will also contain my gitea and Athens proxy installation.</p>
<p>We can, however, take a peak at the ingress resource here and now&hellip; because I hate suspense.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">extensions/v1beta1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Ingress</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">powerhouse</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">powerhouse-ingress</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">annotations</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">kubernetes.io/ingress.class</span>: <span style="color:#e6db74">&#34;nginx&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">certmanager.k8s.io/cluster-issuer</span>: <span style="color:#e6db74">&#34;letsencrypt-prod&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">certmanager.k8s.io/acme-challenge-type</span>: <span style="color:#ae81ff">http01</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">nginx.ingress.kubernetes.io/rewrite-target</span>: <span style="color:#ae81ff">/</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">tls</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">hosts</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">irc.powerhouse.com</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">secretName</span>: <span style="color:#ae81ff">irc-powerhouse-tls</span>
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">hosts</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">gitea.powerhouse.com</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">secretName</span>: <span style="color:#ae81ff">gitea-powerhouse-tls</span>
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">hosts</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">athens.powerhouse.com</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">secretName</span>: <span style="color:#ae81ff">athens-powerhouse-tls</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">rules</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">host</span>: <span style="color:#ae81ff">irc.powerhouse.com</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">http</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">paths</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">backend</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">serviceName</span>: <span style="color:#ae81ff">irc</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">servicePort</span>: <span style="color:#ae81ff">9000</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/</span>
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">host</span>: <span style="color:#ae81ff">gitea.powerhouse.com</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">http</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">paths</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">backend</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">serviceName</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">servicePort</span>: <span style="color:#ae81ff">3000</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/</span>
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">host</span>: <span style="color:#ae81ff">athens.powerhouse.com</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">http</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">paths</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">backend</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">serviceName</span>: <span style="color:#ae81ff">athens-service</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">servicePort</span>: <span style="color:#ae81ff">80</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/</span>
</span></span></code></pre></div><p>We can see that I have multiple paths pointing to three different subdomains with different ports. These ports will be routed to by nginx ingress. Meaning you <strong>DO NOT OPEN THESE ON YOUR LOADBALANCER</strong>. These will all be accessible from 443/HTTPS. Expect for gitea&rsquo;s SSH port later on.</p>
<p>With these in place, cert-manager should pick it up and provide a certificate for it.</p>
<h3 id="side-track----debugging">Side track &ndash; debugging<a hidden class="anchor" aria-hidden="true" href="#side-track----debugging">#</a></h3>
<p>If there is a problem and we can&rsquo;t reach TheLounge we need to debug. I use the following tool to access Kubernetes resources: <a href="https://github.com/derailed/k9s">K9S</a>. It&rsquo;s a neat CLI tool to look at kube resources in an interactive way and not having to type in a bunch of commands. Never the less, I will also paste those in here.</p>
<p>To look at the pods that should have been created, type:</p>
<pre tabindex="0"><code>k get pods -n powerhouse
</code></pre><p>Should see something like this:</p>
<pre tabindex="0"><code>NAME                          READY   STATUS    RESTARTS   AGE
athens-app-857749c59c-lmjnb   1/1     Running   0          6d3h
gitea-app-6974fb995b-pn2vv    1/1     Running   0          9d
gitea-db-59758fbcd9-4562c     1/1     Running   0          9d
irc-app-5f87688f98-dqsvb      1/1     Running   0          9d
</code></pre><p>You can see that my other services are running fine. And there is IRC as well. Now if there would be any kind of problem we could access the Pods information be describing the pod with:</p>
<pre tabindex="0"><code>k describe pod/irc-app-5f87688f98-dqsvb -n powerhouse
</code></pre><p>Which will provide a bunch of information about the pod. But the pod could be absolutely fine, yet our service could be down. (We didn&rsquo;t define any liveliness or readiness probs after all).</p>
<p>We can verify that by taking a peak in the container (also, check if our mounting was successful). Since this is just a container, exec works similar to docker exec.</p>
<pre tabindex="0"><code> $ k exec -it irc-app-5f87688f98-dqsvb -n powerhouse /bin/bash
root@irc-app-5f87688f98-dqsvb:/#
</code></pre><p>Should give us a prompt. We can now look at logs, check out the configuration folder etc.</p>
<p>In k9s you would simply select the right namespace, select the pod, hit <code>d</code> for describe or <code>s</code> for shell. Done.</p>
<h2 id="gitea">Gitea<a hidden class="anchor" aria-hidden="true" href="#gitea">#</a></h2>
<p>Now, we have IRC running. Let&rsquo;s try deploying <a href="https://gitea.io/en-us/">Gitea</a>. This takes a tiny bit more fiddling though.</p>
<h3 id="requirements">Requirements<a hidden class="anchor" aria-hidden="true" href="#requirements">#</a></h3>
<p>Gitea requires the following things to be present:</p>
<ul>
<li>The gitea app configuration file (this can be done via environment properties though)</li>
<li>A DB</li>
<li>A PersistentVolume</li>
<li>SSH Port for SSH based git clones instead of simple https</li>
</ul>
<h4 id="db">DB<a hidden class="anchor" aria-hidden="true" href="#db">#</a></h4>
<p>We shall begin with the simplest of them, the DB. At this point we could go with the DigitalOcean managed Postgres installation, but I didn&rsquo;t want to put that on the bill as well. So I choose to simply put my DB into a container and deploy it within the cluster.</p>
<p>This is actually quite simple. The DB will be a separate deployment / application in the same namespace as the Gitea app. It will also contain a network policy, since the DB doesn&rsquo;t need internet access and the internet shouldn&rsquo;t be able to access it.</p>
<p>In fact the only thing that should be able to access the DB is the Gitea application itself which we will be able to restrict via the usage of&hellip; Labels!</p>
<h5 id="deployment-1">Deployment<a hidden class="anchor" aria-hidden="true" href="#deployment-1">#</a></h5>
<p>But first, take a look at the deployment of a Postgres 11 pod:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">powerhouse</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-db</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">matchLabels</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gitea-db</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">template</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-db</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gitea-db</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">postgres</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">postgres:11</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">env</span>:
</span></span><span style="display:flex;"><span>          - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">POSTGRES_USER</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">value</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span>          - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">POSTGRES_PASSWORD</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">valueFrom</span>:
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">secretKeyRef</span>:
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-db-password</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">key</span>: <span style="color:#ae81ff">password</span>
</span></span><span style="display:flex;"><span>          - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">POSTGRES_DB</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">value</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/var/lib/postgresql/data</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">subPath</span>: <span style="color:#ae81ff">data</span> <span style="color:#75715e"># important so it gets mounted properly</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">name</span>: <span style="color:#ae81ff">git-db-data</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">git-db-data</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">persistentVolumeClaim</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">claimName</span>: <span style="color:#ae81ff">do-storage-gitea-db</span>
</span></span></code></pre></div><p>Okay, there are a lot of things going on here, but the three things we need to note are the following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>      <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gitea-db</span>
</span></span></code></pre></div><p>Our network policy will look for this label to identify the pods which fell under its rule.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>          - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">POSTGRES_PASSWORD</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">valueFrom</span>:
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">secretKeyRef</span>:
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-db-password</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">key</span>: <span style="color:#ae81ff">password</span>
</span></span></code></pre></div><p>The database password will come from a secret.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>        <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/var/lib/postgresql/data</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">subPath</span>: <span style="color:#ae81ff">data</span> <span style="color:#75715e"># important so it gets mounted properly</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">name</span>: <span style="color:#ae81ff">git-db-data</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">git-db-data</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">persistentVolumeClaim</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">claimName</span>: <span style="color:#ae81ff">do-storage-gitea-db</span>
</span></span></code></pre></div><p>We also need a persistent volume otherwise the data will be lost on each pod restart.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">PersistentVolumeClaim</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">powerhouse</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">do-storage-gitea-db</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">accessModes</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#ae81ff">ReadWriteOnce</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">resources</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">requests</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">storage</span>: <span style="color:#ae81ff">10Gi</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">storageClassName</span>: <span style="color:#ae81ff">do-block-storage</span>
</span></span></code></pre></div><h5 id="service">Service<a hidden class="anchor" aria-hidden="true" href="#service">#</a></h5>
<p>We also need a Service so Gitea will be able to reach it. This isn&rsquo;t public though so a NodePort is enough with no clusterIP.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">powerhouse</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-db-service</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">port</span>: <span style="color:#ae81ff">5432</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gitea-db</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">clusterIP</span>: <span style="color:#ae81ff">None</span>
</span></span></code></pre></div><p>In order to reach this DB we can use a URL like this now from the Gitea app: <code>gitea-db-service.powerhouse.svc.cluster.local:5432</code>.</p>
<h5 id="networkpolicy">NetworkPolicy<a hidden class="anchor" aria-hidden="true" href="#networkpolicy">#</a></h5>
<p>We want the Gitea app to be able to reach it. Which means in-out to the Gitea app and nothing else.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">networking.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">NetworkPolicy</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-db-network-policy</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">powehouse</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">podSelector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">matchLabels</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gitea-db</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">policyTypes</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#ae81ff">Ingress</span>
</span></span><span style="display:flex;"><span>  - <span style="color:#ae81ff">Egress</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ingress</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">from</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">podSelector</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">matchLabels</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">port</span>: <span style="color:#ae81ff">5432</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">egress</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">to</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">podSelector</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">matchLabels</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">port</span>: <span style="color:#ae81ff">5432</span>
</span></span></code></pre></div><p>We can test this now by exec-ing into the Pod of the DB deployment and trying to ping google.com for example. It should be denied. Yet later, when we deploy our Gitea app, that should be able to talk to the DB instance.</p>
<h5 id="secret">Secret<a hidden class="anchor" aria-hidden="true" href="#secret">#</a></h5>
<p>Finally, we have a Secret which contains our db password base64 encoded.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Secret</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">cronohub</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-db-password</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">type</span>: <span style="color:#ae81ff">Opaque</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">data</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">password</span>: <span style="color:#ae81ff">Z2l0ZWE=</span>
</span></span></code></pre></div><p>That says password123. To get it, you can run something like <code>echo -n &quot;password123&quot; | base64</code>.</p>
<h4 id="gitea-app-ini">Gitea App ini<a hidden class="anchor" aria-hidden="true" href="#gitea-app-ini">#</a></h4>
<p>Huh, with that done, we can go on with the application ini file. This can be configured via environment properties but once you get over a dozen configuration entries, it&rsquo;s just easier to use an app.ini. My app ini is large, so I won&rsquo;t post it here. I could mount it in as a file, but that proved to be difficult or not work at all properly because Gitea is running under a different user than root. Also, once the mount happened the fact the gitea was trying to write into it caused problems. Mounting as a different user didn&rsquo;t work out either, so I&rsquo;m using an <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">InitContainer</a> to do the job. They are there for that reason. And it was actually a hell of a lot simpler than doing file mounting.</p>
<p>The app.ini is defined as a ConfigMap like this:</p>
<pre tabindex="0"><code>kubectl create configmap gitea-app-ini --from-file=app.ini --namespace powerhouse
</code></pre><p>This was done from the folder where my app.ini was residing.</p>
<h4 id="deployment-2">Deployment<a hidden class="anchor" aria-hidden="true" href="#deployment-2">#</a></h4>
<p>Now comes the big gun. The Gitea deployment file. This is how it looks like in all its glory:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">cronohub</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-app</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">matchLabels</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">template</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">app.kubernetes.io/instance</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">initContainers</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">init-disk</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">busybox:latest</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">command</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#ae81ff">/bin/chown</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#ae81ff">1000</span>:<span style="color:#ae81ff">1000</span> <span style="color:#75715e"># we set the gid and uid of the user for gitea.</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#ae81ff">/data</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">git-data</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">mountPath</span>: <span style="color:#e6db74">&#34;/data&#34;</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">readOnly</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">init-app-ini</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">busybox:latest</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">command</span>: [<span style="color:#e6db74">&#39;sh&#39;</span>, <span style="color:#e6db74">&#39;-c&#39;</span>, <span style="color:#e6db74">&#39;mkdir -p /data/gitea/conf/; cp /data/app.ini /data/gitea/conf&#39;</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">git-data</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">mountPath</span>: <span style="color:#e6db74">&#34;/data&#34;</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">readOnly</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-app-ini-conf</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/data/app.ini</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">subPath</span>: <span style="color:#ae81ff">app.ini</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">readOnly</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">gitea/gitea:1.9.2</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">env</span>:
</span></span><span style="display:flex;"><span>          - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">DB_PASSWD</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">valueFrom</span>:
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">secretKeyRef</span>:
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-db-password</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">key</span>: <span style="color:#ae81ff">password</span>
</span></span><span style="display:flex;"><span>          - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">DB_TYPE</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">valueFrom</span>:
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">configMapKeyRef</span>:
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-config-map</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">key</span>: <span style="color:#ae81ff">DB_TYPE</span>
</span></span><span style="display:flex;"><span>          - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">DB_HOST</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">valueFrom</span>:
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">configMapKeyRef</span>:
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-config-map</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">key</span>: <span style="color:#ae81ff">DB_HOST</span>
</span></span><span style="display:flex;"><span>          - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">DB_NAME</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">valueFrom</span>:
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">configMapKeyRef</span>:
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-config-map</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">key</span>: <span style="color:#ae81ff">DB_NAME</span>
</span></span><span style="display:flex;"><span>          - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">DB_USER</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">valueFrom</span>:
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">configMapKeyRef</span>:
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-config-map</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">key</span>: <span style="color:#ae81ff">DB_USER</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">3000</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-http</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">22</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-ssh</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/data</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">name</span>: <span style="color:#ae81ff">git-data</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">readOnly</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">git-data</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">persistentVolumeClaim</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">claimName</span>: <span style="color:#ae81ff">do-storage-gitea</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-app-ini-conf</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">configMap</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-app-ini</span>
</span></span></code></pre></div><p>The important bit is the initContainer section. What&rsquo;s happening here? We mount the app.ini file to the init container under /data. The awesome part about the initContainer is that the real container will have access to the file system the init container created.</p>
<p>So we take that file, fix the permissions on it and copy it to the right location under <code>/data/gitea/conf</code> for the Gitea app to work with.</p>
<p>Done!</p>
<p>And the configMap is simple:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ConfigMap</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">powerhouse</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-config-map</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">data</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">APP_COLOR</span>: <span style="color:#ae81ff">blue</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">APP_MOD</span>: <span style="color:#ae81ff">prod</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">DB_TYPE</span>: <span style="color:#ae81ff">postgres</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">DB_HOST</span>: <span style="color:#e6db74">&#34;gitea-db-service.cronohub.svc.cluster.local:5432&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">DB_NAME</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">DB_USER</span>: <span style="color:#ae81ff">gitea</span>
</span></span></code></pre></div><h4 id="ssh">SSH<a hidden class="anchor" aria-hidden="true" href="#ssh">#</a></h4>
<p>Normally, Ingress only allows HTTP based traffic control. But what would an ingress be without also regular TCP based routing?</p>
<p>But it&rsquo;s not trivial. Nginx Ingress provides a documentation for this here: <a href="https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/">Exposing TCP and UDP services</a>. What does that mean in practice?</p>
<p>You see we are also exposing port 22 on the container:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>        - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">22</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-ssh</span>
</span></span></code></pre></div><p>I choose to differentiate my SSH port for Gitea from port 22 because that&rsquo;s just cumbersome to get done right. Gitea provides an explanation on how to do port 22 forwarding in a docker container with a custom git command which forwards commands to the container itself. This is all just plain too much to worry about.</p>
<p>I have this in the app.ini:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ini" data-lang="ini"><span style="display:flex;"><span><span style="color:#a6e22e">SSH_PORT</span>         <span style="color:#f92672">=</span> <span style="color:#e6db74">&lt;port of my choosing&gt;</span>
</span></span></code></pre></div><p>And then this in the Service definition:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">powerhouse</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/instance</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/instance</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">http</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">port</span>: <span style="color:#ae81ff">3000</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">gitea-http</span>
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">ssh</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">port</span>: <span style="color:#ae81ff">&lt;port of my choosing&gt;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">gitea-ssh</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
</span></span></code></pre></div><p>And then, we edit the nginx-controller deployment like this:</p>
<pre tabindex="0"><code>kubectl edit deployment.apps/nginx-ingress-controller
</code></pre><p>And add this line <code>--tcp-services-configmap=cronohub/gitea-ssh-service</code> to the container&rsquo;s args field:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>      <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">args</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#ae81ff">/nginx-ingress-controller</span>
</span></span><span style="display:flex;"><span>        - --<span style="color:#ae81ff">default-backend-service=default/nginx-ingress-default-backend</span>
</span></span><span style="display:flex;"><span>        - --<span style="color:#ae81ff">election-id=ingress-controller-leader</span>
</span></span><span style="display:flex;"><span>        - --<span style="color:#ae81ff">ingress-class=nginx</span>
</span></span><span style="display:flex;"><span>        - --<span style="color:#ae81ff">configmap=default/nginx-ingress-controller</span>
</span></span><span style="display:flex;"><span>        - --<span style="color:#ae81ff">tcp-services-configmap=powerhouse/gitea-ssh-service</span>
</span></span></code></pre></div><p>One more thing is that we have to open that port on the load balancer as well to get traffic to it. To that end, edit the nginx ingress service as well:</p>
<pre tabindex="0"><code>kubectl edit services/nginx-ingress-controller
</code></pre><p>And add the exposed port:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">ssh</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">port</span>: <span style="color:#ae81ff">&lt;port of my choosing&gt;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">&lt;port of my choosing&gt;</span>
</span></span></code></pre></div><p>There will probably be a nodePort section in there on the other ports. Ignore that for your change.</p>
<p>Also, if you are doing the nginx installation by hand, just add this or save the yaml file from those deployments like this:</p>
<pre tabindex="0"><code>kubectl get service/nginx-ingress-controller -o yaml &gt; nginx-ingress-controller.yaml
</code></pre><p>So you can deploy / modify it later on.</p>
<h4 id="finished-gitea">Finished Gitea<a hidden class="anchor" aria-hidden="true" href="#finished-gitea">#</a></h4>
<p>And with that, visit <code>gitea.powerhouse.com</code> and it should work including HTTPS and SSH!</p>
<p>You can now clone repositories like this: <code>git clone ssh://git@gitea.powerhouse.com:1234/user/awesome_project.git</code> after you created your user.</p>
<p>User creation is done by using the gitea admin CLI tool described here: <a href="https://docs.gitea.io/en-us/command-line/">Gitea Documentation</a>.</p>
<p>It is important to note that we don&rsquo;t use <code>latest</code> anywhere. It&rsquo;s just not good if you are trying to update a service later on. We could set ImagePolicy to AlwaysPull but that&rsquo;s just not a good thing to do if you have a 2 gig image. Always use version and policy <code>imagePullPolicy: IfNotPresent</code> to save yourself some bandwidth.</p>
<h2 id="idle-checker">Idle Checker<a hidden class="anchor" aria-hidden="true" href="#idle-checker">#</a></h2>
<p><img alt="idle-checker" loading="lazy" src="/img/hosting/idle-checker.png"></p>
<p>Let&rsquo;s create a last resource, then we&rsquo;ll call it a day.</p>
<p>The idle RPG is a cool little game that you play by&hellip; not playing. At all. If you play, you get penalties. Here is a cool resource to start: <a href="https://idlerpg.lolhosting.net">Idle RPG</a>. It looks something like this:</p>
<pre tabindex="0"><code>21:56 &lt;@IdleBot&gt; Verily I say unto thee, the Heavens have burst forth, and the blessed hand of God carried ganome 0 days, 03:52:11 toward level 45.
21:56 &lt;@IdleBot&gt; ganome reaches next level in 0 days, 01:49:16.
22:02 &lt;@IdleBot&gt; himuraken, the level 77 Mage Of BitFlips, is now online from nickname himuraken. Next level in 11 days, 10:35:53.
22:14 &lt;@IdleBot&gt; Nechayev, Sundance, and simple [2011/2347] have team battled HeavyPodda, Sixbierehomme, and L [1417/2717] and won! 0 days, 06:14:54 is removed from their clocks.
22:18 &lt;@IdleBot&gt; canton7 saw an episode of Ally McBeal. This terrible calamity has slowed them 0 days, 05:10:53 from level 85.
22:18 &lt;@IdleBot&gt; canton7 reaches next level in 2 days, 00:21:36.
22:26 &lt;@IdleBot&gt; Tor [4/1142] has challenged Brainiac [232/817] in combat and lost! 3 days, 23:06:05 is added to Tor&#39;s clock.
22:26 &lt;@IdleBot&gt; Tor reaches next level in 39 days, 23:39:35.
</code></pre><p>It could happen that by some misfortune the bouncer gets restarted and it doesn&rsquo;t log you back in. Or you simply just lose connection and you don&rsquo;t re-connect. That is unacceptable because the point is to be present. Otherwise you don&rsquo;t play. So you need an early warning in case you are offline. Luckily, IdleRPG provides an XML based endpoint to get which contains your status.</p>
<p>From there, I&rsquo;m using mailgun with a registered domain to send me an email in case my status is offline. For that, here is a small Go program <a href="https://gist.github.com/Skarlso/318ddd6f8d71dbda8fbbd1a908fdb159">IdleRPG Checker Go Code</a>.</p>
<p>To put that into a Docker container, here is a Dockerfile:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Dockerfile" data-lang="Dockerfile"><span style="display:flex;"><span><span style="color:#66d9ef">FROM</span><span style="color:#e6db74"> golang:latest as build</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> go get -v github.com/sirupsen/logrus <span style="color:#f92672">&amp;&amp;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    go get -v github.com/mailgun/mailgun-go<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">COPY</span> ./main.go /code/<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">WORKDIR</span><span style="color:#e6db74"> /code</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> CGO_ENABLED<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> GOOS<span style="color:#f92672">=</span>linux go build -a -installsuffix cgo -o /idlerpg-checker .<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">FROM</span><span style="color:#e6db74"> alpine:latest</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> apk --no-cache add ca-certificates<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">COPY</span> --from<span style="color:#f92672">=</span>build /idlerpg-checker /idlerpg-checker<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> echo <span style="color:#e6db74">&#34;v0.0.1&#34;</span> &gt;&gt; .version<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">ENTRYPOINT</span> [<span style="color:#e6db74">&#34;/idlerpg-checker&#34;</span>]<span style="color:#960050;background-color:#1e0010">
</span></span></span></code></pre></div><p>And the corresponding cronjob resource definition:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">batch/v1beta1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">CronJob</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">idle-checker</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">idle-checker</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">schedule</span>: <span style="color:#e6db74">&#34;*/20 * * * *&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">jobTemplate</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">template</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>          - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">idle-checker</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">image</span>: <span style="color:#ae81ff">skarlso/idle-checker</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">imagePullPolicy</span>: <span style="color:#ae81ff">IfNotPresent</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">env</span>:
</span></span><span style="display:flex;"><span>              - <span style="color:#f92672">name</span>:  <span style="color:#ae81ff">MG_API_KEY</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">valueFrom</span>:
</span></span><span style="display:flex;"><span>                  <span style="color:#f92672">secretKeyRef</span>:
</span></span><span style="display:flex;"><span>                    <span style="color:#f92672">name</span>:  <span style="color:#ae81ff">idle-rpg-secret</span>
</span></span><span style="display:flex;"><span>                    <span style="color:#f92672">key</span>:  <span style="color:#ae81ff">MG_API_KEY</span>
</span></span><span style="display:flex;"><span>              - <span style="color:#f92672">name</span>:  <span style="color:#ae81ff">MG_DOMAIN</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">valueFrom</span>:
</span></span><span style="display:flex;"><span>                  <span style="color:#f92672">secretKeyRef</span>:
</span></span><span style="display:flex;"><span>                    <span style="color:#f92672">name</span>:  <span style="color:#ae81ff">idle-rpg-secret</span>
</span></span><span style="display:flex;"><span>                    <span style="color:#f92672">key</span>:  <span style="color:#ae81ff">MG_DOMAIN</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">args</span>: [<span style="color:#e6db74">&#39;-username&#39;</span>, <span style="color:#e6db74">&#39;username&#39;</span>, <span style="color:#e6db74">&#39;-email&#39;</span>, <span style="color:#e6db74">&#39;user@powerhouse.com&#39;</span>]
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">restartPolicy</span>: <span style="color:#ae81ff">OnFailure</span>
</span></span></code></pre></div><p>Aaaand, the secret for the API key:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Secret</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">idle-checker</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">idle-rpg-secret</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">type</span>: <span style="color:#ae81ff">Opaque</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">data</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">MG_API_KEY</span>: <span style="color:#ae81ff">asdf=</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">MG_DOMAIN</span>: <span style="color:#ae81ff">asdf==</span>
</span></span></code></pre></div><p>Done. Huh. This will run every 20 minutes and check if the user with username <code>username</code> is online. If not, it will send an email to the given email address. Your levels are safe.</p>
<h1 id="closing-words">Closing words<a hidden class="anchor" aria-hidden="true" href="#closing-words">#</a></h1>
<p>Phew. This has been quite the ride. The post is now really long, so I will split the rest out into a Part 2. That is, Athens and Monitoring.</p>
<p>Thank you for reading this far!</p>
<p>Cheers,
Gergely.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://skarlso.github.io/2019/10/01/killing-kubernetes-cluster/">
    <span class="title">« Prev</span>
    <br>
    <span>How I killed my entire Kubernetes cluster</span>
  </a>
  <a class="next" href="https://skarlso.github.io/2019/09/19/updated-face-recog-drawing/">
    <span class="title">Next »</span>
    <br>
    <span>Updated Face-recog architecture drawing</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://skarlso.github.io/">Ramblings of a cloud engineer</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
