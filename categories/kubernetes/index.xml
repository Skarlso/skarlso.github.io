<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes on Ramblings of a cloud engineer</title>
    <link>https://skarlso.github.io/categories/kubernetes/</link>
    <description>Recent content in Kubernetes on Ramblings of a cloud engineer</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-GB</language>
    <lastBuildDate>Thu, 23 Jul 2020 21:01:00 +0100</lastBuildDate>
    <atom:link href="https://skarlso.github.io/categories/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>How to deploy a Go (Golang) backend with a React frontend separately on Kubernetes - Part One</title>
      <link>https://skarlso.github.io/2020/07/23/kubernetes-deploy-golang-react-apps-separately-part1/</link>
      <pubDate>Thu, 23 Jul 2020 21:01:00 +0100</pubDate>
      <guid>https://skarlso.github.io/2020/07/23/kubernetes-deploy-golang-react-apps-separately-part1/</guid>
      <description>&lt;h1 id=&#34;intro&#34;&gt;Intro&lt;/h1&gt;
&lt;p&gt;Welcome. This is a longer post about how to deploy a Go backend with a React frontend
on Kubernetes as separate entities. Instead of the usual compiled together single binary Go
application, we are going to separate the two. Why? Because usually a React frontend is just a &amp;ldquo;static&amp;rdquo;
SPA app with very little requirements in terms of resources, while the Go backend does most of the
leg work, requiring a lot more resources.&lt;/p&gt;
&lt;p&gt;Part two of this will contain scaling, utilization configuration, health probes, readiness probes,
and how to make sure our application can run multiple instances without stepping on each other&amp;rsquo;s toes.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: This isn&amp;rsquo;t going to be a Kubernetes guide. Some knowledge is assumed.&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/kube/short-version.png&#34; alt=&#34;Give me the short version&#34;&gt;&lt;/p&gt;
&lt;p&gt;This post details a complex setup of an infrastructure with a second part coming on scaling and how to make
your application scalable in the first place by doing idempotent transactions or dealing with locking and
several instances of the same application not stepping on each other&amp;rsquo;s foot.&lt;/p&gt;
&lt;p&gt;This, part one, details how to deploy traditional REST + Frontend based application in Go + React, but not bundled
together as a single binary, instead having the backend separate from the frontend. They key in doing so is explained
at the &lt;a href=&#34;#ingress&#34;&gt;Ingress&lt;/a&gt; section when talking about routing specific URIs to the backend and frontend services.&lt;/p&gt;
&lt;p&gt;If you are familiar with Kubernetes and infrastructure setup, feel free to skip ahead to that section. Otherwise, enjoy
the drawings or the writing or both.&lt;/p&gt;
&lt;h2 id=&#34;technology&#34;&gt;Technology&lt;/h2&gt;
&lt;p&gt;The SPA app will be handled by &lt;a href=&#34;https://www.npmjs.com/package/serve&#34;&gt;Serve&lt;/a&gt; while the Go backend
will use &lt;a href=&#34;https://echo.labstack.com/&#34;&gt;Echo&lt;/a&gt;. The database will be Postgres.&lt;/p&gt;
&lt;p&gt;We are going to apply some best practices using Network Policies to cordon off traffic that we don&amp;rsquo;t
want to go outside.&lt;/p&gt;
&lt;p&gt;We will set up HTTPS using cert-manager and let&amp;rsquo;s encrypt. We&amp;rsquo;ll be using nginx as ingress
provider.&lt;/p&gt;
&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/kube/architect.png&#34; alt=&#34;Let me show you the code&#34;&gt;&lt;/p&gt;
&lt;p&gt;All, or most of the code, including the application can be found here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/staple-org&#34;&gt;Staple&lt;/a&gt;. The application is a simple reading list manager with
user handling, email sending and lots of database access.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s get to it then!&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-provider&#34;&gt;Kubernetes Provider&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/kube/audition.png&#34; alt=&#34;Difficult Choice&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start with the obvious one. Where do you would like to create your Kubernetes cluster?&lt;/p&gt;
&lt;p&gt;There are four major providers now-a-days. AWS &lt;a href=&#34;https://aws.amazon.com/eks/&#34;&gt;EKS&lt;/a&gt;, GCP &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;GKE&lt;/a&gt;,
Azure &lt;a href=&#34;https://azure.microsoft.com/en-us/services/kubernetes-service/&#34;&gt;AKS&lt;/a&gt; and DigitalOcean &lt;a href=&#34;https://www.digitalocean.com/products/kubernetes/&#34;&gt;DKE&lt;/a&gt;.
Personally, I prefer DO because, it&amp;rsquo;s a lot cheaper than the others. The downside is that DO only
provides ReadWriteOnce persistent volumes. This gets to be a problem when we are trying to update
and the new Pod can&amp;rsquo;t mount the volume because it&amp;rsquo;s already taken by the existing one. This can be
solved by a good ol NFS instance. But that&amp;rsquo;s another story.&lt;/p&gt;
&lt;p&gt;AWS&#39; was late to the party and their solution is quite fragile and the API is terrible. GCP is best in terms
of technicalities, api, handling, and updates. Azure is surprisingly good, however, the documentation is
most of the times out of date or even plain incorrect at some places.&lt;/p&gt;
&lt;h2 id=&#34;setup-basics&#34;&gt;Setup Basics&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/kube/owl.jpg&#34; alt=&#34;Owl&#34;&gt;&lt;/p&gt;
&lt;p&gt;To setup your Kubernetes instance, follow DigitalOcean&amp;rsquo;s Kubernetes Getting Started guide. It&amp;rsquo;s really simple.
When you have access to the cluster via kubectl I highly recommend using this tool: &lt;a href=&#34;https://github.com/derailed/k9s&#34;&gt;k9s&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s a flexible and quite handy tool for quick observations, logs, shells to pods, edits and generally following what&amp;rsquo;s
happening to your cluster.&lt;/p&gt;
&lt;p&gt;Now that we are all set with our own little cluster, it&amp;rsquo;s time to have some people move in. First, we are going to
install cert-manager.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: I&amp;rsquo;m not going to use Helm because I think it&amp;rsquo;s unnecessary in this setting. We aren&amp;rsquo;t going to install
these things in a highly configurable way and updating with helm is a pain in the butt. For example, for cert-manager
the update with helm takes several steps, whilst updating with a plain yaml file is just applying the next version
of the yaml file.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m not going to explain how to install cert-manager or nginx. I&amp;rsquo;ll link to their respective guides because frankly, they
are simple to follow and work out of the box.&lt;/p&gt;
&lt;p&gt;To install nginx, simply apply the yaml file located here: &lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/deploy/#digital-ocean&#34;&gt;DigitalOcean Nginx&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To install cert-manager follow this guide: &lt;a href=&#34;https://cert-manager.io/docs/installation/kubernetes/&#34;&gt;https://cert-manager.io/docs/installation/kubernetes/&lt;/a&gt;.
Follow the regular manifest install part, then ignore the Helm part and proceed with verification and then install
your issuer. I used a simple ACME/http01 issuer from here: &lt;a href=&#34;https://cert-manager.io/docs/configuration/acme/http01/&#34;&gt;https://cert-manager.io/docs/configuration/acme/http01/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: That acme configuration contains the &lt;strong&gt;staging&lt;/strong&gt; url. This is to test that things are working. Once you are
sure that everything is wired up correctly, switch that url to this one:
&lt;code&gt;https://acme-v02.api.letsencrypt.org/directory&lt;/code&gt; -&amp;gt; prod url. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cert-manager.io/v1alpha2&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ClusterIssuer&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;letsencrypt-prod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;acme&lt;/span&gt;:
    &lt;span style=&#34;color:#75715e&#34;&gt;# The ACME server URL&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;server&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://acme-v02.api.letsencrypt.org/directory&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# Email address used for ACME registration&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;email&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;your@email.com&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# Name of a secret used to store the ACME account private key&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;privateKeySecretRef&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;letsencrypt-prod&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# Enable the HTTP-01 challenge provider&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;solvers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;http01&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;ingress&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;class&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: I&amp;rsquo;m using a ClusterIssuer because I have multiple domains and multiple namespaces.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s it. Cert-manager and nginx should be up and running. Later on, we will create our own
ingress rules.&lt;/p&gt;
&lt;h2 id=&#34;domain&#34;&gt;Domain&lt;/h2&gt;
&lt;p&gt;Next, you&amp;rsquo;ll need a domain to bind too. There are a gazillion domain providers out there like
no-ip, GoDaddy, HostGator, Shopify and so on. Choose one which is available to you or has the best
prices.&lt;/p&gt;
&lt;p&gt;There are some good guides on how to choose a domain and where to create it.
For example: &lt;a href=&#34;https://domains.google/learning-center/5-things-to-watch-out-for-when-buying-a-domain/&#34;&gt;5 things to watch out for when buying a domain&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;the-application&#34;&gt;The application&lt;/h1&gt;
&lt;p&gt;Alright, let&amp;rsquo;s put together the application.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/kube/assemble.png&#34; alt=&#34;Assemble&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;structure&#34;&gt;Structure&lt;/h2&gt;
&lt;p&gt;Every piece of our infrastructure will be laid out in yaml files. I believe in infrastructure as code.
If you run a command you will most likely forget about it, unless it&amp;rsquo;s logged and / or is replayable.&lt;/p&gt;
&lt;p&gt;This is the structure I&amp;rsquo;m using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.
├── LICENSE
├── README.md
├── certificate_request
│   └── certificate_request.yml
├── configmaps
│   └── staple_initdb_script.yaml
├── database
│   ├── staple_db_deployment.yaml
│   ├── staple_db_network_policy.yaml
│   ├── staple_db_pvc.yaml
│   └── staple_db_service.yaml
├── namespace
│   └── staple_namespace.yaml
├── primer.sql
├── rbac
├── secrets
│   ├── staple_db_password.yaml
│   └── staple_mg_creds.yaml
├── staple-backend
│   ├── staple_deployment.yaml
│   └── staple_service.yaml
└── staple-frontend
    ├── staple_deployment.yaml
    └── staple_service.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;One other possible combination is, if you have multiple applications:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.
├── README.md
├── applications
│   ├── confluence
│   │   ├── db
│   │   │   ├── db_deployment.yaml
│   │   │   └── db_service.yaml
│   │   ├── deployment
│   │   │   └── deployment.yaml
│   │   ├── pvc
│   │   │   └── confluence_app_pvc.yaml
│   │   └── service
│   │       └── service.yaml
│   ├── gitea
│   │   ├── config
│   │   │   ├── app.ini
│   │   │   └── gitea_config_map.yaml
│   │   ├── db
│   │   │   ├── gitea_db_deployment.yaml
│   │   │   ├── gitea_db_network_policy.yaml
│   │   │   ├── gitea_db_pvc.yaml
│   │   │   └── gitea_db_service.yaml
│   │   ├── deployment
│   │   │   └── gitea_deployment.yaml
│   │   ├── pvc
│   │   │   └── gitea_app_pvc.yaml
│   │   └── service
│   │       └── gitea_service.yaml
├── cronjobs
│   ├── cronjob1
│   │   ├── Dockerfile
│   │   ├── README.md
│   │   ├── go.mod
│   │   ├── go.sum
│   │   ├── cron.yaml
│   │   └── main.go
├── ingress
│   ├── example1
│   │   ├── example1_ingress_resource.yaml
│   │   └── gitea_ssh_configmap.yaml
│   ├── example2
│   │   └── example2_ingress_resource.yaml
│   ├── lets-encrypt-issuer.yaml
│   └── nginx
│       ├── nginx-ingress-controller-deployment.yaml
│       └── nginx-ingress-controller-service.yaml
└── namespaces
    ├── example1_namespace.yaml
    ├── example2_namespace.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;namespace&#34;&gt;Namespace&lt;/h2&gt;
&lt;p&gt;Before we begin, we&amp;rsquo;ll create a namespace for our application to properly partition all our entities.&lt;/p&gt;
&lt;p&gt;To create a namespace we&amp;rsquo;ll use this yaml &lt;code&gt;example_namespace.yaml&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Namespace&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;example&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Apply this with &lt;code&gt;kubectl -f apply example_namespace.yaml&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;the-database&#34;&gt;The Database&lt;/h2&gt;
&lt;p&gt;Deploying a Postgres database on Kubernetes is actually really easy. You need five things to have a basic, but
relatively secure installation.&lt;/p&gt;
&lt;h3 id=&#34;secret&#34;&gt;Secret&lt;/h3&gt;
&lt;p&gt;The secret contains our password and our database user. In postgres, if you define a user using &lt;code&gt;POSTGRES_USER&lt;/code&gt;
postgres will create the user and a database with the user&amp;rsquo;s name. This could come from Vault too, but
the Kubernetes secret is usually enough since it should be a closed environment anyways. But for important information
I would definitely use an admission policy and some vault secret goodness. (Maybe another post?)&lt;/p&gt;
&lt;p&gt;Our secret looks like this:
database_secret.yaml&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-db-password&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;POSTGRES_PASSWORD&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cGFzc3dvcmQxMjM=&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;# This creates a user and a db with the same name.&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;POSTGRES_USER&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;c3RhcGxl&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To generate the base64 code for a password and a user, use:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;echo -n &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;password123&amp;#34;&lt;/span&gt; | base64
echo -n &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;username&amp;#34;&lt;/span&gt; | base64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&amp;hellip;and paste the result in the respective fields. Once done, apply with &lt;code&gt;kubectl -f apply database_secret.yaml&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;deployment&#34;&gt;Deployment&lt;/h3&gt;
&lt;p&gt;The deployment which configures our database. Looks something like this (database_deployment.yaml):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-db&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-db&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-db&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-db&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;postgres&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;postgres:11&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;POSTGRES_USER&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;POSTGRES_PASSWORD&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;secretKeyRef&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-db-password&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;POSTGRES_PASSWORD&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/lib/postgresql/data&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;subPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;data&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# important so it gets mounted correctly&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-db-data&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/docker-entrypoint-initdb.d/staple_initdb.sql&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;subPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple_initdb.sql&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bootstrap-script&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-db-data&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeClaim&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;claimName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;do-storage-staple-db&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bootstrap-script&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;configMap&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-initdb-script&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note the two volume mounts.&lt;/p&gt;
&lt;p&gt;The first one makes sure that our data isn&amp;rsquo;t lost when the database pod itself restarts. It creates a mount
to a persistent volume which is defined a few lines below by &lt;code&gt;persistentVolumeClaim&lt;/code&gt;. &lt;code&gt;subPath&lt;/code&gt; is important
in this case otherwise you&amp;rsquo;ll end up with a lost&amp;amp;found folder.&lt;/p&gt;
&lt;p&gt;The second mount is a postgres specific initialization file. Postgres will run that sql file when it
starts up. I&amp;rsquo;m using it to create my application&amp;rsquo;s schema.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;create&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;database&lt;/span&gt; staples;
&lt;span style=&#34;color:#66d9ef&#34;&gt;create&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;table&lt;/span&gt; users (email varchar(&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;), password text, confirm_code text, max_staples int);
&lt;span style=&#34;color:#66d9ef&#34;&gt;create&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;table&lt;/span&gt; staples (name varchar(&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;), id serial, content text, created_at &lt;span style=&#34;color:#66d9ef&#34;&gt;timestamp&lt;/span&gt;, archived bool, user_email varchar(&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;));
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And it comes from a configmap which looks like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ConfigMap&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-initdb-script&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;staple_initdb.sql&lt;/span&gt;:
    &lt;span style=&#34;color:#ae81ff&#34;&gt;create table users (email varchar(255), password text, confirm_code text, max_staples int);&lt;/span&gt;
    &lt;span style=&#34;color:#ae81ff&#34;&gt;create table staples (name varchar(255), id serial, content text, created_at timestamp, archived bool, user_email varchar(255));&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;network-policy&#34;&gt;Network Policy&lt;/h3&gt;
&lt;p&gt;Network policies are important if you value your privacy. They restrict a PODs communication to a certain namespace
OR even to between applications only. By default I like to deny all traffic and then slowly open the valve until everything works.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/kube/szaffi.png&#34; alt=&#34;Szaffi&#34;&gt;
Kudos if you know who this is. (mind my terrible drawing capabilities)&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll use a basic network policy which will restrict the DB to talk to anything BUT the backend. Nothing else
will be able to talk to this Pod.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-db-network-policy&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-db&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Egress&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ingress&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;5432&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;egress&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;to&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;5432&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The important bit here is the &lt;code&gt;podSelector&lt;/code&gt; part. The label will be the label used by the application deployment.
This will restrict the Pod&amp;rsquo;s incoming and outgoing traffic to that of the application Pod including denying internet
traffic.&lt;/p&gt;
&lt;h3 id=&#34;pvc&#34;&gt;PVC&lt;/h3&gt;
&lt;p&gt;The persistent volume claim definition is straight forward:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;do-storage-staple-db&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10Gi&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;do-block-storage&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;10 gigs should be enough anything.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/kube/gates.png&#34; alt=&#34;Gates&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;service&#34;&gt;Service&lt;/h3&gt;
&lt;p&gt;The service will expose the database deployment to our cluster.&lt;/p&gt;
&lt;p&gt;Our service is fairly basic:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-db-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;5432&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-db&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;clusterIP&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;None&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;That&amp;rsquo;s done with the database. Next up is the backend.&lt;/p&gt;
&lt;h2 id=&#34;the-backend&#34;&gt;The backend&lt;/h2&gt;
&lt;p&gt;The backend itself is written in a way that it doesn&amp;rsquo;t require a persistent storage so
we can skip that part. It only needs three pieces. A secret, a deployment definition and the
service exposing the deployment.&lt;/p&gt;
&lt;h3 id=&#34;secret-1&#34;&gt;Secret&lt;/h3&gt;
&lt;p&gt;First, we create a secret which contains Mailgun credentials.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-mg-creds&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;MG_DOMAIN&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cGFzc3dvcmQxMjM=&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;MG_API_KEY&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cGFzc3dvcmQxMjM=&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;database-connection&#34;&gt;Database connection&lt;/h3&gt;
&lt;p&gt;The connection settings are handled through the same secret which is used to spin up the DB itself.
We have to only mount that here too and we are good.&lt;/p&gt;
&lt;h3 id=&#34;deployment-1&#34;&gt;Deployment&lt;/h3&gt;
&lt;p&gt;Which brings us to the deployment. This is a bit more involved.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-app&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;skarlso/staple:v0.1.0&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;imagePullPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IfNotPresent&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;memory&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;500Mi&amp;#34;&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;cpu&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;250m&amp;#34;&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;limits&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;memory&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1000Mi&amp;#34;&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;cpu&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;500m&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;POD_NAMESPACE&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;fieldRef&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;fieldPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;metadata.namespace&lt;/span&gt;
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DB_PASSWORD&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;secretKeyRef&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-db-password&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;POSTGRES_PASSWORD&lt;/span&gt;
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MG_DOMAIN&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;secretKeyRef&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-mg-creds&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MG_DOMAIN&lt;/span&gt;
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MG_API_KEY&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;secretKeyRef&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-mg-creds&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MG_API_KEY&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;:
          - --&lt;span style=&#34;color:#ae81ff&#34;&gt;staple-db-hostname=staple-db-service.cronohub.svc.cluster.local:5432&lt;/span&gt;
          - --&lt;span style=&#34;color:#ae81ff&#34;&gt;staple-db-username=staple&lt;/span&gt;
          - --&lt;span style=&#34;color:#ae81ff&#34;&gt;staple-db-database=staple&lt;/span&gt;
          - --&lt;span style=&#34;color:#ae81ff&#34;&gt;staple-db-password=$(DB_PASSWORD)&lt;/span&gt;
          - --&lt;span style=&#34;color:#ae81ff&#34;&gt;mg-domain=$(MG_DOMAIN)&lt;/span&gt;
          - --&lt;span style=&#34;color:#ae81ff&#34;&gt;mg-api-key=$(MG_API_KEY)&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-port&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9998&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;There are a few important points here and I won&amp;rsquo;t explain them all, like the resource restrictions,
which you should be familiar with by now. I&amp;rsquo;m using a mix of 12factor app&amp;rsquo;s environment configuration
and command line arguments for the application configuration. The app itself is not using os.Environ
but the args.&lt;/p&gt;
&lt;p&gt;The args point to the cluster local dns of the database, some db settings, and the mailgun credentials.&lt;/p&gt;
&lt;p&gt;It also exposes the container port 9998 which is Echo&amp;rsquo;s default port.&lt;/p&gt;
&lt;p&gt;Now all we need is the service.&lt;/p&gt;
&lt;h3 id=&#34;service-1&#34;&gt;Service&lt;/h3&gt;
&lt;p&gt;Without much fanfare:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-service&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service-port&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9998&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-port&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And with this, the backend is done.&lt;/p&gt;
&lt;h2 id=&#34;the-frontend&#34;&gt;The frontend&lt;/h2&gt;
&lt;p&gt;The frontend, similarly to the backend, does not require a persistent volume. We can skip that one too.&lt;/p&gt;
&lt;p&gt;In fact it only needs two things, a deployment and a service, and that&amp;rsquo;s all. It uses serve to host the
static files. Honestly, that could also be a Go application serving the static content or anything
that can serve static files.&lt;/p&gt;
&lt;h3 id=&#34;deployment-2&#34;&gt;Deployment&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-frontend&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-frontend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-frontend&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-frontend&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-frontend&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-frontend&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-frontend&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;skarlso/staple-frontend:v0.0.9&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;imagePullPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IfNotPresent&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;memory&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;500Mi&amp;#34;&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;cpu&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;250m&amp;#34;&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;limits&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;memory&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1000Mi&amp;#34;&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;cpu&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;500m&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;POD_NAMESPACE&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;fieldRef&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;fieldPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;metadata.namespace&lt;/span&gt;
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;REACT_APP_STAPLE_DEV_HOST&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-front&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;5000&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;service-2&#34;&gt;Service&lt;/h3&gt;
&lt;p&gt;And the service:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-front-service&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-frontend&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-frontend&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-frontend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-frontend&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-frontend&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-frontend&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-front&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;5000&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-front&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And with that the backend and frontend are wired together and ready to receive traffic.&lt;/p&gt;
&lt;p&gt;All pods should be up and running without problems at this point. If you have any trouble deploying
things, please don&amp;rsquo;t hesitate to leave a question in the comments.&lt;/p&gt;
&lt;h2 id=&#34;ingress&#34;&gt;Ingress&lt;/h2&gt;
&lt;p&gt;Fantastic. Now, our application is running. We just need to expose it and route traffic to it.
The backend has the api route &lt;code&gt;/rest/api/v1/&lt;/code&gt;. The frontend has the route syntax &lt;code&gt;/login&lt;/code&gt;, &lt;code&gt;/register&lt;/code&gt;
and a bunch of others. The key here is that all of them are under the same domain name but based on the URI
we need to direct one request to the backend the other to the frontend.&lt;/p&gt;
&lt;p&gt;This is done via nginx&amp;rsquo;s routing logic using regex. In an nginx config this would be the &lt;code&gt;location&lt;/code&gt; part.
It&amp;rsquo;s imperative that the order of the routing is from more specific towards more general Because we need to catch
the specific URIs first.&lt;/p&gt;
&lt;h3 id=&#34;ingress-resource&#34;&gt;Ingress Resource&lt;/h3&gt;
&lt;p&gt;To do this, we will create something called an &lt;a href=&#34;https://docs.nginx.com/nginx-ingress-controller/configuration/ingress-resources/&#34;&gt;Ingress Resource&lt;/a&gt;.
Note that this is Nginx&amp;rsquo;s ingress resource and not Kubernetes&#39;. There is a difference.&lt;/p&gt;
&lt;p&gt;I suggest reading up on that link about the ingress resource because it reads quite well and will explain how it
works and fits into the Kubernetes environment.&lt;/p&gt;
&lt;p&gt;Got it? Good. We&amp;rsquo;ll create one for &lt;code&gt;staple.app&lt;/code&gt; domain:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-app-ingress&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;kubernetes.io/ingress.class&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nginx&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;cert-manager.io/cluster-issuer&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;letsencrypt-prod&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;cert-manager.io/acme-challenge-type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;http01&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;nginx.ingress.kubernetes.io/rewrite-target&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/$1&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# this is important&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;tls&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;hosts&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;staple.app&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-app-tls&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple.app&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;serviceName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-service&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;servicePort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ss-port&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# 9998&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/(rest/api/1.*)&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple.app&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;serviceName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;staple-front-service&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;servicePort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;sfs-port&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# 5000&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/(.*)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s take a look at what&amp;rsquo;s going on here. The first thing to catch the eye are the annotations.
These are configuration settings for nginx, cert-manager and Kubernetes.
We have the cluster issuer&amp;rsquo;s name. The challenge type, which we decided should be http01,
and the most important part, the rewrite-target setting. This will use the first capture group
as a base after the host.&lt;/p&gt;
&lt;p&gt;With this rewrite rule in place, the &lt;code&gt;paths&lt;/code&gt; values need to provide a capture group. The first in line will see
everything that goes to the urls like: &lt;code&gt;staple.app/rest/api/1/token&lt;/code&gt;, &lt;code&gt;staple.app/rest/api/1/staples&lt;/code&gt;,
&lt;code&gt;staple.app/rest/api/1/user&lt;/code&gt;, etc. The first part of the url is the host &lt;code&gt;staple.app&lt;/code&gt;, second part is &lt;code&gt;/(rest/api/1/.*)&lt;/code&gt;
for which the result is that group number one ($1) will be &lt;code&gt;rest/api/1/token&lt;/code&gt;. Nginx now sees that we
have a backend route for that and will send this URI along to the service. Our service picks it up
and will match that URI to the router configuration.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/kube/regex.png&#34; alt=&#34;Regex&#34;&gt;&lt;/p&gt;
&lt;p&gt;If there is a request like, &lt;code&gt;staple.app/login&lt;/code&gt;, which is our frontend&amp;rsquo;s job to pick up, the first rule
will not catch it because the regex isn&amp;rsquo;t matching, so it falls through to the second one, which
is the frontend service that is using a &amp;ldquo;catch all&amp;rdquo; regex. Like ip tables, we go from
specific to more general.&lt;/p&gt;
&lt;h1 id=&#34;ending-words&#34;&gt;Ending words&lt;/h1&gt;
&lt;p&gt;And that&amp;rsquo;s it. If everything works correctly, then the certificate service wired up the https certs and
we should be able ping the rest endpoint under &lt;code&gt;https://staple.app/rest/api/1/token&lt;/code&gt; and log in to the app
in the browser using &lt;code&gt;https://staple.app&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Stay tuned for the second part where we&amp;rsquo;ll scale the thing up!&lt;/p&gt;
&lt;p&gt;Thanks for reading!
Gergely.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using a Kubernetes based Cluster for Various Services with auto HTTPS - Part 2</title>
      <link>https://skarlso.github.io/2019/10/15/kubernetes-cluster-part2/</link>
      <pubDate>Tue, 15 Oct 2019 21:01:00 +0100</pubDate>
      <guid>https://skarlso.github.io/2019/10/15/kubernetes-cluster-part2/</guid>
      <description>&lt;h1 id=&#34;intro&#34;&gt;Intro&lt;/h1&gt;
&lt;p&gt;Hi folks.&lt;/p&gt;
&lt;p&gt;This is a continuation of the previous post about my Kubernetes infrastructure located &lt;a href=&#34;https://skarlso.github.io/2019/09/21/kubernetes-cluster/&#34;&gt;here&lt;/a&gt;. The two remaining points are to deploy Athens Go proxy and setting up monitoring.&lt;/p&gt;
&lt;h1 id=&#34;athens&#34;&gt;Athens&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/hosting/athens.png&#34; alt=&#34;Athens&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start with &lt;a href=&#34;https://github.com/gomods/athens&#34;&gt;Athens&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First of all if you are a helm user, Athens has an awesome set of helm charts which you can use to deploy it in your cluster.
Located &lt;a href=&#34;https://github.com/gomods/athens/tree/master/charts/athens-proxy&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I prefer to deploy my own config files, but that&amp;rsquo;s me. So here is my preferred way of deploying Athens.&lt;/p&gt;
&lt;p&gt;Since this is also a subdomain of the previously created &lt;code&gt;powerhouse&lt;/code&gt; namespace we are going to use that.&lt;/p&gt;
&lt;h2 id=&#34;pvc&#34;&gt;PVC&lt;/h2&gt;
&lt;p&gt;We are going to need a PersistentVolumeClaim for Athens so it can store all the things forever.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;powerhouse&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-storage&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10Gi&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;do-block-storage&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Claim is very boring. Which means it just works.&lt;/p&gt;
&lt;h2 id=&#34;deployment&#34;&gt;Deployment&lt;/h2&gt;
&lt;p&gt;This is more interesting. Athens provides a lot of possibilities for the deployment. I&amp;rsquo;m just deploying the barest possible here. Which means, no user auth, no private repository support, ssh key configuration, etc&amp;hellip; It&amp;rsquo;s a plain proxy installation.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;powerhouse&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-app&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-proxy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-proxy&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-proxy&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-proxy&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-proxy&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-proxy&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gomods/athens:v0.6.0&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;livenessProbe&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;httpGet&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/healthz&amp;#34;&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3000&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;readinessProbe&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;httpGet&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/readyz&amp;#34;&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3000&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ATHENS_GOGET_WORKERS&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;3&amp;#34;&lt;/span&gt;
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ATHENS_STORAGE_TYPE&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;disk&amp;#34;&lt;/span&gt;
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ATHENS_DISK_STORAGE_ROOT&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/lib/athens&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3000&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-http&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-data&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/lib/athens&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;subPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-data&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeClaim&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;claimName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-storage&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Fun fact. The name of the app must not be just plain &lt;code&gt;athens&lt;/code&gt; because that will result in an error: &lt;code&gt;too many colons in address&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The issue is here: &lt;a href=&#34;https://github.com/gomods/athens/issues/1038#issuecomment-457145658&#34;&gt;https://github.com/gomods/athens/issues/1038#issuecomment-457145658&lt;/a&gt; Basically it&amp;rsquo;s because of the name used for the environment properties inside the container.&lt;/p&gt;
&lt;h2 id=&#34;service&#34;&gt;Service&lt;/h2&gt;
&lt;p&gt;Now, let&amp;rsquo;s expose it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;powerhouse&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-service&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-proxy&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-proxy&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-proxy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-proxy&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-proxy&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-proxy&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3000&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;ingress&#34;&gt;Ingress&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;m using port 80 here because it&amp;rsquo;s convenient. But if you use any other port, don&amp;rsquo;t forget to alter your ingress to forward to that port and service.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
...
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;tls&lt;/span&gt;:
...
  - &lt;span style=&#34;color:#f92672&#34;&gt;hosts&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;athens.powerhouse.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-cronohub-tls&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
...
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens.powerhouse.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;serviceName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-service&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;servicePort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1234&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And that&amp;rsquo;s it! If you now visit &lt;code&gt;https://athens.powerhouse.com&lt;/code&gt; it should say &lt;code&gt;&amp;quot;Welcome to The Athens Proxy&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now, if you set this proxy with &lt;code&gt;export GOPROXY=https://athens.powerhouse.com&lt;/code&gt; it should start to cache modules. It&amp;rsquo;s a fantastic proxy with a lot of capabilities. I encourage you to check it out and drop by it&amp;rsquo;s slack channel on Gopher slack called Athens.&lt;/p&gt;
&lt;h1 id=&#34;monitoring&#34;&gt;Monitoring&lt;/h1&gt;
&lt;p&gt;Monitoring is a huge topic so I&amp;rsquo;m not going to talk about how to monitor or what. That is described in great many of posts. I especially recommend reading sysdig&amp;rsquo;s 6 part post on doing monitoring with Prometheus and Grafana and what to monitor and the four golden signals and whatnot. Starting &lt;a href=&#34;https://sysdig.com/blog/kubernetes-monitoring-prometheus/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://sysdig.com/blog/monitoring-kubernetes-with-sysdig-cloud/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;prometheus&#34;&gt;Prometheus&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;m going to deploy &lt;a href=&#34;https://prometheus.io&#34;&gt;Prometheus&lt;/a&gt;. Prometheus is a monitoring tool which sits inside your cluster and gathers data about running pods, nodes, services, whatever you expose and wants to send data to it. It can also alert on things and can be integrated with tools like Graphana for nice front-end and metrics. Prometheus itself uses PromQL as its query language to gather data from different sources and do time series analytics and much much more.&lt;/p&gt;
&lt;p&gt;Please visit the website and documentation for more details. It&amp;rsquo;s the defacto monitoring tool for Kubernetes. Again, I&amp;rsquo;m going to do a very basic installation of Prometheus. So basic in fact, that I don&amp;rsquo;t even have a PVC for it, because I don&amp;rsquo;t care at this point about retaining data.&lt;/p&gt;
&lt;h3 id=&#34;namespace&#34;&gt;Namespace&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s create it&amp;rsquo;s own namespace.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Namespace&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;monitoring&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;monitoring&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;config&#34;&gt;Config&lt;/h3&gt;
&lt;p&gt;Prometheus Server config is massive. I don&amp;rsquo;t expect you to pick up on everything in this thing, but I would encourage you to at least try to find out what these setting do&amp;hellip; Our config yaml file contains the configuration file for Prometheus which we&amp;rsquo;ll later set up via a command line argument. It&amp;rsquo;s called &lt;code&gt;prometheus.yml&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ConfigMap&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prometheus-server-conf&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prometheus-server-conf&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;monitoring&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;prometheus.yml&lt;/span&gt;: |-&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    global:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      scrape_interval: 5s
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      evaluation_interval: 5s
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    scrape_configs:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      - job_name: &amp;#39;kubernetes-apiservers&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        kubernetes_sd_configs:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - role: endpoints
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        scheme: https
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        tls_config:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        relabel_configs:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          action: keep
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          regex: default;kubernetes;https
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      - job_name: &amp;#39;kubernetes-nodes&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        scheme: https
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        tls_config:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        kubernetes_sd_configs:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - role: node
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        relabel_configs:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - action: labelmap
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          regex: __meta_kubernetes_node_label_(.+)
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - target_label: __address__
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          replacement: kubernetes.default.svc:443
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - source_labels: [__meta_kubernetes_node_name]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          regex: (.+)
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          target_label: __metrics_path__
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          replacement: /api/v1/nodes/${1}/proxy/metrics
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      - job_name: &amp;#39;kubernetes-pods&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        kubernetes_sd_configs:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - role: pod
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        relabel_configs:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          action: keep
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          regex: true
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          action: replace
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          target_label: __metrics_path__
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          regex: (.+)
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          action: replace
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          regex: ([^:]+)(?::\d+)?;(\d+)
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          replacement: $1:$2
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          target_label: __address__
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - action: labelmap
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          regex: __meta_kubernetes_pod_label_(.+)
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - source_labels: [__meta_kubernetes_namespace]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          action: replace
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          target_label: kubernetes_namespace
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - source_labels: [__meta_kubernetes_pod_name]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          action: replace
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          target_label: kubernetes_pod_name
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      - job_name: &amp;#39;kubernetes-cadvisor&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        scheme: https
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        tls_config:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        kubernetes_sd_configs:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - role: node
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        relabel_configs:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - action: labelmap
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          regex: __meta_kubernetes_node_label_(.+)
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - target_label: __address__
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          replacement: kubernetes.default.svc:443
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - source_labels: [__meta_kubernetes_node_name]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          regex: (.+)
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          target_label: __metrics_path__
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      - job_name: &amp;#39;kubernetes-service-endpoints&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        kubernetes_sd_configs:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - role: endpoints
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        relabel_configs:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          action: keep
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          regex: true
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          action: replace
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          target_label: __scheme__
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          regex: (https?)
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          action: replace
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          target_label: __metrics_path__
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          regex: (.+)
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          action: replace
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          target_label: __address__
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          regex: ([^:]+)(?::\d+)?;(\d+)
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          replacement: $1:$2
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - action: labelmap
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          regex: __meta_kubernetes_service_label_(.+)
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - source_labels: [__meta_kubernetes_namespace]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          action: replace
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          target_label: kubernetes_namespace
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        - source_labels: [__meta_kubernetes_service_name]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          action: replace
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;          target_label: kubernetes_name&lt;/span&gt;    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Mostly it&amp;rsquo;s just setting up what Prometheus should monitor and how. The important bits are the &lt;code&gt;labels&lt;/code&gt;. How this is going to work is, that we will &lt;code&gt;annotate&lt;/code&gt; the resources we want Prometheus to see. Which is pretty cool. Basically we will just alter a pod to include an annotation and it will begin monitoring it. No need to install anything anywhere or restart anything. Just add an annotation and bamm, you&amp;rsquo;re done.&lt;/p&gt;
&lt;h2 id=&#34;rbac&#34;&gt;RBAC&lt;/h2&gt;
&lt;p&gt;Prometheus needs permissions to access resources in the cluster such as API end-points and gathering data about the cluster itself. We will provide it with this permission through &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/rbac/&#34;&gt;Role Based Access Control&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll create a service account which Prometheus can use. We want it to access the whole cluster so we&amp;rsquo;ll use a &lt;code&gt;ClusterRole&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;rbac.authorization.k8s.io/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ClusterRole&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prometheus&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
- &lt;span style=&#34;color:#f92672&#34;&gt;apiGroups&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;]
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;nodes&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;nodes/proxy&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;services&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;endpoints&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;pods&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;verbs&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;get&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;list&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;watch&amp;#34;&lt;/span&gt;]
- &lt;span style=&#34;color:#f92672&#34;&gt;apiGroups&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;extensions&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;ingresses&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;verbs&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;get&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;list&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;watch&amp;#34;&lt;/span&gt;]
- &lt;span style=&#34;color:#f92672&#34;&gt;nonResourceURLs&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/metrics&amp;#34;&lt;/span&gt;]
  &lt;span style=&#34;color:#f92672&#34;&gt;verbs&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;get&amp;#34;&lt;/span&gt;]
---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;rbac.authorization.k8s.io/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ClusterRoleBinding&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prometheus&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;roleRef&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;apiGroup&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;rbac.authorization.k8s.io&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ClusterRole&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prometheus&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;subjects&lt;/span&gt;:
- &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ServiceAccount&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;monitoring&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will give access to monitor the following resources: nodes, nodes/proxy, services, endpoints and pods. The action are get, list, watch. No modifications.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll also allow Prometheus to watch ingresses for data traffic and allow it to do get requests to non-resource endpoint &lt;code&gt;/metrics&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;deployment-1&#34;&gt;Deployment&lt;/h2&gt;
&lt;p&gt;Now, the deployment is actually pretty easy.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prometheus-deployment&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;monitoring&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prometheus-server&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prometheus&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prom/prometheus:v2.2.1&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;:
            - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;--config.file=/etc/prometheus/prometheus.yml&amp;#34;&lt;/span&gt;
            - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;--storage.tsdb.path=/prometheus/&amp;#34;&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
            - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9090&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
            - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prometheus-config-volume&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/etc/prometheus/&lt;/span&gt;
            - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prometheus-storage-volume&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/prometheus/&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prometheus-config-volume&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;configMap&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;defaultMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;420&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prometheus-server-conf&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prometheus-storage-volume&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;emptyDir&lt;/span&gt;: {}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The two interesting things here are the two arguments. The config file, which we include through the &lt;code&gt;configMap&lt;/code&gt; and the storage. Which I&amp;rsquo;m not bind mounting.&lt;/p&gt;
&lt;h2 id=&#34;service-1&#34;&gt;Service&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s expose Prometheus. Now, this may come as a surprise if you don&amp;rsquo;t know anything about Prometheus, but this is an in cluster monitoring tool. It&amp;rsquo;s usually not supposed to be accessed directly, but through tools like Graphana or used by tools like Alerting or traefik as a reverse proxy. As such, Prometheus does not support authentication or authorization or user management of any kind. That is usually taken care of by a reverse proxy or other means written about &lt;a href=&#34;https://prometheus.io/docs/operating/security/#authentication-authorization-and-encryption&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://prometheus.io/docs/guides/basic-auth/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As such, we can do a number of things. We can expose it as a NodePort service for example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prometheus-service&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;monitoring&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;prometheus.io/scrape&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;true&amp;#39;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;prometheus.io/port&lt;/span&gt;:   &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;9090&amp;#39;&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prometheus-server&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NodePort&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9090&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;nodePort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;30000&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Or we just port forward the pod like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;k port-forward pods/prometheus-deployment-6bf45557bd-qc6t6 9090:9090 -n monitoring
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And access it by simply opening the url: http://127.0.0.1:9090.&lt;/p&gt;
&lt;h2 id=&#34;prometheus-1&#34;&gt;Prometheus&lt;/h2&gt;
&lt;p&gt;Once you open it, you should see something like this, after running a small query:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/hosting/prometheus.png&#34; alt=&#34;prometheus.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;adding-in-resources-to-monitor&#34;&gt;Adding in Resources to monitor&lt;/h2&gt;
&lt;p&gt;In order to add a resource to monitor simply insert these annotations:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;prometheus.io/scrape&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;true&amp;#39;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;prometheus.io/port&lt;/span&gt;:   &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;9090&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Done.&lt;/p&gt;
&lt;h1 id=&#34;bonus-round----graphana&#34;&gt;Bonus Round &amp;ndash; Graphana&lt;/h1&gt;
&lt;p&gt;We deployed Athens and Prometheus to monitor our cluster. We don&amp;rsquo;t have anything before Prometheus that would be fancy, but installing Graphana is actually pretty easy. You can follow the instructions &lt;a href=&#34;https://prometheus.io/docs/visualization/grafana/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A very easy way of looking at some nice metrics without worrying about anything like users and such, is running a Graphana instance in docker on your local machine with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker run -d -p 3000:3000 grafana/grafana
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&amp;hellip; and while you are forwarding the Prometheus end-point you navigate to your Graphana instance by opening &lt;code&gt;127.0.0.1:3000&lt;/code&gt; and install a Prometheus data-point like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/hosting/graphana_config.png&#34; alt=&#34;graphana config&#34;&gt;&lt;/p&gt;
&lt;p&gt;After that navigate to a new dashboard and select a simple PromQL metric to see if it&amp;rsquo;s working. You should see something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/hosting/graphana.png&#34; alt=&#34;graphana&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now you can create a new dashboard add a PVC to our Prometheus instance and enjoy all the metrics you can store!&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;And this is it folks. Everything is installed and we can monitor things now. If you give Prometheus a PVC you can build some pretty awesome time series graphs too and see how your cluster behaves over time.&lt;/p&gt;
&lt;p&gt;Thank you for reading!
Gergely.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How I killed my entire Kubernetes cluster</title>
      <link>https://skarlso.github.io/2019/10/01/killing-kubernetes-cluster/</link>
      <pubDate>Tue, 01 Oct 2019 21:01:00 +0100</pubDate>
      <guid>https://skarlso.github.io/2019/10/01/killing-kubernetes-cluster/</guid>
      <description>&lt;h1 id=&#34;intro&#34;&gt;Intro&lt;/h1&gt;
&lt;p&gt;One morning I woke up and tried to access my gitea just to find that it wasn&amp;rsquo;t running.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/kube_dead.png&#34; alt=&#34;dead kube&#34;&gt;&lt;/p&gt;
&lt;p&gt;I checked my cluster and found that the whole thing was dead as meat. I quickly jumped in and ran &lt;code&gt;k get pods -A&lt;/code&gt; to see what&amp;rsquo;s
going on. None of my services worked.&lt;/p&gt;
&lt;p&gt;What immediately struck my eye was a 100+ pods of my fork_updater cronjob. The fork_updater cronjob which runs once a month, looks
like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;batch/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;CronJob&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fork-updater&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fork-updater&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;schedule&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;* * 1 * *&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;jobTemplate&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fork-updater-ssh-key&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fork-updater-ssh-key&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;defaultMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;256&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# yaml spec does not support octal mode&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fork-updater&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;imagePullPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IfNotPresent&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;skarlso/repo-updater:1.0.4&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;:
              - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;:  &lt;span style=&#34;color:#ae81ff&#34;&gt;GIT_TOKEN&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
                  &lt;span style=&#34;color:#f92672&#34;&gt;secretKeyRef&lt;/span&gt;:
                    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;:  &lt;span style=&#34;color:#ae81ff&#34;&gt;fork-updater-secret&lt;/span&gt;
                    &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;:  &lt;span style=&#34;color:#ae81ff&#34;&gt;GIT_TOKEN&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
            - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fork-updater-ssh-key&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/etc/secret&amp;#34;&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;OnFailure&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Inherently there is nothing wrong with this at first glance. But on a second glance, the problem is &lt;code&gt;restartPolicy: Always&lt;/code&gt;.
For whatever the reason, the cronjob died when it started up. The restart policy then&amp;hellip; restarted the cronjob, which failed again
really fast. Then it scheduled a new one and a new one and a new one&amp;hellip; and I had 100+ containers pending and running and
creating.&lt;/p&gt;
&lt;p&gt;At that point the cluster was basically DDOSd into oblivion. Once the other resources started to die ( since this was a private
cluster and I didn&amp;rsquo;t bother to set up restrictions on resources ) the cronjob hogged even more and it basically blocked everything
else from being able to run. It overwhelmed the scheduler.&lt;/p&gt;
&lt;p&gt;Lovevly that.&lt;/p&gt;
&lt;p&gt;This is how you could potentionally kill a cluster which doesn&amp;rsquo;t have any resource limits and restrictions set up.&lt;/p&gt;
&lt;p&gt;Gergely.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using a Kubernetes based Cluster for Various Services with auto HTTPS</title>
      <link>https://skarlso.github.io/2019/09/21/kubernetes-cluster/</link>
      <pubDate>Sat, 21 Sep 2019 21:01:00 +0100</pubDate>
      <guid>https://skarlso.github.io/2019/09/21/kubernetes-cluster/</guid>
      <description>&lt;h1 id=&#34;intro&#34;&gt;Intro&lt;/h1&gt;
&lt;p&gt;Hi folks.&lt;/p&gt;
&lt;p&gt;Today, I would like to show you how my infrastructure is deployed and managed. Spoiler alert, I&amp;rsquo;m using Kubernetes to do that.&lt;/p&gt;
&lt;p&gt;I know&amp;hellip; What a twist!&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s get to it.&lt;/p&gt;
&lt;h1 id=&#34;what&#34;&gt;What&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/hosting/kube-architecture.png&#34; alt=&#34;kube-architecture&#34;&gt;&lt;/p&gt;
&lt;p&gt;What services am I running exactly? Here is a list I&amp;rsquo;m running at the time of this writing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Athens Go Proxy&lt;/li&gt;
&lt;li&gt;Gitea&lt;/li&gt;
&lt;li&gt;The Lounge (IRC bouncer)&lt;/li&gt;
&lt;li&gt;Two CronJobs
&lt;ul&gt;
&lt;li&gt;Fork Updater&lt;/li&gt;
&lt;li&gt;IDLE RPG online checker&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;My WebSite (gergelybrautigam.com)&lt;/li&gt;
&lt;li&gt;Monitoring&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And it&amp;rsquo;s really simple to add more.&lt;/p&gt;
&lt;h1 id=&#34;where&#34;&gt;Where&lt;/h1&gt;
&lt;p&gt;My cluster is deployed at DigitalOcean using two droplets each 1vCPU and 2GB RAM.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/hosting/kube-on-digitalocean.png&#34; alt=&#34;kube-on-digitalocean&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;what-not&#34;&gt;What Not&lt;/h1&gt;
&lt;p&gt;This isn&amp;rsquo;t going to be a production grade cluster. What I don&amp;rsquo;t include in here:&lt;/p&gt;
&lt;h2 id=&#34;rbac-for-various-services-and-users&#34;&gt;RBAC for various services and users&lt;/h2&gt;
&lt;p&gt;Since I&amp;rsquo;m the only user of my cluster I didn&amp;rsquo;t create any kind of access limits / users or such. You are free to create them though. The only role based auth that&amp;rsquo;s going on is for Prometheus.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m not using any third party things which require access to the API.&lt;/p&gt;
&lt;h2 id=&#34;resource-limitation&#34;&gt;Resource limitation&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;m the sole user of my things. I&amp;rsquo;m not really scaling my gitea up or down based on usage and as such, I did not define things like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Resource limits&lt;/li&gt;
&lt;li&gt;Nodes with certain capabilities&lt;/li&gt;
&lt;li&gt;Affinities and Taints &amp;ndash; which means, everything can run anywhere&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;readiness-liveliness&#34;&gt;Readiness Liveliness&lt;/h2&gt;
&lt;p&gt;Most of by deploys and services don&amp;rsquo;t have these except for Athens.&lt;/p&gt;
&lt;h1 id=&#34;how&#34;&gt;How&lt;/h1&gt;
&lt;p&gt;Okay, with that out of the way, let&amp;rsquo;s get into the hows of things&amp;hellip;&lt;/p&gt;
&lt;h1 id=&#34;beginning&#34;&gt;Beginning&lt;/h1&gt;
&lt;p&gt;The most important thing that you need to do in order to use Kubernetes is Containerizing all the things.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/hosting/containers.png&#34; alt=&#34;containers&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since Kubernetes is a container orchestration tool, without containers it&amp;rsquo;s pretty useless.&lt;/p&gt;
&lt;p&gt;As a driver, I&amp;rsquo;m going to use Docker. Kubernetes can use anything that&amp;rsquo;s OCI compatible, which means if you would like to use runc as a container engine, you can do that. I&amp;rsquo;d like to keep my sanity though.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/hosting/fork-updater.png&#34; alt=&#34;fork-updater&#34;&gt;&lt;/p&gt;
&lt;p&gt;To show you what I mean&amp;hellip; I have a cronjob which is running every month. It gathers all my forks on github and updates them with the latest from their parents. This a small ruby script located here: &lt;a href=&#34;https://gist.github.com/Skarlso/fd5bd5971a78a5fa9760b31683de690e&#34;&gt;Fork Updater&lt;/a&gt;. How do we run this? It requires two things. First, a token. We pass that currently as an environment property. It could be in a file in a vault or a secret mounted in as a file it doesn&amp;rsquo;t matter. Currently, it&amp;rsquo;s an environment property. The second thing is more subtle.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m pushing the changes back into my remote forks. I&amp;rsquo;m doing this via SSH. So, we need a key in there too. How we&amp;rsquo;ll get that in there, I&amp;rsquo;ll talk about later when we are talking about how to set this cron job up. For now though, the container needs to look for a key in a specific location because we don&amp;rsquo;t want to over-mount &lt;code&gt;/root/.ssh/&lt;/code&gt; and we also don&amp;rsquo;t want to use an initContainer to copy over an SSH key (because it&amp;rsquo;s mounted in as a symlink (but that&amp;rsquo;s a different issue all together)). Also, we certianly do NOT want to have a key in the container.&lt;/p&gt;
&lt;p&gt;To achieve this, we simply set up a &lt;code&gt;config&lt;/code&gt; file for SSH like this one:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Host github.com
    IdentityFile /etc/secret/id_rsa
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;/etc/secret&lt;/code&gt; will be the destination of the ssh key we create.&lt;/p&gt;
&lt;p&gt;And we also need to have a known_hosts file, otherwise git clone will complain. We also bake this into the container. Why? Why not generate that on the fly? Because I want it to fail in case there is something wrong or there is a MIMA going on etc.&lt;/p&gt;
&lt;p&gt;All this translated into a Dockerfile looks like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Dockerfile&#34; data-lang=&#34;Dockerfile&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We are using alpine for a minimalistic image&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; alpine:latest&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; apk --no-cache add ca-certificates&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; apk update&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Openssh is needed for the SSH command&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; apk --no-cache add ruby vim curl git build-base ruby-dev openssh openssh-client&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Setup dependencies for the fork ruby file&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; gem install octokit logger multi_json json --no-ri --no-rdoc&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; mkdir /data&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WORKDIR&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; /data&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Setup some data about the committer&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; git config --global user.name &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Fork Updater&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; git config --global user.email &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;email@email.com&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; mkdir -p /root/.ssh&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Get the host config for github.com&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; ssh-keyscan github.com &amp;gt;&amp;gt; /root/.ssh/known_hosts&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Setup the SSH config&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;COPY&lt;/span&gt; ./config /root/.ssh&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;COPY&lt;/span&gt; ./fork_updater.rb .&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CMD&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ruby&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/data/fork_updater.rb&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;That&amp;rsquo;s it. Now our updater is containerized and ready to be deployed as a cronjob on a kube cluster. Oh, and we also need to create the SSH key like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create secret generic ssh-key-secret --from-file=ssh-privatekey=/path/to/.ssh/id_rsa
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;before-we-begin&#34;&gt;Before we Begin&lt;/h1&gt;
&lt;p&gt;There are two things we will need though to set up for our cluster before we even begin adding the first service. And that&amp;rsquo;s an ingress with a load-balancer and cert-manager.&lt;/p&gt;
&lt;h1 id=&#34;cert-manager&#34;&gt;Cert-Manager&lt;/h1&gt;
&lt;p&gt;Now, you have the option of installing cert-manager via helm, or via the provided kube config yaml file. I &lt;strong&gt;STRONGLY&lt;/strong&gt; recommend using the config yaml file because the upgrading process with helm is a hell of a lot dirtier / failure prone than simply applying a new yaml file with a different version in it.&lt;/p&gt;
&lt;p&gt;Either way, to install cert-manager follow this simple guide: &lt;a href=&#34;https://docs.cert-manager.io/en/latest/getting-started/install/kubernetes.html#installing-with-regular-manifests&#34;&gt;Cert-manager Install Manual&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;ingress&#34;&gt;Ingress&lt;/h1&gt;
&lt;p&gt;An Ingress is a must. This is the component which manages external access to the services which we will define. Like a proxy before your http server. This component will handle the hostname based routing between our services.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m using nginx ingress, although there are a couple of implementations out there.&lt;/p&gt;
&lt;p&gt;To install nginx ingress, follow their guide here: &lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/deploy/&#34;&gt;Installing Nginx-Ingress&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;from-easy-to-complicated&#34;&gt;From Easy to Complicated&lt;/h1&gt;
&lt;p&gt;Alright. Now that we have the prereqs out of the way, let&amp;rsquo;s get our hands dirty. I&amp;rsquo;ll start with the easiest of them all, my web site, and then will progress towards the more complicated ones, like Gitea and Athens, which require a lot more fiddling and have more moving parts.&lt;/p&gt;
&lt;h2 id=&#34;my-website&#34;&gt;My Website&lt;/h2&gt;
&lt;p&gt;The site, located here: &lt;a href=&#34;https://gergelybrautigam.com&#34;&gt;Gergely&amp;rsquo;s Domain&lt;/a&gt;; is a really simple, static, &lt;a href=&#34;https://gohugo.io&#34;&gt;Hugo&lt;/a&gt; based website. It contains nothing fancy, no real Javascript magic, has a simple list of things I&amp;rsquo;ve done and who I am.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s powered / served by an nginx instance running on port 9090 define by a very simple Dockerfile:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Dockerfile&#34; data-lang=&#34;Dockerfile&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; golang:latest as builder&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; apt-get update &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt install -y git make vim hugo&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; mkdir -p /opt/website&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; git clone https://github.com/Skarlso/gergelybrautigam /opt/website&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WORKDIR&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; /opt/website&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; make&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; nginx:latest&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; mkdir -p /var/www/html/gergelybrautigam&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WORKDIR&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; /var/www/html/gergelybrautigam&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;COPY&lt;/span&gt; --from&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;builder /opt/website/public .&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;COPY&lt;/span&gt; 01_gergelybrautigam /etc/nginx/sites-available/&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; mkdir -p /etc/nginx/sites-enabled/&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; ln -s /etc/nginx/sites-available/01_gergelybrautigam /etc/nginx/sites-enabled/01_gergelybrautigam&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Easy as goblin pie. Nginx has a command set like this &lt;code&gt;CMD [&amp;quot;nginx&amp;quot;, &amp;quot;-g&amp;quot;, &amp;quot;daemon off;&amp;quot;]&lt;/code&gt; and exposes port 80.&lt;/p&gt;
&lt;h3 id=&#34;the-deployment&#34;&gt;The deployment&lt;/h3&gt;
&lt;p&gt;In order to deploy this in the cluster, I created a deployment as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gb-deployment&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gergely-brautigam&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gergelybrautigam&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;prometheus.io/scrape&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;true&amp;#39;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;prometheus.io/port&lt;/span&gt;:   &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;9090&amp;#39;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gergelybrautigam&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gergelybrautigam&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;prometheus.io/scrape&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;true&amp;#39;&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;prometheus.io/port&lt;/span&gt;:   &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;9090&amp;#39;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gergelybrautigam&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;skarlso/gergelybrautigam:v0.0.26&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9090&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The metadata section defines information about the deployment. It&amp;rsquo;s name is gb-deployment. The namespace in which this sits is called gergely-brautigam and it has some labels to it so Prometheus monitoring tool can discover the pod.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s running a single replica, has a bunch of more metadata and template settings, and finally the container spec which defines the image, and the exposed container port on which the application is running.&lt;/p&gt;
&lt;p&gt;Now we need a service to expose this deployment.&lt;/p&gt;
&lt;h3 id=&#34;the-service&#34;&gt;The service&lt;/h3&gt;
&lt;p&gt;The service is also simple. It looks like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gergely-brautigam&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gb-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gergelybrautigam&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9090&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Again, nothing fancy here, just a simple service exposing a port to a different port on the front-end side. This service will be picked up by our previously created routing facility.&lt;/p&gt;
&lt;h3 id=&#34;ingress-1&#34;&gt;Ingress&lt;/h3&gt;
&lt;p&gt;Now that we have the service we need to expose it to the domain. I have the domain gergelybrautigam.com and I already pointed it at the LoadBalancer&amp;rsquo;s IP which was created by the nginx ingress controller.&lt;/p&gt;
&lt;p&gt;We only want one LoadBalancer, but we have multiple hostnames. We can achieve that by creating an Ingress resource in the namespace our service is in like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gergely-brautigam&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gergely-brautigam-ingress&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;kubernetes.io/ingress.class&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nginx&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;certmanager.k8s.io/cluster-issuer&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;letsencrypt-prod&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;certmanager.k8s.io/acme-challenge-type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;http01&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;nginx.ingress.kubernetes.io/rewrite-target&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;tls&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;hosts&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;gergelybrautigam.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gergelybrautigam-tls&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gergelybrautigam.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;serviceName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gb-service&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;servicePort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Remember, we already have the nginx ingress resource in the default namespace when we installed the controller previously. That is the main entrypoint. We are taking advantage of the rewrite-target annotation. That is our key to success &lt;code&gt;nginx.ingress.kubernetes.io/rewrite-target: /&lt;/code&gt;. The rest is basic routing. We&amp;rsquo;ll have something like this in the other namespace to.&lt;/p&gt;
&lt;p&gt;And with that, our website is done and it should be working under HTTPS. Cert-manager should have picked it up and generated a certificate for it. Let&amp;rsquo;s check.&lt;/p&gt;
&lt;p&gt;Running &lt;code&gt;k get certs -n gergely-brautigam&lt;/code&gt; you should see something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; $ k get certs -n gergely-brautigam
NAME                   READY   SECRET                 AGE
gergelybrautigam-tls   True    gergelybrautigam-tls   86d
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If there is a problem, just describe the cert resource and look for the generated challenge and if it was successful or not. The challenge contains mostly good error messages.&lt;/p&gt;
&lt;h2 id=&#34;irc-bouncer&#34;&gt;IRC bouncer&lt;/h2&gt;
&lt;p&gt;That wasn&amp;rsquo;t too bad, right? Let&amp;rsquo;s do something a bit more complex this time. We are going to deploy &lt;a href=&#34;https://github.com/thelounge/thelounge&#34;&gt;The lounge&lt;/a&gt; irc bouncer.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s actually quite easy to do but can be daunting to look at at first.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/hosting/the-climb.png&#34; alt=&#34;easy&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;the-container&#34;&gt;The container&lt;/h3&gt;
&lt;p&gt;Lucky for us, the bouncer already provides a container located here: &lt;a href=&#34;https://hub.docker.com/r/thelounge/thelounge/&#34;&gt;The Lounge Docker Hub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We just need two things. To expose the port 9000 and to give it something called a PersistentVolume. What&amp;rsquo;s a persistent volume? Well, look it up here: &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/&#34;&gt;Kubernetes Persistent Volumes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;TL;DR: We need to preserve data. Containers are ephemeral in nature. Meaning if there is a problem we usually just delete the pod. Which means that all data will be lost. But we need persistence in this case because we&amp;rsquo;ll have user data and user information which we would like to persist between pods. That&amp;rsquo;s what a volume is for.&lt;/p&gt;
&lt;p&gt;It will be mounted into the pod so we can point the bouncer to use that location for data management.&lt;/p&gt;
&lt;h3 id=&#34;pvc&#34;&gt;PVC&lt;/h3&gt;
&lt;p&gt;With that, this is how our PersistentVolumeClaim will look like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;powerhouse&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;do-storage-irc&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;5Gi&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;do-block-storage&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;DigitalOcean provides a block storage implementation for this claim so we use that storage class &lt;code&gt;do-block-storage&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;deployment&#34;&gt;Deployment&lt;/h3&gt;
&lt;p&gt;With that, this is how the deployment will look like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;powerhouse&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc-app&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;thelounge/thelounge:3.1.1&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9000&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc-http&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/opt/thelounge&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;subPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;thelounge&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc-data&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc-data&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeClaim&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;claimName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;do-storage-irc&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Short and sweet. The important bits are the labels, those are used by cert-manager and ingress to find the right deployment, and the &lt;code&gt;volumeMounts&lt;/code&gt;. We mount into the /var/opt/thelounge folder because that&amp;rsquo;s the main configuration location. The subPath is important for a correct mounting.&lt;/p&gt;
&lt;h3 id=&#34;the-service-1&#34;&gt;The service&lt;/h3&gt;
&lt;p&gt;Alright, with the deployment in place, let&amp;rsquo;s take a look at the service.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;powerhouse&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;http&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9000&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc-http&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Again, very boring stuff. Boring is good. Boring is predictable. We expose port 9000 to the named targetPort called irc-http which we defined in the above deployment.&lt;/p&gt;
&lt;p&gt;Now, I have a domain in which these things are running, let&amp;rsquo;s call it &lt;code&gt;powerhouse.com&lt;/code&gt; (because I&amp;rsquo;m tired of example.com). And I have multiple services in this namespace too, so I&amp;rsquo;ll call the namespace here, powerhouse and put this irc service in there. This also means that the ingress resource for this namespace will contain a couple more routings, because my powerhouse namespace will also contain my gitea and Athens proxy installation.&lt;/p&gt;
&lt;p&gt;We can, however, take a peak at the ingress resource here and now&amp;hellip; because I hate suspense.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;powerhouse&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;powerhouse-ingress&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;kubernetes.io/ingress.class&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nginx&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;certmanager.k8s.io/cluster-issuer&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;letsencrypt-prod&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;certmanager.k8s.io/acme-challenge-type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;http01&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;nginx.ingress.kubernetes.io/rewrite-target&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;tls&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;hosts&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;irc.powerhouse.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc-powerhouse-tls&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;hosts&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea.powerhouse.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-powerhouse-tls&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;hosts&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;athens.powerhouse.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-powerhouse-tls&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc.powerhouse.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;serviceName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;irc&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;servicePort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9000&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea.powerhouse.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;serviceName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;servicePort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3000&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens.powerhouse.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;serviceName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;athens-service&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;servicePort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can see that I have multiple paths pointing to three different subdomains with different ports. These ports will be routed to by nginx ingress. Meaning you &lt;strong&gt;DO NOT OPEN THESE ON YOUR LOADBALANCER&lt;/strong&gt;. These will all be accessible from 443/HTTPS. Expect for gitea&amp;rsquo;s SSH port later on.&lt;/p&gt;
&lt;p&gt;With these in place, cert-manager should pick it up and provide a certificate for it.&lt;/p&gt;
&lt;h3 id=&#34;side-track----debugging&#34;&gt;Side track &amp;ndash; debugging&lt;/h3&gt;
&lt;p&gt;If there is a problem and we can&amp;rsquo;t reach TheLounge we need to debug. I use the following tool to access Kubernetes resources: &lt;a href=&#34;https://github.com/derailed/k9s&#34;&gt;K9S&lt;/a&gt;. It&amp;rsquo;s a neat CLI tool to look at kube resources in an interactive way and not having to type in a bunch of commands. Never the less, I will also paste those in here.&lt;/p&gt;
&lt;p&gt;To look at the pods that should have been created, type:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;k get pods -n powerhouse
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Should see something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME                          READY   STATUS    RESTARTS   AGE
athens-app-857749c59c-lmjnb   1/1     Running   0          6d3h
gitea-app-6974fb995b-pn2vv    1/1     Running   0          9d
gitea-db-59758fbcd9-4562c     1/1     Running   0          9d
irc-app-5f87688f98-dqsvb      1/1     Running   0          9d
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can see that my other services are running fine. And there is IRC as well. Now if there would be any kind of problem we could access the Pods information be describing the pod with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;k describe pod/irc-app-5f87688f98-dqsvb -n powerhouse
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Which will provide a bunch of information about the pod. But the pod could be absolutely fine, yet our service could be down. (We didn&amp;rsquo;t define any liveliness or readiness probs after all).&lt;/p&gt;
&lt;p&gt;We can verify that by taking a peak in the container (also, check if our mounting was successful). Since this is just a container, exec works similar to docker exec.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; $ k exec -it irc-app-5f87688f98-dqsvb -n powerhouse /bin/bash
root@irc-app-5f87688f98-dqsvb:/#
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Should give us a prompt. We can now look at logs, check out the configuration folder etc.&lt;/p&gt;
&lt;p&gt;In k9s you would simply select the right namespace, select the pod, hit &lt;code&gt;d&lt;/code&gt; for describe or &lt;code&gt;s&lt;/code&gt; for shell. Done.&lt;/p&gt;
&lt;h2 id=&#34;gitea&#34;&gt;Gitea&lt;/h2&gt;
&lt;p&gt;Now, we have IRC running. Let&amp;rsquo;s try deploying &lt;a href=&#34;https://gitea.io/en-us/&#34;&gt;Gitea&lt;/a&gt;. This takes a tiny bit more fiddling though.&lt;/p&gt;
&lt;h3 id=&#34;requirements&#34;&gt;Requirements&lt;/h3&gt;
&lt;p&gt;Gitea requires the following things to be present:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The gitea app configuration file (this can be done via environment properties though)&lt;/li&gt;
&lt;li&gt;A DB&lt;/li&gt;
&lt;li&gt;A PersistentVolume&lt;/li&gt;
&lt;li&gt;SSH Port for SSH based git clones instead of simple https&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;db&#34;&gt;DB&lt;/h4&gt;
&lt;p&gt;We shall begin with the simplest of them, the DB. At this point we could go with the DigitalOcean managed Postgres installation, but I didn&amp;rsquo;t want to put that on the bill as well. So I choose to simply put my DB into a container and deploy it within the cluster.&lt;/p&gt;
&lt;p&gt;This is actually quite simple. The DB will be a separate deployment / application in the same namespace as the Gitea app. It will also contain a network policy, since the DB doesn&amp;rsquo;t need internet access and the internet shouldn&amp;rsquo;t be able to access it.&lt;/p&gt;
&lt;p&gt;In fact the only thing that should be able to access the DB is the Gitea application itself which we will be able to restrict via the usage of&amp;hellip; Labels!&lt;/p&gt;
&lt;h5 id=&#34;deployment-1&#34;&gt;Deployment&lt;/h5&gt;
&lt;p&gt;But first, take a look at the deployment of a Postgres 11 pod:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;powerhouse&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-db&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-db&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-db&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-db&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;postgres&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;postgres:11&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;POSTGRES_USER&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;POSTGRES_PASSWORD&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;secretKeyRef&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-db-password&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;password&lt;/span&gt;
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;POSTGRES_DB&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/lib/postgresql/data&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;subPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;data&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# important so it gets mounted properly&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;git-db-data&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;git-db-data&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeClaim&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;claimName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;do-storage-gitea-db&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Okay, there are a lot of things going on here, but the three things we need to note are the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-db&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Our network policy will look for this label to identify the pods which fell under its rule.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;POSTGRES_PASSWORD&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;secretKeyRef&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-db-password&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;password&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The database password will come from a secret.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/lib/postgresql/data&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;subPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;data&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# important so it gets mounted properly&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;git-db-data&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;git-db-data&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeClaim&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;claimName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;do-storage-gitea-db&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We also need a persistent volume otherwise the data will be lost on each pod restart.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;powerhouse&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;do-storage-gitea-db&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10Gi&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;do-block-storage&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h5 id=&#34;service&#34;&gt;Service&lt;/h5&gt;
&lt;p&gt;We also need a Service so Gitea will be able to reach it. This isn&amp;rsquo;t public though so a NodePort is enough with no clusterIP.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;powerhouse&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-db-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;5432&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-db&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;clusterIP&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;None&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In order to reach this DB we can use a URL like this now from the Gitea app: &lt;code&gt;gitea-db-service.powerhouse.svc.cluster.local:5432&lt;/code&gt;.&lt;/p&gt;
&lt;h5 id=&#34;networkpolicy&#34;&gt;NetworkPolicy&lt;/h5&gt;
&lt;p&gt;We want the Gitea app to be able to reach it. Which means in-out to the Gitea app and nothing else.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-db-network-policy&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;powehouse&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-db&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Egress&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ingress&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;5432&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;egress&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;to&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;5432&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can test this now by exec-ing into the Pod of the DB deployment and trying to ping google.com for example. It should be denied. Yet later, when we deploy our Gitea app, that should be able to talk to the DB instance.&lt;/p&gt;
&lt;h5 id=&#34;secret&#34;&gt;Secret&lt;/h5&gt;
&lt;p&gt;Finally, we have a Secret which contains our db password base64 encoded.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cronohub&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-db-password&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Opaque&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;password&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Z2l0ZWE=&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;That says password123. To get it, you can run something like &lt;code&gt;echo -n &amp;quot;password123&amp;quot; | base64&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;gitea-app-ini&#34;&gt;Gitea App ini&lt;/h4&gt;
&lt;p&gt;Huh, with that done, we can go on with the application ini file. This can be configured via environment properties but once you get over a dozen configuration entries, it&amp;rsquo;s just easier to use an app.ini. My app ini is large, so I won&amp;rsquo;t post it here. I could mount it in as a file, but that proved to be difficult or not work at all properly because Gitea is running under a different user than root. Also, once the mount happened the fact the gitea was trying to write into it caused problems. Mounting as a different user didn&amp;rsquo;t work out either, so I&amp;rsquo;m using an &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/init-containers/&#34;&gt;InitContainer&lt;/a&gt; to do the job. They are there for that reason. And it was actually a hell of a lot simpler than doing file mounting.&lt;/p&gt;
&lt;p&gt;The app.ini is defined as a ConfigMap like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create configmap gitea-app-ini --from-file=app.ini --namespace powerhouse
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This was done from the folder where my app.ini was residing.&lt;/p&gt;
&lt;h4 id=&#34;deployment-2&#34;&gt;Deployment&lt;/h4&gt;
&lt;p&gt;Now comes the big gun. The Gitea deployment file. This is how it looks like in all its glory:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cronohub&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-app&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;initContainers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;init-disk&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox:latest&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;:
        - &lt;span style=&#34;color:#ae81ff&#34;&gt;/bin/chown&lt;/span&gt;
        - &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# we set the gid and uid of the user for gitea.&lt;/span&gt;
        - &lt;span style=&#34;color:#ae81ff&#34;&gt;/data&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;git-data&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/data&amp;#34;&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;init-app-ini&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox:latest&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sh&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;-c&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mkdir -p /data/gitea/conf/; cp /data/app.ini /data/gitea/conf&amp;#39;&lt;/span&gt;]
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;git-data&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/data&amp;#34;&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-app-ini-conf&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/data/app.ini&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;subPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;app.ini&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea/gitea:1.9.2&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DB_PASSWD&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;secretKeyRef&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-db-password&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;password&lt;/span&gt;
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DB_TYPE&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;configMapKeyRef&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-config-map&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DB_TYPE&lt;/span&gt;
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DB_HOST&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;configMapKeyRef&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-config-map&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DB_HOST&lt;/span&gt;
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DB_NAME&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;configMapKeyRef&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-config-map&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DB_NAME&lt;/span&gt;
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DB_USER&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;configMapKeyRef&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-config-map&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DB_USER&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3000&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-http&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;22&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-ssh&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/data&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;git-data&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;git-data&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeClaim&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;claimName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;do-storage-gitea&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-app-ini-conf&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;configMap&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-app-ini&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The important bit is the initContainer section. What&amp;rsquo;s happening here? We mount the app.ini file to the init container under /data. The awesome part about the initContainer is that the real container will have access to the file system the init container created.&lt;/p&gt;
&lt;p&gt;So we take that file, fix the permissions on it and copy it to the right location under &lt;code&gt;/data/gitea/conf&lt;/code&gt; for the Gitea app to work with.&lt;/p&gt;
&lt;p&gt;Done!&lt;/p&gt;
&lt;p&gt;And the configMap is simple:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ConfigMap&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;powerhouse&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-config-map&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;APP_COLOR&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;blue&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;APP_MOD&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;DB_TYPE&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;postgres&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;DB_HOST&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gitea-db-service.cronohub.svc.cluster.local:5432&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;DB_NAME&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;DB_USER&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;ssh&#34;&gt;SSH&lt;/h4&gt;
&lt;p&gt;Normally, Ingress only allows HTTP based traffic control. But what would an ingress be without also regular TCP based routing?&lt;/p&gt;
&lt;p&gt;But it&amp;rsquo;s not trivial. Nginx Ingress provides a documentation for this here: &lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/&#34;&gt;Exposing TCP and UDP services&lt;/a&gt;. What does that mean in practice?&lt;/p&gt;
&lt;p&gt;You see we are also exposing port 22 on the container:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;22&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-ssh&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I choose to differentiate my SSH port for Gitea from port 22 because that&amp;rsquo;s just cumbersome to get done right. Gitea provides an explanation on how to do port 22 forwarding in a docker container with a custom git command which forwards commands to the container itself. This is all just plain too much to worry about.&lt;/p&gt;
&lt;p&gt;I have this in the app.ini:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-ini&#34; data-lang=&#34;ini&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;SSH_PORT&lt;/span&gt;         &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;port of my choosing&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And then this in the Service definition:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;powerhouse&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;http&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3000&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-http&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ssh&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;&amp;lt;port of my choosing&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gitea-ssh&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And then, we edit the nginx-controller deployment like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit deployment.apps/nginx-ingress-controller
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And add this line &lt;code&gt;--tcp-services-configmap=cronohub/gitea-ssh-service&lt;/code&gt; to the container&amp;rsquo;s args field:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;:
        - &lt;span style=&#34;color:#ae81ff&#34;&gt;/nginx-ingress-controller&lt;/span&gt;
        - --&lt;span style=&#34;color:#ae81ff&#34;&gt;default-backend-service=default/nginx-ingress-default-backend&lt;/span&gt;
        - --&lt;span style=&#34;color:#ae81ff&#34;&gt;election-id=ingress-controller-leader&lt;/span&gt;
        - --&lt;span style=&#34;color:#ae81ff&#34;&gt;ingress-class=nginx&lt;/span&gt;
        - --&lt;span style=&#34;color:#ae81ff&#34;&gt;configmap=default/nginx-ingress-controller&lt;/span&gt;
        - --&lt;span style=&#34;color:#ae81ff&#34;&gt;tcp-services-configmap=powerhouse/gitea-ssh-service&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;One more thing is that we have to open that port on the load balancer as well to get traffic to it. To that end, edit the nginx ingress service as well:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit services/nginx-ingress-controller
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And add the exposed port:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ssh&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;&amp;lt;port of my choosing&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;&amp;lt;port of my choosing&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;There will probably be a nodePort section in there on the other ports. Ignore that for your change.&lt;/p&gt;
&lt;p&gt;Also, if you are doing the nginx installation by hand, just add this or save the yaml file from those deployments like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get service/nginx-ingress-controller -o yaml &amp;gt; nginx-ingress-controller.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So you can deploy / modify it later on.&lt;/p&gt;
&lt;h4 id=&#34;finished-gitea&#34;&gt;Finished Gitea&lt;/h4&gt;
&lt;p&gt;And with that, visit &lt;code&gt;gitea.powerhouse.com&lt;/code&gt; and it should work including HTTPS and SSH!&lt;/p&gt;
&lt;p&gt;You can now clone repositories like this: &lt;code&gt;git clone ssh://git@gitea.powerhouse.com:1234/user/awesome_project.git&lt;/code&gt; after you created your user.&lt;/p&gt;
&lt;p&gt;User creation is done by using the gitea admin CLI tool described here: &lt;a href=&#34;https://docs.gitea.io/en-us/command-line/&#34;&gt;Gitea Documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It is important to note that we don&amp;rsquo;t use &lt;code&gt;latest&lt;/code&gt; anywhere. It&amp;rsquo;s just not good if you are trying to update a service later on. We could set ImagePolicy to AlwaysPull but that&amp;rsquo;s just not a good thing to do if you have a 2 gig image. Always use version and policy &lt;code&gt;imagePullPolicy: IfNotPresent&lt;/code&gt; to save yourself some bandwidth.&lt;/p&gt;
&lt;h2 id=&#34;idle-checker&#34;&gt;Idle Checker&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/hosting/idle-checker.png&#34; alt=&#34;idle-checker&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s create a last resource, then we&amp;rsquo;ll call it a day.&lt;/p&gt;
&lt;p&gt;The idle RPG is a cool little game that you play by&amp;hellip; not playing. At all. If you play, you get penalties. Here is a cool resource to start: &lt;a href=&#34;https://idlerpg.lolhosting.net&#34;&gt;Idle RPG&lt;/a&gt;. It looks something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;21:56 &amp;lt;@IdleBot&amp;gt; Verily I say unto thee, the Heavens have burst forth, and the blessed hand of God carried ganome 0 days, 03:52:11 toward level 45.
21:56 &amp;lt;@IdleBot&amp;gt; ganome reaches next level in 0 days, 01:49:16.
22:02 &amp;lt;@IdleBot&amp;gt; himuraken, the level 77 Mage Of BitFlips, is now online from nickname himuraken. Next level in 11 days, 10:35:53.
22:14 &amp;lt;@IdleBot&amp;gt; Nechayev, Sundance, and simple [2011/2347] have team battled HeavyPodda, Sixbierehomme, and L [1417/2717] and won! 0 days, 06:14:54 is removed from their clocks.
22:18 &amp;lt;@IdleBot&amp;gt; canton7 saw an episode of Ally McBeal. This terrible calamity has slowed them 0 days, 05:10:53 from level 85.
22:18 &amp;lt;@IdleBot&amp;gt; canton7 reaches next level in 2 days, 00:21:36.
22:26 &amp;lt;@IdleBot&amp;gt; Tor [4/1142] has challenged Brainiac [232/817] in combat and lost! 3 days, 23:06:05 is added to Tor&#39;s clock.
22:26 &amp;lt;@IdleBot&amp;gt; Tor reaches next level in 39 days, 23:39:35.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It could happen that by some misfortune the bouncer gets restarted and it doesn&amp;rsquo;t log you back in. Or you simply just lose connection and you don&amp;rsquo;t re-connect. That is unacceptable because the point is to be present. Otherwise you don&amp;rsquo;t play. So you need an early warning in case you are offline. Luckily, IdleRPG provides an XML based endpoint to get which contains your status.&lt;/p&gt;
&lt;p&gt;From there, I&amp;rsquo;m using mailgun with a registered domain to send me an email in case my status is offline. For that, here is a small Go program &lt;a href=&#34;https://gist.github.com/Skarlso/318ddd6f8d71dbda8fbbd1a908fdb159&#34;&gt;IdleRPG Checker Go Code&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To put that into a Docker container, here is a Dockerfile:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Dockerfile&#34; data-lang=&#34;Dockerfile&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; golang:latest as build&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; go get -v github.com/sirupsen/logrus &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    go get -v github.com/mailgun/mailgun-go&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;COPY&lt;/span&gt; ./main.go /code/&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WORKDIR&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; /code&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; CGO_ENABLED&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; GOOS&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;linux go build -a -installsuffix cgo -o /idlerpg-checker .&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; alpine:latest&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; apk --no-cache add ca-certificates&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;COPY&lt;/span&gt; --from&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;build /idlerpg-checker /idlerpg-checker&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; echo &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;v0.0.1&amp;#34;&lt;/span&gt; &amp;gt;&amp;gt; .version&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;ENTRYPOINT&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/idlerpg-checker&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And the corresponding cronjob resource definition:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;batch/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;CronJob&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;idle-checker&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;idle-checker&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;schedule&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*/20 * * * *&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;jobTemplate&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;idle-checker&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;skarlso/idle-checker&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;imagePullPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IfNotPresent&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;:
              - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;:  &lt;span style=&#34;color:#ae81ff&#34;&gt;MG_API_KEY&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
                  &lt;span style=&#34;color:#f92672&#34;&gt;secretKeyRef&lt;/span&gt;:
                    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;:  &lt;span style=&#34;color:#ae81ff&#34;&gt;idle-rpg-secret&lt;/span&gt;
                    &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;:  &lt;span style=&#34;color:#ae81ff&#34;&gt;MG_API_KEY&lt;/span&gt;
              - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;:  &lt;span style=&#34;color:#ae81ff&#34;&gt;MG_DOMAIN&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
                  &lt;span style=&#34;color:#f92672&#34;&gt;secretKeyRef&lt;/span&gt;:
                    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;:  &lt;span style=&#34;color:#ae81ff&#34;&gt;idle-rpg-secret&lt;/span&gt;
                    &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;:  &lt;span style=&#34;color:#ae81ff&#34;&gt;MG_DOMAIN&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;-username&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;username&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;-email&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;user@powerhouse.com&amp;#39;&lt;/span&gt;]
          &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;OnFailure&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Aaaand, the secret for the API key:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;idle-checker&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;idle-rpg-secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Opaque&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;MG_API_KEY&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;asdf=&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;MG_DOMAIN&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;asdf==&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Done. Huh. This will run every 20 minutes and check if the user with username &lt;code&gt;username&lt;/code&gt; is online. If not, it will send an email to the given email address. Your levels are safe.&lt;/p&gt;
&lt;h1 id=&#34;closing-words&#34;&gt;Closing words&lt;/h1&gt;
&lt;p&gt;Phew. This has been quite the ride. The post is now really long, so I will split the rest out into a Part 2. That is, Athens and Monitoring.&lt;/p&gt;
&lt;p&gt;Thank you for reading this far!&lt;/p&gt;
&lt;p&gt;Cheers,
Gergely.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Updated Face-recog architecture drawing</title>
      <link>https://skarlso.github.io/2019/09/19/updated-face-recog-drawing/</link>
      <pubDate>Thu, 19 Sep 2019 13:01:00 +0100</pubDate>
      <guid>https://skarlso.github.io/2019/09/19/updated-face-recog-drawing/</guid>
      <description>&lt;p&gt;I had a lot of fun using &lt;a href=&#34;https://procreate.art&#34;&gt;Procreate&lt;/a&gt; to re-draw the architecture image I&amp;rsquo;ve drawn for my distribute face recognition application detailed in this post &lt;a href=&#34;https://skarlso.github.io/2018/03/15/kubernetes-distributed-application/&#34;&gt;Distributed Face-Recognition App&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Without much fanfare, here is the drawing:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/kube/kube_architecture.png&#34; alt=&#34;kube_architecture.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Thanks,
Gergely.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes distributed application deployment with sample Face Recognition App</title>
      <link>https://skarlso.github.io/2018/03/15/kubernetes-distributed-application/</link>
      <pubDate>Thu, 15 Mar 2018 23:01:00 +0100</pubDate>
      <guid>https://skarlso.github.io/2018/03/15/kubernetes-distributed-application/</guid>
      <description>&lt;h1 id=&#34;intro&#34;&gt;Intro&lt;/h1&gt;
&lt;p&gt;Alright folks. Settle in and get comfortable. This is going to be a long, but hopefully, fun ride.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m going to deploy a distributed application with &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt;. I attempted to create an application that I thought resembled a real world app. Obviously I had to cut some corners due to time and energy constraints.&lt;/p&gt;
&lt;p&gt;My focus will be on Kubernetes and deployment.&lt;/p&gt;
&lt;p&gt;Shall we delve right in?&lt;/p&gt;
&lt;h1 id=&#34;the-application&#34;&gt;The Application&lt;/h1&gt;
&lt;h2 id=&#34;tldr&#34;&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/kube_overview.png&#34; alt=&#34;kube overview&#34;&gt;&lt;/p&gt;
&lt;p&gt;The application itself consists of six parts. The repository can be found here: &lt;a href=&#34;https://github.com/Skarlso/kube-cluster-sample&#34;&gt;Kube Cluster Sample&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It’s a face recognition service which identifies images of people, comparing them to known individuals. A simple frontend displays a table of these images whom they belong to. This happens by sending a request to a &lt;a href=&#34;https://github.com/Skarlso/kube-cluster-sample/tree/master/receiver&#34;&gt;receiver&lt;/a&gt;. The request contains a path to an image. This image can sit on an NFS somewhere. The receiver stores this path in the DB (MySQL) and sends a processing request to a queue. The queue uses: &lt;a href=&#34;http://nsq.io/&#34;&gt;NSQ&lt;/a&gt;. The request contains the ID of the saved image.&lt;/p&gt;
&lt;p&gt;An &lt;a href=&#34;https://github.com/Skarlso/kube-cluster-sample/tree/master/image_processor&#34;&gt;Image Processing&lt;/a&gt; service is constantly monitoring the queue for jobs to do. The processing consists of the following steps: taking the ID; loading the image; and finally,  sending the image to a &lt;a href=&#34;https://github.com/Skarlso/kube-cluster-sample/tree/master/face_recognition&#34;&gt;face recognition&lt;/a&gt; backend written in Python via &lt;a href=&#34;https://grpc.io/&#34;&gt;gRPC&lt;/a&gt;. If the identification is successful, the backend will return the name of the image corresponding to that person. The image_processor then updates the image’s record with the person’s ID and marks the image as “processed successfully”. If identification is unsuccessful, the image will be left as “pending”. If there was a failure during identification, the image will be flagged as “failed”.&lt;/p&gt;
&lt;p&gt;Failed images can be retried  with a cron job, for example:&lt;/p&gt;
&lt;p&gt;So how does this all work? Let&amp;rsquo;s check it out .&lt;/p&gt;
&lt;h2 id=&#34;receiver&#34;&gt;Receiver&lt;/h2&gt;
&lt;p&gt;The receiver service is the starting point of the process. It&amp;rsquo;s an API which receives a request in the following format:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#34;path&amp;#34;:&amp;#34;/unknown_images/unknown0001.jpg&amp;#34;}&amp;#39;&lt;/span&gt; http://127.0.0.1:8000/image/post
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In this instance, the receiver stores the path using a shared database cluster. The entity will then receive an ID from the database service. This application is based on the model where unique identification for Entity Objects is provided by the persistence layer. Once the ID is procured, the receiver will send a message to NSQ. At this point in the process, the receiver&amp;rsquo;s job is done.&lt;/p&gt;
&lt;h2 id=&#34;image-processor&#34;&gt;Image Processor&lt;/h2&gt;
&lt;p&gt;Here is where the excitement begins. When Image Processor first runs it creates two Go routines. These are&amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;consume&#34;&gt;Consume&lt;/h3&gt;
&lt;p&gt;This is an NSQ consumer. It has three integral jobs. Firstly, it listens for messages on the queue. Secondly, when there is a message, it appends the received ID to a thread safe slice of IDs that the second routine processes. And lastly, it signals the second routine that there is work to be do. It does this through &lt;a href=&#34;https://golang.org/pkg/sync/#Cond&#34;&gt;sync.Condition&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;processimages&#34;&gt;ProcessImages&lt;/h3&gt;
&lt;p&gt;This routine processes a slice of IDs until the slice is drained completely. Once the slice is drained, the routine suspends instead of sleep-waiting on a channel. The processing of a single ID can be seen in the following linear steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Establish a gRPC connection to the Face Recognition service (explained under Face Recognition)&lt;/li&gt;
&lt;li&gt;Retrieve the image record from the database&lt;/li&gt;
&lt;li&gt;Setup two functions for the &lt;a href=&#34;#circuit-breaker&#34;&gt;Circuit Breaker&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Function 1: The main function which runs  the RPC method call&lt;/li&gt;
&lt;li&gt;Function 2: A health check for the Ping of the circuit breaker&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Call Function 1 which sends the path of the image to the face recognition service. This path should be accessible by the face recognition service. Preferably something shared like an NFS&lt;/li&gt;
&lt;li&gt;If this call fails, update the image record as FAILED PROCESSING&lt;/li&gt;
&lt;li&gt;If it succeeds, an image name should come back which corresponds to a person in the db. It runs a joined SQL query which gets the corresponding person&amp;rsquo;s ID&lt;/li&gt;
&lt;li&gt;Update the Image record in the database with PROCESSED status and the ID of the person that image was identified as&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This service can be replicated. In other words, more than one can run at the same time.&lt;/p&gt;
&lt;h3 id=&#34;circuit-breaker&#34;&gt;Circuit Breaker&lt;/h3&gt;
&lt;p&gt;A  system in which replicating resources requires little to no effort, there still can be cases where, for example, the network goes down, or there are communication problems of any kind between two services. I like to implement a little circuit breaker around the gRPC calls for fun.&lt;/p&gt;
&lt;p&gt;This is how it works:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/kube_circuit1.png&#34; alt=&#34;kube circuit&#34;&gt;&lt;/p&gt;
&lt;p&gt;As you can see, once there are 5 unsuccessful calls to the service, the circuit breaker activates, not allowing any more calls to go through. After a configured amount of time, it will send a Ping call to the service to see if it&amp;rsquo;s back up. If that still errors out, it will increase the timeout. If not, it opens the circuit, allowing traffic to proceed.&lt;/p&gt;
&lt;h2 id=&#34;front-end&#34;&gt;Front-End&lt;/h2&gt;
&lt;p&gt;This is only a simple table view with Go&amp;rsquo;s own html/template used to render a list of images.&lt;/p&gt;
&lt;h2 id=&#34;face-recognition&#34;&gt;Face Recognition&lt;/h2&gt;
&lt;p&gt;Here is where the identification magic happens. I decided to make this a gRPC based service for the  sole purpose of its flexibility. I started writing it in Go but decided that a Python implementation would be much sorter. In fact, excluding the gRPC code, the recognition part is approximately 7 lines of Python code. I&amp;rsquo;m using this fantastic library which contains all the C bindings to OpenCV. &lt;a href=&#34;https://github.com/ageitgey/face_recognition&#34;&gt;Face Recognition&lt;/a&gt;. Having an API contract here means that I can change the implementation anytime as long as it adheres to the contract.&lt;/p&gt;
&lt;p&gt;Please note that there exist a great Go library OpenCV. I was about to use it but they had yet to write the C bindings for that part of OpenCV. It&amp;rsquo;s called &lt;a href=&#34;https://gocv.io/&#34;&gt;GoCV&lt;/a&gt;. Check them out! They have some pretty amazing things, like real-time camera feed processing that only needs a couple of lines of code.&lt;/p&gt;
&lt;p&gt;The python library is simple in nature. Have a set of images of people you know. I have a folder with a couple of images named, &lt;code&gt;hannibal_1.jpg, hannibal_2.jpg, gergely_1.jpg, john_doe.jpg&lt;/code&gt;. In the database I have two tables named, &lt;code&gt;person, person_images&lt;/code&gt;. They look like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;+----+----------+
| id | name     |
+----+----------+
|  &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; | Gergely  |
|  &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; | John Doe |
|  &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; | Hannibal |
+----+----------+
+----+----------------+-----------+
| id | image_name     | person_id |
+----+----------------+-----------+
|  &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; | hannibal_1.jpg |         &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; |
|  &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; | hannibal_2.jpg |         &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; |
+----+----------------+-----------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The face recognition library returns the name of the image from the known people which matches the person on the unknown image. After that, a simple joined query -like this- will return the person in question.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;select&lt;/span&gt; person.name, person.id &lt;span style=&#34;color:#66d9ef&#34;&gt;from&lt;/span&gt; person &lt;span style=&#34;color:#66d9ef&#34;&gt;inner&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;join&lt;/span&gt; person_images &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; pi &lt;span style=&#34;color:#66d9ef&#34;&gt;on&lt;/span&gt; person.id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pi.person_id &lt;span style=&#34;color:#66d9ef&#34;&gt;where&lt;/span&gt; image_name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;hannibal_2.jpg&amp;#39;&lt;/span&gt;;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The gRPC call returns the ID of the person which is then used to update the image&amp;rsquo;s ‘person` column.&lt;/p&gt;
&lt;h2 id=&#34;nsq&#34;&gt;NSQ&lt;/h2&gt;
&lt;p&gt;NSQ is a nice little Go based queue. It can be scaled and has a minimal footprint on the system. It also has a lookup service that consumers use to receive messages, and a daemon that senders use when sending messages.&lt;/p&gt;
&lt;p&gt;NSQ&amp;rsquo;s philosophy is that the daemon should run with the sender application. That way, the sender will send to the localhost only. But the daemon is connected to the lookup service, and that&amp;rsquo;s how they achieve a global queue.&lt;/p&gt;
&lt;p&gt;This means that there are as many NSQ daemons deployed as there are senders. Because the daemon has a minuscule resource requirement, it won&amp;rsquo;t interfere with the requirements of the main application.&lt;/p&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;p&gt;In order to be as flexible as possible, as well as making use of Kubernetes&amp;rsquo;s ConfigSet, I&amp;rsquo;m using .env files in development to store configurations like the location of the database service, or NSQ&amp;rsquo;s lookup address. In production- and that means the Kubernetes’s environment- I&amp;rsquo;ll use environment properties.&lt;/p&gt;
&lt;h2 id=&#34;conclusion-for-the-application&#34;&gt;Conclusion for the Application&lt;/h2&gt;
&lt;p&gt;And that&amp;rsquo;s all there is to the architecture of the application we are about to deploy. All of its components are changeable and coupled only through the database, a queue and gRPC. This is imperative when deploying a distributed application due to how updating mechanics work. I will cover that part in the Deployment section.&lt;/p&gt;
&lt;h1 id=&#34;deployment-with-kubernetes&#34;&gt;Deployment with Kubernetes&lt;/h1&gt;
&lt;h2 id=&#34;basics&#34;&gt;Basics&lt;/h2&gt;
&lt;p&gt;What &lt;strong&gt;is&lt;/strong&gt; Kubernetes?&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m going to cover some of the basics here. I won&amp;rsquo;t go too much into detail-  that would require a whole book like this one: &lt;a href=&#34;http://shop.oreilly.com/product/0636920043874.do&#34;&gt;Kubernetes Up And Running&lt;/a&gt;. Also, if you’re daring enough, you can have a look through this documentation: &lt;a href=&#34;https://kubernetes.io/docs/&#34;&gt;Kubernetes Documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Kubernetes is a containerized service and application manager. It scales easily, employs a swarm of containers, and most importantly, it&amp;rsquo;s highly configurable via yaml based template files. People often compare Kubernetes to Docker swarm, but Kubernetes does way more than that! For example: it&amp;rsquo;s container agnostic. You could use LXC with Kubernetes and it would work the same way as you using it with Docker. It provides a layer above managing a cluster of deployed services and applications. How? Let&amp;rsquo;s take a quick look at the building blocks of Kubernetes.&lt;/p&gt;
&lt;p&gt;In Kubernetes, you’ll describe a desired state of the application and Kubernetes will do what it can to reach that state. States could be something such as deployed; paused; replicated twice; and so on and so forth.&lt;/p&gt;
&lt;p&gt;One of the basics of Kubernetes is that it uses Labels and Annotations for all of its components. Services, Deployments, ReplicaSets, DaemonSets, everything is labelled. Consider the following scenario. In order to identify what pod belongs to what application, a label is used called &lt;code&gt;app: myapp&lt;/code&gt;. Let’s assume you have two containers of this application deployed; if you would remove the label &lt;code&gt;app&lt;/code&gt; from one of the containers, Kubernetes would only detect one and thus would launch a new instance of &lt;code&gt;myapp&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;kubernetes-cluster&#34;&gt;Kubernetes Cluster&lt;/h3&gt;
&lt;p&gt;For Kuberenetes to work, a Kubernetes cluster needs to be present. Setting that up might be a tad painful, but luckily, help is on hand. Minikube sets up a cluster for us locally with one Node. And AWS has a beta service running in the form of a Kubernetes cluster in which the only thing you need to do is request nodes and define your deployments. The Kubernetes cluster components are documented here: &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/components/&#34;&gt;Kubernetes Cluster Components&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;nodes&#34;&gt;Nodes&lt;/h3&gt;
&lt;p&gt;A Node is a worker machine. It can be anything- from a vm to a physical machine- including all sorts of cloud provided vms.&lt;/p&gt;
&lt;h3 id=&#34;pods&#34;&gt;Pods&lt;/h3&gt;
&lt;p&gt;Pods are a logically grouped collection of containers, meaning one Pod can potentially house a multitude of containers. A Pod gets its own DNS and virtual IP address after it has been created so Kubernetes can load balancer traffic to it. You rarely need to deal with containers directly. Even when debugging, (like looking at logs), you usually invoke &lt;code&gt;kubectl logs deployment/your-app -f&lt;/code&gt; instead of looking at a specific container. Although it is possible with &lt;code&gt;-c container_name&lt;/code&gt;. The &lt;code&gt;-f&lt;/code&gt; does a tail on the log.&lt;/p&gt;
&lt;h3 id=&#34;deployments&#34;&gt;Deployments&lt;/h3&gt;
&lt;p&gt;When creating any kind of resource in Kubernetes, it will use a Deployment in the background. A deployment describes a desired state of the current application. It&amp;rsquo;s an object you can use to update Pods or a Service to be in a different state, do an update, or rollout new version of your app. You don&amp;rsquo;t directly control a ReplicaSet, (as described later), but control the deployment object which creates and manages a ReplicaSet.&lt;/p&gt;
&lt;h3 id=&#34;services&#34;&gt;Services&lt;/h3&gt;
&lt;p&gt;By default a Pod will get an IP address. However, since Pods are a volatile thing in Kubernetes, you&amp;rsquo;ll need something more permanent. A queue, mysql, or an internal API, a frontend; these need to be long running and behind a static, unchanging IP or preferably a DNS record.&lt;/p&gt;
&lt;p&gt;For this purpose, Kubernetes has Services for which you can define modes of accessibility. Load Balanced, simple IP or internal DNS.&lt;/p&gt;
&lt;p&gt;How does Kubernetes know if a service is running correctly? You can configure Health Checks and Availability Checks. A Health Check will check whether a container is running, but that doesn&amp;rsquo;t mean that your service is running. For that, you have the availability check which pings a different endpoint in your application.&lt;/p&gt;
&lt;p&gt;Since Services are pretty important, I recommend that you read up on them later here: &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/&#34;&gt;Services&lt;/a&gt;. Advanced  warning though, this document is quite dense. Twenty four A4 pages of networking, services and discovery. It&amp;rsquo;s also vital to decide whether you want to seriously employ Kubernetes in production.&lt;/p&gt;
&lt;h3 id=&#34;dns--service-discovery&#34;&gt;DNS / Service Discovery&lt;/h3&gt;
&lt;p&gt;If you create a service in the cluster, that service will get a DNS record in Kubernetes provided by special Kubernetes deployments called kube-proxy and kube-dns. These two provide service discover inside a cluster. If you have a mysql service running and set &lt;code&gt;clusterIP: none&lt;/code&gt;, then everyone in the cluster can reach that service by pinging &lt;code&gt;mysql.default.svc.cluster.local&lt;/code&gt;. Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;mysql&lt;/code&gt; &amp;ndash; is the name of the service&lt;/li&gt;
&lt;li&gt;&lt;code&gt;default&lt;/code&gt; &amp;ndash; is the namespace name&lt;/li&gt;
&lt;li&gt;&lt;code&gt;svc&lt;/code&gt; &amp;ndash; is services&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cluster.local&lt;/code&gt; &amp;ndash; is a local cluster domain&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The domain can be changed via a custom definition. To access a service outside the cluster, a DNS provider has to be used, and Nginx (for example), to bind an IP address to a record. The public IP address of a service can be queried with the following commands:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NodePort &amp;ndash; &lt;code&gt;kubectl get -o jsonpath=&amp;quot;{.spec.ports[0].nodePort}&amp;quot; services mysql&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;LoadBalancer &amp;ndash; &lt;code&gt;kubectl get -o jsonpath=&amp;quot;{.spec.ports[0].LoadBalancer}&amp;quot; services mysql&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;template-files&#34;&gt;Template Files&lt;/h3&gt;
&lt;p&gt;Like Docker Compose, TerraForm or other service management tools, Kubernetes also provides infrastructure describing templates. What that means is that you rarely need  to do anything by hand.&lt;/p&gt;
&lt;p&gt;For example, consider the following yaml template which describes an nginx Deployment:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;#(1)&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;: &lt;span style=&#34;color:#75715e&#34;&gt;#(2)&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx-deployment&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;: &lt;span style=&#34;color:#75715e&#34;&gt;#(3)&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;: &lt;span style=&#34;color:#75715e&#34;&gt;#(4)&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;#(5)&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;: &lt;span style=&#34;color:#75715e&#34;&gt;#(6)&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx:1.7.9&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This is a simple deployment in which we do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(1) Define the type of the template with kind&lt;/li&gt;
&lt;li&gt;(2) Add metadata that will identify this deployment and every resource that it would create with a label (3)&lt;/li&gt;
&lt;li&gt;(4) Then comes the spec which describes the desired state
&lt;ul&gt;
&lt;li&gt;(5) For the nginx app, have 3 replicas&lt;/li&gt;
&lt;li&gt;(6) This is the template definition for the containers that this Pod will contain
&lt;ul&gt;
&lt;li&gt;nginx named container&lt;/li&gt;
&lt;li&gt;nginx:1.7.9 image (docker in this case)&lt;/li&gt;
&lt;li&gt;exposed ports&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;replicaset&#34;&gt;ReplicaSet&lt;/h3&gt;
&lt;p&gt;A ReplicaSet is a low level replication manager. It ensures that the correct number of replicates are running for a application. However, Deployments are at a higher level and should always manage ReplicaSets. You rarely need to use ReplicaSets directly unless you have a fringe case in which you want to control the specifics of replication.&lt;/p&gt;
&lt;h3 id=&#34;daemonset&#34;&gt;DaemonSet&lt;/h3&gt;
&lt;p&gt;Remember how I said Kubernetes is using Labels all the time? A DaemonSet is a controller that ensures that at daemonized application is always running on a node with a certain label.&lt;/p&gt;
&lt;p&gt;For example: you want all the nodes labelled with &lt;code&gt;logger&lt;/code&gt; or &lt;code&gt;mission_critical&lt;/code&gt; to run an logger / auditing service daemon. Then you create a DaemonSet and give it a node selector called &lt;code&gt;logger&lt;/code&gt; or &lt;code&gt;mission_critical&lt;/code&gt;. Kubernetes will look for a node that has that label. Always ensure that it will have an instance of that daemon running on it. Thus everyone running on that node will have access to that daemon locally.&lt;/p&gt;
&lt;p&gt;In case of my application, the NSQ daemon could be a DaemonSet. Make sure it&amp;rsquo;s up on a node which has the receiver component running by labelling a node with &lt;code&gt;receiver&lt;/code&gt; and specifying a DaemonSet with a &lt;code&gt;receiver&lt;/code&gt; application selector.&lt;/p&gt;
&lt;p&gt;The DaemonSet has all the benefits of the ReplicaSet. It&amp;rsquo;s scalable and Kubernetes manages it; which means, all life cycle events are handled by Kube ensuring it never dies, and when it does,  it will be immediately replaced.&lt;/p&gt;
&lt;h3 id=&#34;scaling&#34;&gt;Scaling&lt;/h3&gt;
&lt;p&gt;It&amp;rsquo;s trivial to scale in Kubernetes. The ReplicaSets take care of the number of instances of a Pod to run- as seen in the nginx deployment with the setting &lt;code&gt;replicas:3&lt;/code&gt;. It&amp;rsquo;s up to us to write our application in a way that allows Kubernetes to run multiple copies of it.&lt;/p&gt;
&lt;p&gt;Of course the settings are vast. You can specify which replicates must run on what Nodes, or on various waiting times as to how long to wait for an instance to come up. You can read more on this subject here: &lt;a href=&#34;https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/&#34;&gt;Horizontal Scaling&lt;/a&gt; and here: &lt;a href=&#34;https://kubernetes.io/docs/tutorials/kubernetes-basics/scale-interactive/&#34;&gt;Interactive Scaling with Kubernetes&lt;/a&gt; and of course the details of a &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/&#34;&gt;ReplicaSet&lt;/a&gt; which controls all the scaling made possible in Kubernetes.&lt;/p&gt;
&lt;h3 id=&#34;conclusion-for-kubernetes&#34;&gt;Conclusion for Kubernetes&lt;/h3&gt;
&lt;p&gt;It&amp;rsquo;s a convenient tool to handle container orchestration. Its unit of work are Pods and it has a layered architecture. The top level layer is Deployments through which you handle all other resources. It&amp;rsquo;s highly configurable. It provides an API for all calls you make, so potentially, instead of running &lt;code&gt;kubectl&lt;/code&gt; you can also write your own logic to send information to the Kubernetes API.&lt;/p&gt;
&lt;p&gt;It provides support for all major cloud providers natively by now and it&amp;rsquo;s completely open source. Feel free to contribute! And check the code if you would like to have a deeper understanding on how it works: &lt;a href=&#34;https://github.com/kubernetes/kubernetes&#34;&gt;Kubernetes on Github&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;minikube&#34;&gt;Minikube&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;m going to use &lt;a href=&#34;https://github.com/kubernetes/minikube/&#34;&gt;Minikube&lt;/a&gt;. Minikube is a local Kubernetes cluster simulator. It&amp;rsquo;s not great in simulating multiple nodes though, but for starting out and local play without any costs, it&amp;rsquo;s great. It uses a VM that can be fine tuned if necessary using VirtualBox and the likes.&lt;/p&gt;
&lt;p&gt;All of the kube template files that I&amp;rsquo;ll be using can be found here: &lt;a href=&#34;https://github.com/Skarlso/kube-cluster-sample/tree/master/kube_files&#34;&gt;Kube files&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; If, later on, you would like to play with scaling but notice that the replicates are always in &lt;code&gt;Pending&lt;/code&gt; state, remember that minikube employs a single node only. It might not allow multiple replicas on the same node, or just plainly ran out of resources to use. You can check available resources with the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl get nodes -o yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;building-the-containers&#34;&gt;Building the containers&lt;/h2&gt;
&lt;p&gt;Kubernetes supports most of the containers out there. I&amp;rsquo;m going to use Docker. For all the services I&amp;rsquo;ve built, there is a Dockerfile included in the repository. I encourage you to study them. Most of them are simple. For the go services, I&amp;rsquo;m using a multi stage build that has been  recently introduced. The Go services are Alpine Linux based. The Face Recognition service is Python. NSQ and MySQL are using their own containers.&lt;/p&gt;
&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;Kubernetes uses namespaces. If you don&amp;rsquo;t specify any, it will use the &lt;code&gt;default&lt;/code&gt; namespace. I&amp;rsquo;m going to permanently set a context to avoid polluting the default namespace. You do that like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;❯ kubectl config set-context kube-face-cluster --namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;face
Context &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kube-face-cluster&amp;#34;&lt;/span&gt; created.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You have to also start using the context once it&amp;rsquo;s created, like so:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;❯ kubectl config use-context kube-face-cluster
Switched to context &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kube-face-cluster&amp;#34;&lt;/span&gt;.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;After this, all &lt;code&gt;kubectl&lt;/code&gt; commands will use the namespace &lt;code&gt;face&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;deploying-the-application&#34;&gt;Deploying the Application&lt;/h2&gt;
&lt;p&gt;Overview of Pods and Services:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/kube_deployed.png&#34; alt=&#34;kube deployed&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;mysql&#34;&gt;MySQL&lt;/h3&gt;
&lt;p&gt;The first Service I&amp;rsquo;m going to deploy is my database.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m using the Kubernetes example located here &lt;a href=&#34;https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/#deploy-mysql&#34;&gt;Kube MySQL&lt;/a&gt; which fits my needs. Please note that this file is using a plain password for MYSQL_PASSWORD. I&amp;rsquo;m going to employ a vault as described here &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/secret/&#34;&gt;Kubernetes Secrets&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve created a secret locally as described in that document using a secret yaml:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kube-face-secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Opaque&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;mysql_password&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;base64codehere&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I created the  base64 code via the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;echo -n &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ubersecurepassword&amp;#34;&lt;/span&gt; | base64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And, this is what you&amp;rsquo;ll see in my deployment yaml file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;...
- &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MYSQL_ROOT_PASSWORD&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;secretKeyRef&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kube-face-secret&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql_password&lt;/span&gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Another thing worth mentioning: It&amp;rsquo;s using a volume to persist the database. The volume definition is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;...
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql-persistent-storage&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/lib/mysql&lt;/span&gt;
...
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql-persistent-storage&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeClaim&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;claimName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql-pv-claim&lt;/span&gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;presistentVolumeClain&lt;/code&gt; is key here. This tells Kubernetes that this resource requires a persistent volume. How it&amp;rsquo;s provided is abstracted away from the user. You can be sure that Kubernetes will provide a volume that will always be there. It is similar to Pods. To read up on the details, check out this document: &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes&#34;&gt;Kubernetes Persistent Volumes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Deploying the mysql Service is done with the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f mysql.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;apply&lt;/code&gt; vs &lt;code&gt;create&lt;/code&gt;. In short, &lt;code&gt;apply&lt;/code&gt; is considered a declarative object configuration command while &lt;code&gt;create&lt;/code&gt; is imperative. What this means for now is that ‘create’ is usually for a one of tasks, like running something or creating a deployment. While, when using apply, the user doesn&amp;rsquo;t define the action to be taken. That will be defined by Kubernetes based on the current status of the cluster. Thus, when there is no service called &lt;code&gt;mysql&lt;/code&gt; and I&amp;rsquo;m calling &lt;code&gt;apply -f mysql.yaml&lt;/code&gt; it will create the service. When running again, Kubernetes won&amp;rsquo;t do anything. But if I would run &lt;code&gt;create&lt;/code&gt; again it will throw an error saying the service is already created.&lt;/p&gt;
&lt;p&gt;For more information, check out the following docs: &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/&#34;&gt;Kubernetes Object Management&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/&#34;&gt;Imperative Configuration&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/&#34;&gt;Declarative Configuration&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To see progress information, run:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Describes the whole process&lt;/span&gt;
kubectl describe deployment mysql
&lt;span style=&#34;color:#75715e&#34;&gt;# Shows only the pod&lt;/span&gt;
kubectl get pods -l app&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;mysql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Output should be similar to this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;...
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &amp;lt;none&amp;gt;
NewReplicaSet:   mysql-55cd6b9f47 &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;1/1 replicas created&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Or in case of &lt;code&gt;get pods&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;NAME                     READY     STATUS    RESTARTS   AGE
mysql-78dbbd9c49-k6sdv   1/1       Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          18s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To test the instance, run the following snippet:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl run -it --rm --image&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;mysql:5.6 --restart&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Never mysql-client -- mysql -h mysql -pyourpasswordhere
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;GOTCHA&lt;/strong&gt;: If you change the password now, it&amp;rsquo;s not enough to re-apply your yaml file to update the container. Since the DB is persisted, the password will not be changed. You have to delete the whole deployment with &lt;code&gt;kubectl delete -f mysql.yaml&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You should see the following when running a &lt;code&gt;show databases&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;If you don&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&amp;#39;&lt;/span&gt;t see a command prompt, try pressing enter.
mysql&amp;gt;
mysql&amp;gt;
mysql&amp;gt; show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| kube               |
| mysql              |
| performance_schema |
+--------------------+
&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt; rows in set &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;0.00 sec&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;

mysql&amp;gt; exit
Bye
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You&amp;rsquo;ll also notice that I’ve mounted a file located here: &lt;a href=&#34;https://github.com/Skarlso/kube-cluster-sample/blob/master/database_setup.sql&#34;&gt;Database Setup SQL&lt;/a&gt; into the container. MySQL container automatically executes these. That file will bootstrap some data and the schema I&amp;rsquo;m going to use.&lt;/p&gt;
&lt;p&gt;The volume definition is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql-persistent-storage&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/lib/mysql&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bootstrap-script&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/docker-entrypoint-initdb.d/database_setup.sql&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
- &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql-persistent-storage&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeClaim&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;claimName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql-pv-claim&lt;/span&gt;
- &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bootstrap-script&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/Users/hannibal/golang/src/github.com/Skarlso/kube-cluster-sample/database_setup.sql&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;File&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To check if the bootstrap script was successful, run this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;~/golang/src/github.com/Skarlso/kube-cluster-sample/kube_files master*
❯ kubectl run -it --rm --image&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;mysql:5.6 --restart&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Never mysql-client -- mysql -h mysql -uroot -pyourpasswordhere kube
If you don&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&amp;#39;&lt;/span&gt;t see a command prompt, try pressing enter.

mysql&amp;gt; show tables;
+----------------+
| Tables_in_kube |
+----------------+
| images         |
| person         |
| person_images  |
+----------------+
&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; rows in set &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;0.00 sec&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;

mysql&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This concludes the database service setup. Logs for this service can be viewed with the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl logs deployment/mysql -f
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;nsq-lookup&#34;&gt;NSQ Lookup&lt;/h3&gt;
&lt;p&gt;The NSQ Lookup will run as an internal service. It doesn&amp;rsquo;t need access from the outside, so I&amp;rsquo;m setting &lt;code&gt;clusterIP: None&lt;/code&gt; which will tell Kubernetes that this service is a headless service. This means that it won&amp;rsquo;t be load balanced, and it won&amp;rsquo;t be a single IP service. The DNS will be based upon service selectors.&lt;/p&gt;
&lt;p&gt;Our NSQ Lookup selector is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nsqlookup&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Thus, the internal DNS will look like this: &lt;code&gt;nsqlookup.default.svc.cluster.local&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Headless services are described in detail here: &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/#headless-services&#34;&gt;Headless Service&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Basically it&amp;rsquo;s the same as MySQL, just with slight modifications. As stated earlier, I&amp;rsquo;m using NSQ&amp;rsquo;s own Docker Image called &lt;code&gt;nsqio/nsq&lt;/code&gt;. All nsq commands are there, so nsqd will also use this image just with a different command. For nsqlookupd, the command is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/nsqlookupd&amp;#34;&lt;/span&gt;]
&lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;--broadcast-address=nsqlookup.default.svc.cluster.local&amp;#34;&lt;/span&gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What&amp;rsquo;s the &lt;code&gt;--broadcast-address&lt;/code&gt; for, you might ask? By default, nsqlookup will use the &lt;code&gt;hostname&lt;/code&gt; as broadcast address. When the consumer runs a callback it will try to connect to something like: &lt;code&gt;http://nsqlookup-234kf-asdf:4161/lookup?topics=image&lt;/code&gt;. Please note that &lt;code&gt;nsqlookup-234kf-asdf&lt;/code&gt; is the hostname of the container. By setting the broadcast-address to the internal DNS, the callback will be: &lt;code&gt;http://nsqlookup.default.svc.cluster.local:4161/lookup?topic=images&lt;/code&gt;. Which will work as expected.&lt;/p&gt;
&lt;p&gt;NSQ Lookup also requires two ports forwarded: One for broadcasting and one for nsqd callback. These are exposed in the Dockerfile, and then utilized in the Kubernetes template. Like this:&lt;/p&gt;
&lt;p&gt;In the container template:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4160&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;hostPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4160&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4161&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;hostPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4161&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the service template:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;tcp&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4160&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4160&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;http&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4161&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4161&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Names are required by Kubernetes.&lt;/p&gt;
&lt;p&gt;To create this service, I&amp;rsquo;m using the same command as before:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f nsqlookup.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This concludes nsqlookupd. Two of the major players are in the sack!&lt;/p&gt;
&lt;h3 id=&#34;receiver-1&#34;&gt;Receiver&lt;/h3&gt;
&lt;p&gt;This is a more complex one. The receiver will do three things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create some deployments;&lt;/li&gt;
&lt;li&gt;Create the nsq daemon;&lt;/li&gt;
&lt;li&gt;Expose the service to the public.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;deployments-1&#34;&gt;Deployments&lt;/h4&gt;
&lt;p&gt;The first deployment it creates is its own. The receiver’s container is &lt;code&gt;skarlso/kube-receiver-alpine&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;nsq-daemon&#34;&gt;Nsq Daemon&lt;/h4&gt;
&lt;p&gt;The receiver starts an nsq daemon. As stated earlier, the receiver runs an nsqd with it-self. It does this so talking to it can happen locally and not over the network. By making the receiver do this, they will end up on the same node.&lt;/p&gt;
&lt;p&gt;NSQ daemon also needs some adjustments and parameters.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4150&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;hostPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4150&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4151&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;hostPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4151&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NSQLOOKUP_ADDRESS&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nsqlookup.default.svc.cluster.local&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NSQ_BROADCAST_ADDRESS&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nsqd.default.svc.cluster.local&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/nsqd&amp;#34;&lt;/span&gt;]
        &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;--lookupd-tcp-address=$(NSQLOOKUP_ADDRESS):4160&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;--broadcast-address=$(NSQ_BROADCAST_ADDRESS)&amp;#34;&lt;/span&gt;]

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can see that the lookup-tcp-address and the broadcast-address are set. Lookup tcp address is the DNS for the nsqlookupd service. And the broadcast address is necessary, just like with nsqlookupd, so the callbacks are working properly.&lt;/p&gt;
&lt;h4 id=&#34;public-facing&#34;&gt;Public facing&lt;/h4&gt;
&lt;p&gt;Now, this is the first time I&amp;rsquo;m deploying a public facing service. There are two options. I could use a LoadBalancer since this API will be under heavy load. And if this would be deployed anywhere in production, then it should be using one.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m doing this locally though- with one node- so something called a &lt;code&gt;NodePort&lt;/code&gt; is enough. A &lt;code&gt;NodePort&lt;/code&gt; exposes a service on each node&amp;rsquo;s IP at a static port. If not specified, it will assign a random port on the host between 30000-32767. But it can also be configured to be a specific port, using &lt;code&gt;nodePort&lt;/code&gt; in the template file. To reach this service, use &lt;code&gt;&amp;lt;NodeIP&amp;gt;:&amp;lt;NodePort&amp;gt;&lt;/code&gt;. If more than one node is configured, a LoadBalancer can multiplex them to a single IP.&lt;/p&gt;
&lt;p&gt;For further information, check out this document: &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services---service-types&#34;&gt;Publishing Services&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Putting this all together, we&amp;rsquo;ll get a receiver-service for which the template for is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;receiver-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;8000&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;8000&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;receiver&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NodePort&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For a fixed nodePort on 8000 a definition of &lt;code&gt;nodePort&lt;/code&gt; must be provided:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;receiver-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;8000&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;8000&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;receiver&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NodePort&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;nodePort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;8000&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;image-processor-1&#34;&gt;Image processor&lt;/h3&gt;
&lt;p&gt;The Image Processor is where I&amp;rsquo;m handling passing off images to be identified. It should have access to nsqlookupd, mysql and the gRPC endpoint of the face recognition service. This is actually quite a boring service. In fact, it&amp;rsquo;s not even a service at all. It doesn&amp;rsquo;t expose anything, and thus it&amp;rsquo;s the first deployment only component. For brevity, here is the whole template:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;image-processor-deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;image-processor&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;image-processor&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;image-processor&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;skarlso/kube-processor-alpine:latest&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MYSQL_CONNECTION&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;mysql.default.svc.cluster.local&amp;#34;&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MYSQL_USERPASSWORD&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;secretKeyRef&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kube-face-secret&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql_userpassword&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MYSQL_PORT&lt;/span&gt;
          &lt;span style=&#34;color:#75715e&#34;&gt;# TIL: If this is 3306 without &amp;#34; kubectl throws an error.&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;3306&amp;#34;&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MYSQL_DBNAME&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kube&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NSQ_LOOKUP_ADDRESS&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nsqlookup.default.svc.cluster.local:4161&amp;#34;&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;GRPC_ADDRESS&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;face-recog.default.svc.cluster.local:50051&amp;#34;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The only interesting points in this file are the multitude of environment properties that are used to configure the application. Note the nsqlookupd address and the grpc address.&lt;/p&gt;
&lt;p&gt;To create this deployment, run:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f image_processor.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;face---recognition&#34;&gt;Face - Recognition&lt;/h3&gt;
&lt;p&gt;The face recognition service does have a service. It&amp;rsquo;s a simple one. Only needed by image-processor. It&amp;rsquo;s template is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;face-recog&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;50051&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;50051&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;face-recog&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;clusterIP&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;None&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The more interesting part is that it requires two volumes. The two volumes are &lt;code&gt;known_people&lt;/code&gt; and &lt;code&gt;unknown_people&lt;/code&gt;. Can you guess what they will contain? Yep, images. The &lt;code&gt;known_people&lt;/code&gt; volume contains all the images associated to the known people in the database. The &lt;code&gt;unknown_people&lt;/code&gt; volume will contain all new images. And that&amp;rsquo;s the path we will need to use when sending images from the receiver; that is wherever the mount point points too, which in my case is &lt;code&gt;/unknown_people&lt;/code&gt;. Basically, the path needs to be one that the face recognition service can access.&lt;/p&gt;
&lt;p&gt;Now, with Kubernetes and Docker, this is easy. It can be a mounted S3 or some kind of nfs or a local mount from host to guest. The possibilities are endless (around a dozen or so). I&amp;rsquo;m going to use a local mount for the sake of simplicity.&lt;/p&gt;
&lt;p&gt;Mounting a volume is done in two parts. Firstly, the Dockerfile has to specify volumes:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Dockerfile&#34; data-lang=&#34;Dockerfile&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;VOLUME&lt;/span&gt; [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/unknown_people&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/known_people&amp;#34;&lt;/span&gt; ]&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Secondly, the Kubernetes template needs add &lt;code&gt;volumeMounts&lt;/code&gt; as seen in the MySQL service; the difference being &lt;code&gt;hostPath&lt;/code&gt; instead of claimed volume:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;known-people-storage&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/known_people&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;unknown-people-storage&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/unknown_people&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;known-people-storage&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/Users/hannibal/Temp/known_people&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Directory&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;unknown-people-storage&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/Users/hannibal/Temp/&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Directory&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We also need to set the &lt;code&gt;known_people&lt;/code&gt; folder config setting for the face recognition service. This is done via an environment property:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;KNOWN_PEOPLE&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/known_people&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then the Python code will look up images, like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;        known_people &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;getenv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;KNOWN_PEOPLE&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;known_people&amp;#39;&lt;/span&gt;)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Known people images location is: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; known_people)
        images &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;image_files_in_folder(known_people)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Where &lt;code&gt;image_files_in_folder&lt;/code&gt; is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;image_files_in_folder&lt;/span&gt;(self, folder):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; [os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(folder, f) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; f &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;listdir(folder) &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;match(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.*\.(jpg|jpeg|png)&amp;#39;&lt;/span&gt;, f, flags&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;I)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Neat.&lt;/p&gt;
&lt;p&gt;Now, if the receiver receives a request (and sends it off further down the line) similar to the one below&amp;hellip;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#34;path&amp;#34;:&amp;#34;/unknown_people/unknown220.jpg&amp;#34;}&amp;#39;&lt;/span&gt; http://192.168.99.100:30251/image/post
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&amp;hellip;it will look for an image called unknown220.jpg under &lt;code&gt;/unknown_people&lt;/code&gt;, locate an image in the known_folder that corresponds to the person in the unknown image and return the name of the image that matches.&lt;/p&gt;
&lt;p&gt;Looking at logs, you should see something like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Receiver&lt;/span&gt;
❯ curl -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#34;path&amp;#34;:&amp;#34;/unknown_people/unknown219.jpg&amp;#34;}&amp;#39;&lt;/span&gt; http://192.168.99.100:30251/image/post
got path: &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;Path:/unknown_people/unknown219.jpg&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
image saved with id: &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;
image sent to nsq

&lt;span style=&#34;color:#75715e&#34;&gt;# Image Processor&lt;/span&gt;
2018/03/26 18:11:21 INF    &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;images/ch&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; querying nsqlookupd http://nsqlookup.default.svc.cluster.local:4161/lookup?topic&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;images
2018/03/26 18:11:59 Got a message: &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;
2018/03/26 18:11:59 Processing image id:  &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;
2018/03/26 18:12:00 got person:  Hannibal
2018/03/26 18:12:00 updating record with person id
2018/03/26 18:12:00 &lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And that concludes all of the services that we need to deploy.&lt;/p&gt;
&lt;h3 id=&#34;frontend&#34;&gt;Frontend&lt;/h3&gt;
&lt;p&gt;Lastly, there is a small web-app which displays the information in the db for convenience. This is also a public facing service with the same parameters as the receiver.&lt;/p&gt;
&lt;p&gt;It looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/kube-frontend.png&#34; alt=&#34;frontend&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;recap&#34;&gt;Recap&lt;/h3&gt;
&lt;p&gt;We are now at the point in which I’ve deployed a bunch of services. A recap off the commands I’ve used so far:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f mysql.yaml
kubectl apply -f nsqlookup.yaml
kubectl apply -f receiver.yaml
kubectl apply -f image_processor.yaml
kubectl apply -f face_recognition.yaml
kubectl apply -f frontend.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;These could be in any order since the application does not allocate connections on start. (Except for image_processor&amp;rsquo;s NSQ consumer. But that re-tries.)&lt;/p&gt;
&lt;p&gt;Query-ing kube for running pods with &lt;code&gt;kubectl get pods&lt;/code&gt; should show something like this if there were no errors:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;❯ kubectl get pods
NAME                                          READY     STATUS    RESTARTS   AGE
face-recog-6bf449c6f-qg5tr                    1/1       Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          1m
image-processor-deployment-6467468c9d-cvx6m   1/1       Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          31s
mysql-7d667c75f4-bwghw                        1/1       Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          36s
nsqd-584954c44c-299dz                         1/1       Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          26s
nsqlookup-7f5bdfcb87-jkdl7                    1/1       Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          11s
receiver-deployment-5cb4797598-sf5ds          1/1       Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          26s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Running &lt;code&gt;minikube service list&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;❯ minikube service list
|-------------|----------------------|-----------------------------|
|  NAMESPACE  |         NAME         |             URL             |
|-------------|----------------------|-----------------------------|
| default     | face-recog           | No node port                |
| default     | kubernetes           | No node port                |
| default     | mysql                | No node port                |
| default     | nsqd                 | No node port                |
| default     | nsqlookup            | No node port                |
| default     | receiver-service     | http://192.168.99.100:30251 |
| kube-system | kube-dns             | No node port                |
| kube-system | kubernetes-dashboard | http://192.168.99.100:30000 |
|-------------|----------------------|-----------------------------|
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;rolling-update&#34;&gt;Rolling update&lt;/h3&gt;
&lt;p&gt;What happens during a rolling update?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://skarlso.github.io/img/kube_rotate.png&#34; alt=&#34;kube rotate&#34;&gt;&lt;/p&gt;
&lt;p&gt;As it happens during software development, change is requested/needed to some parts of the system. So what happens to our cluster if I change one of its components without breaking the others whilst also maintaining backwards compatibility with no disruption to user experience? Thankfully Kubernetes can help with that.&lt;/p&gt;
&lt;p&gt;What I don&amp;rsquo;t like is that the API only handles one image at a time. Unfortunately there is no bulk upload option.&lt;/p&gt;
&lt;h4 id=&#34;code&#34;&gt;Code&lt;/h4&gt;
&lt;p&gt;Currently, we have the following code segment dealing with a single image:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// PostImage handles a post of an image. Saves it to the database
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// and sends it to NSQ for further processing.
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;PostImage&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;w&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;http&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;ResponseWriter&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;r&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;http&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Request&lt;/span&gt;) {
&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;
}

&lt;span style=&#34;color:#66d9ef&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;() {
    &lt;span style=&#34;color:#a6e22e&#34;&gt;router&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mux&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;NewRouter&lt;/span&gt;()
    &lt;span style=&#34;color:#a6e22e&#34;&gt;router&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;HandleFunc&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/image/post&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;PostImage&lt;/span&gt;).&lt;span style=&#34;color:#a6e22e&#34;&gt;Methods&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;POST&amp;#34;&lt;/span&gt;)
    &lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Fatal&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;http&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;ListenAndServe&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:8000&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;router&lt;/span&gt;))
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We have two options: Add a new endpoint with &lt;code&gt;/images/post&lt;/code&gt; and make the client use that, or modify the existing one.&lt;/p&gt;
&lt;p&gt;The new client code has the advantage in that it can fall back to submitting the old way if the new endpoint isn&amp;rsquo;t available. The old client code, however, doesn&amp;rsquo;t have this advantage so we can&amp;rsquo;t change the way our code works right now. Consider this: You have 90 servers and you do a slow paced rolling update that will take out servers one step at a time whilst doing an update. If an update lasts around a minute, the whole process will take around one and a half hours to complete, (not counting any parallel updates).&lt;/p&gt;
&lt;p&gt;During this time, some of your servers will run the new code and some will run the old one. Calls are load balanced, thus you have no control over which servers will be hit. If a client is trying to do a call the new way but hits an old server, the client will fail. The client can try and fallback, but since you eliminated the old version it will not succeed unless it, by mere chance, hits a server with the new code (assuming no sticky sessions are set).&lt;/p&gt;
&lt;p&gt;Also, once all your servers are updated, an old client will not be able to use your service any longer.&lt;/p&gt;
&lt;p&gt;Now, you can argue that you don&amp;rsquo;t want to keep old versions of your code forever. And that’s true in a sense. That&amp;rsquo;s why we are going to modify the old code to simply call the new one with some slight augmentations. This way, once all clients have been migrated, the code can simply be deleted without any problems.&lt;/p&gt;
&lt;h4 id=&#34;new-endpoint&#34;&gt;New Endpoint&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s add a new route method:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;router&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;HandleFunc&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/images/post&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;PostImages&lt;/span&gt;).&lt;span style=&#34;color:#a6e22e&#34;&gt;Methods&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;POST&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Updating the old one to call the new one with a modified body looks like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// PostImage handles a post of an image. Saves it to the database
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// and sends it to NSQ for further processing.
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;PostImage&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;w&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;http&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;ResponseWriter&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;r&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;http&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Request&lt;/span&gt;) {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;var&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;p&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Path&lt;/span&gt;
    &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;json&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;NewDecoder&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;r&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Body&lt;/span&gt;).&lt;span style=&#34;color:#a6e22e&#34;&gt;Decode&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;p&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {
      &lt;span style=&#34;color:#a6e22e&#34;&gt;fmt&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Fprintf&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;w&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;got error while decoding body: %s&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;)
      &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt;
    }
    &lt;span style=&#34;color:#a6e22e&#34;&gt;fmt&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Fprintf&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;w&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;got path: %+v\n&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;p&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;var&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ps&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Paths&lt;/span&gt;
    &lt;span style=&#34;color:#a6e22e&#34;&gt;paths&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; make([]&lt;span style=&#34;color:#a6e22e&#34;&gt;Path&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
    &lt;span style=&#34;color:#a6e22e&#34;&gt;paths&lt;/span&gt; = append(&lt;span style=&#34;color:#a6e22e&#34;&gt;paths&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;p&lt;/span&gt;)
    &lt;span style=&#34;color:#a6e22e&#34;&gt;ps&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Paths&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;paths&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;var&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;pathsJSON&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bytes&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Buffer&lt;/span&gt;
    &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;json&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;NewEncoder&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;pathsJSON&lt;/span&gt;).&lt;span style=&#34;color:#a6e22e&#34;&gt;Encode&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;ps&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {
      &lt;span style=&#34;color:#a6e22e&#34;&gt;fmt&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Fprintf&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;w&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;failed to encode paths: %s&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;)
      &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt;
    }
    &lt;span style=&#34;color:#a6e22e&#34;&gt;r&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Body&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;ioutil&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;NopCloser&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;pathsJSON&lt;/span&gt;)
    &lt;span style=&#34;color:#a6e22e&#34;&gt;r&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;ContentLength&lt;/span&gt; = int64(&lt;span style=&#34;color:#a6e22e&#34;&gt;pathsJSON&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Len&lt;/span&gt;())
    &lt;span style=&#34;color:#a6e22e&#34;&gt;PostImages&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;w&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;r&lt;/span&gt;)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Well, the naming could be better, but you should get the basic idea. I&amp;rsquo;m modifying the incoming single path by wrapping it into the new format and sending it over to the new endpoint handler. And that&amp;rsquo;s it! There are a few more modifications. To check them out, take a look at this PR: &lt;a href=&#34;https://github.com/Skarlso/kube-cluster-sample/pull/1&#34;&gt;Rolling Update Bulk Image Path PR&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now, the receiver can be called in two ways:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Single Path:&lt;/span&gt;
curl -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#34;path&amp;#34;:&amp;#34;unknown4456.jpg&amp;#34;}&amp;#39;&lt;/span&gt; http://127.0.0.1:8000/image/post

&lt;span style=&#34;color:#75715e&#34;&gt;# Multiple Paths:&lt;/span&gt;
curl -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#34;paths&amp;#34;:[{&amp;#34;path&amp;#34;:&amp;#34;unknown4456.jpg&amp;#34;}]}&amp;#39;&lt;/span&gt; http://127.0.0.1:8000/images/post
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here, the client is curl. Normally, if the client is a service, I would modify it that in case the new end-point throws a 404 it would try the old one next.&lt;/p&gt;
&lt;p&gt;For brevity, I&amp;rsquo;m not modifying NSQ and the others to handle bulk image processing; they will still receive it one by one. I&amp;rsquo;ll leave that up to you as homework ;)&lt;/p&gt;
&lt;h4 id=&#34;new-image&#34;&gt;New Image&lt;/h4&gt;
&lt;p&gt;To perform a rolling update, I must create a new image first from the receiver service.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker build -t skarlso/kube-receiver-alpine:v1.1 .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once this is complete, we can begin rolling out the change.&lt;/p&gt;
&lt;h4 id=&#34;rolling-update-1&#34;&gt;Rolling update&lt;/h4&gt;
&lt;p&gt;In Kubernetes, you can configure your rolling update in multiple ways:&lt;/p&gt;
&lt;h5 id=&#34;manual-update&#34;&gt;Manual Update&lt;/h5&gt;
&lt;p&gt;If I was using a container version in my config file called &lt;code&gt;v1.0&lt;/code&gt;, then doing an update is simply calling:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl rolling-update receiver --image:skarlso/kube-receiver-alpine:v1.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If there is a problem during the rollout we can always rollback.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl rolling-update receiver --rollback
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It will set back the previous version. No fuss, no muss.&lt;/p&gt;
&lt;h5 id=&#34;apply-a-new-configuration-file&#34;&gt;Apply a new configuration file&lt;/h5&gt;
&lt;p&gt;The problem with by-hand updates is that they aren&amp;rsquo;t in source control.&lt;/p&gt;
&lt;p&gt;Consider this: Something has changed, A couple of servers got updated by hand to do a quick “patch fix”, but nobody witnessed it and it wasn’t documented. A new person comes along and does a change to the template and applies the template to the cluster. All the servers are updated, and then all of a sudden there is a service outage.&lt;/p&gt;
&lt;p&gt;Long story short, the servers which got updated are written over because the template doesn’t  reflect what has been done manually.&lt;/p&gt;
&lt;p&gt;The recommended way is to change the template in order to use the new version, and than apply the template with the &lt;code&gt;apply&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;Kubernetes recommends that a Deployment with ReplicaSets should handle a rollout. This means there must be at least two replicates present for a rolling update. If less than two replicates are present then the update won&amp;rsquo;t work (unless &lt;code&gt;maxUnavailable&lt;/code&gt; is set to 1). I increase the replica count in yaml. I also set the new image version for the receiver container.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
...
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;receiver&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;skarlso/kube-receiver-alpine:v1.1&lt;/span&gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Looking at the progress, this is what you should see :&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;❯ kubectl rollout status deployment/receiver-deployment
Waiting &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; rollout to finish: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; out of &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; new replicas have been updated...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can add in additional rollout configuration settings by specifying the &lt;code&gt;strategy&lt;/code&gt; part of the template like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;strategy&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;RollingUpdate&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;rollingUpdate&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;maxSurge&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;maxUnavailable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Additional information on rolling update can be found in the below documents: &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-a-deployment&#34;&gt;Deployment Rolling Update&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment&#34;&gt;Updating a Deployment&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#updating-your-application-without-a-service-outage&#34;&gt;Manage Deployments&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/&#34;&gt;Rolling Update using ReplicaController&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE MINIKUBE USERS&lt;/strong&gt;: Since we are doing this on a local machine with one node and 1 replica of an application, we have to set &lt;code&gt;maxUnavailable&lt;/code&gt; to &lt;code&gt;1&lt;/code&gt;; otherwise Kubernetes won&amp;rsquo;t allow the update to happen, and the new version will remain in &lt;code&gt;Pending&lt;/code&gt; state. That’s because we aren’t allowing for a services to exist with no running containers; which basically means service outage.&lt;/p&gt;
&lt;h3 id=&#34;scaling-1&#34;&gt;Scaling&lt;/h3&gt;
&lt;p&gt;Scaling is dead easy with Kubernetes. Since it&amp;rsquo;s managing the whole cluster, you basically just need to put a number into the template of the desired replicas to use.&lt;/p&gt;
&lt;p&gt;This has been a great post so far, but it&amp;rsquo;s getting too long. I&amp;rsquo;m planning on writing a follow-up where I will be truly scaling things up on AWS with multiple nodes and replicas; plus deploying a Kubernetes cluster with &lt;a href=&#34;https://github.com/kubernetes/kops&#34;&gt;Kops&lt;/a&gt;. So stay tuned!&lt;/p&gt;
&lt;h3 id=&#34;cleanup&#34;&gt;Cleanup&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl delete deployments --all
kubectl delete services -all
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;final-words&#34;&gt;Final Words&lt;/h1&gt;
&lt;p&gt;And that’s it ladies and gentlemen. We wrote, deployed, updated and scaled (well, not yet really) a distributed application with Kubernetes.&lt;/p&gt;
&lt;p&gt;If you have any questions, please feel free to chat in the comments below. I&amp;rsquo;m happy to answer.&lt;/p&gt;
&lt;p&gt;I hope you’ve enjoyed reading this. I know it&amp;rsquo;s quite long; I was thinking of splitting it up multiple posts, but having a cohesive, one page guide is useful and makes it easy to find, save, and print.&lt;/p&gt;
&lt;p&gt;Thank you for reading,
Gergely.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
