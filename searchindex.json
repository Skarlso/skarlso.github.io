[{"content":"Reader’s Digest I thought it would a cool idea if I kept a summary of the things I’ve read or listened to on a monthly basis. Here is April of 2021 so far. Enjoy.\nInvincible After seeing it air on Amazon Prime, I had to go and read the comic. It’s quite a lot, but I assure you it’s worth it. Some spoiler alerts…\nOkay, so we follow this guy, Mark Grayson. He’s Invincible. He is also and alien, called a Viltrumite. I can hardly summaries over 3000 pages of comic books so I’ll try to be brief.\nThe Viltrumites are a race of warrior people who wants to conquer the whole galaxy. They also have the power to do so. They are virtually immortal, live for thousands of years and are very durable. They can fly, have super strength and a bunch of other things. The stories are insane. We follow all kinds of people across multiverses, a lot of villains and heroes. There is gore, sex, violance and immature jokes all around. It’s fantastic.\nThere are better episodes and there are boring episodes as it goes with these things. The one thing that bothered me a lot is that the Viltrumites, of course, have a weak spot. You have to remember that these are people who want to be eliminated by a lot of other people in the whole galaxy. There is a literal coalition of planets who want to kill them.\nAnd there are two main things which can kill / incapacitate them. The first one is a virus, which was developed by a traiter Viltrumite which killed like 99.9% of them. And the other is the same weakness as the things have in the movie called The quiet place. It’s specific frequency sound. Granted, some very specific frequency, but it’s sound. Because Viltrumites have a delicate equilibrium because of their flying ability, if there is a specific frequency sound being emitted in close proximity it can incapacitate or even kill a Viltrumite.\nWhich makes me wonder that for thousands of years in multiple, parallel universes and with thousands of alien races, nobody even thought of doing that? It had to be Earth and Humans ( and a Dinosaur ) who came up with it? That’s just implausible really. And after being used only a couple of times, they don’t use it again on a global scale!\nAnyways. The story is still incredible and it has a lot of amazing content so I encourage comic lovers to read it all!\nConclusion That’s it for this month. Some lengthy blog posts and Invincible pretty much took up my reading time.\nAnd as always, Thanks for reading, Gergely.\n","title":"Reader’s Digest 2021-04","uri":"/2021/04/21/readers-digest/"},{"content":"Reader’s Digest I thought it would a cool idea if I kept a summary of the things I’ve read or listened to on a monthly basis. Here is March of 2021 so far. Enjoy.\nThe Aurora Database paper The paper about Aurora database from AWS can be found here: Paper. It details the design decision taken to support a highly available, fault tolerant, fast replicating database. They take the following approach… They modified mysql database such as that they only send around the redo log and the redo log is enough to recover / replicate in order to achieve write and read consistency. They separate the data into Protected Groups and speed up terabytes of recovery by doing 10 Gigabyte segments in parallel. The database IS the logs. By only replicating the log instead of the data and the data page, they save millions in networks costs. The main gain however, is that the storage was modified to understand the application. Instead of using General store they use a storage which understand the data. In this case, decoupling storage from the database, as so many do, was actually a drawback.\nGCatch paper This paper is a static concurrency bug analyser for Go found here Paper.\nIt’s ingenious! It’s a static analyser which finds mostly blocking bugs using channels in Go. In Go, it’s really easy to write concurrent software using something called Channels. They are basically coroutines multiplexed onto kernel threads and thus you can have a million of them running around doing stuff. Go effectively made IO operators CPU bound with them. Coroutines aren’t new, however, it’s really easy to mess up code with channels in subtle ways. Analyzers exist, however, GCatch argues that they can’t find the most subtle of bugs, only some surface bugs really.\nThis paper proposes a tool which does inter-procedural, path-sensitive analysis and uses Z3 to find paths which can lead to deadlocks in code that uses locking primitives and channels. It also contains five other prominent tools. It converts mutexes into channels internally with buffer size zero and sends on it on Lock and reads from it on Unlock, then performs a bunch of path combinations and goes through those suspicious paths and performs its analysis.\nThey found a hundred and something bugs in Docker and Kubernetes. Things like, sending on a channel in select when in fact, a timeout already returned, thus that Go routine is not indefinitely stuck. Since it can’t send its output on the channel, the program didn’t quit so it’s not GC-d. A simple fix is to make the channel of size 1 so even if there is a chance that the scope quit it can still send and quit. Like Exec.\nIt’s an interesting read and the tool is awesome, however…. It was written with Go 14 and it’s proving to be difficult to port to current version using modules. I would hate to see this tool getting left behind because it can’t be turned into a linter.\nRhythm of War - Brandon Sanderson Amazon.\nAn epic continuation of this saga with over a 1000 pages long and 54 hours of listening time on Audible. This story has been ongoing for a while now. Brandon Sanderson came out with the first book back in 2010. This is the continuation of the Stormlight Archive series. These are massive master pieces. I first came along Brandon Sanderson when I read the Mistborn series. That was another epic novel. I love reading Sanderson because he comes up with some unique ways of magic or magic like abilities which have some divine sense in the end, or have some interesting explanation. And their abilities are almost always used in interesting ways.\nFor example, a simple ability to pull or push metal. Turns out that results in things like, shooting coins, or literally flying as the person tosses a coin to the ground and pushes on it, pushing themselves upwards in the end.\nI could write many many pages about each and every fantastic novel, but I’m going to stick to this one expecting that people know about the series.\nI’ve listened to this one as I’m insane busy, I couldn’t have read a 1000 something pages book. The fantastic work of Micheal Kramer and Kate Reading is always a treat to listen too. They are both excellent readers always making the characters live through their words.\nSPOILERS:\nThis time we mostly follow Eshonai’s and Venli’s but we finally also get what we wanted all these years. Finally, Kaladin and Shallan face their inner demons. And even though they aren’t fully okay, Kaladin speaks his fourth oat and Shallan remembers her past. As much as I love this story, I don’t believe I would have been able to listen or read another 1000 pages without these two resolving their problems. You root for them so hard, it’s exhausting.\nI won’t spoil everything but the twist at the end left me dumbstruck! It was such an amazing finish.\nThe story follows the fused as they invade Urithiru. There is a side story for Navani and Jasna doing their own thing and we do root for Navani and her fantastic discoveries regarding light and powers, but Jasna is a side character in this story. Another main character is Witt. We finally get to know who he is and where he comes from. We also understand now that the Fused are actually from another planet in the same system and Odium just wants to get off this system and fight a holy war with some ancient Gods somewhere. A lot of things which made no sense are revealed finally. I recommend it if you have the time to listen or read it.\nHow to take smart notes Amazon.\nThis one’s review will be condensed because it would be rather lengthy otherwise. It’s basically talking about how to use the Zettelkasten system. But it does so much more then that. It challenges the way you think, the way you learn the way things are taught in school and the way you process and store information. Condensed I would say these are the main points:\n Connect new information to existing information. Information without connection isn’t worth much and will be remembered poorly or not at all. Always read with a pen in your hand and take notes about what you are reading. Always use your own words and never just copy blindly; by doing this, you will better understand what you just read. The same goes to things like, writing a blog in which you explain something you think you know. It reveals the black holes in your knowledge which you didn’t even know exist. Don’t try to group based on topics. That will result in forced connections and will leave you confined within that topic. Topics should emerge from your notes and then gathered into indexes which contain links to related notes and information. Tags are useful but don’t over do them. If you have a 1000 tags your information will be lost and hard to find because things that are unrelated will show up in the searches. So go easy on the tags Note taking is a chore. It’s not something that you just do and it just works. Good note taking requires effort. You take notes while you read then transcribe them into Zettelkasten and throw away the rest. Those are transient notes. Zettelkasten notes focus on the gist of things. On the meat!  Conclusion And that’s all for this months. Rhythm of War, the papers and the note taking book pretty much took all my time away, so not much else got done since January. But I still think this is a nice finish. Especially considering Rhythm of War was such a huge epic.\nAnd as always, Thanks for reading, Gergely.\n","title":"Reader’s Digest 2021-03","uri":"/2021/03/23/readers-digest/"},{"content":"Reader’s Digest I thought it would a cool idea if I kept a summary of the things I’ve read or listened to on a monthly basis. Here is January of 2021 so far. Enjoy.\nAll systems red - Murderbot This book is SHORT. It’s little over 3 hours of listening time. I’m listening at 1.30 so it’s just short of 2 hours. But it’s entertaining. The murderbot series is following a rouge security bot which hacked its own governor module and is self aware and free. But… it kind of hates humans and interacting with them. It just calls itself murderbot but has no intention of killing all humans. Instead, all its wants to do is basically… watch movies and various series on something called a Feed. In the first book of the series, this one, we follow Murderbot protect a few humans that it gets to short of like after an attempt on their lives.\nThe narrator, Kevin R. Free, is doing a great job of impersonating the robot and you can feel the anxiety from his words when inevitably, the robot has to interact with humans. Whether it likes it or not…\nArtificial Condition - Murderbot Another short story following our Murderbot. This time, it wants to get to the bottom of a mystery involving its past. In his past, the Murderbot, when it hacked its governor module, it went berserk and killed 80 people in the process. The company who owned it, covered it all up. Or so it thought. During its journey to find out what happened it met a new friend, Art. Art is a huge vessel capable of massive computing capacity, and happens to also love watching series. They team up and help a rag-tag group of researchers while trying to find out what happened to Murderbot and those 80 people.\nRogue Protocol Another fantastic episode of Murderbot. You can sense that the bot is slowly evolving into a more caring bot. Even though it’s saying repeatedly that it doesn’t case, it starts to care. This book was an action packed one. Lot’s of tension and fear inducing moments. Silence then all out war! We also meet Micky, who is a “pet bot” for some humans. Their relationship starts off as rocky but after a while, Murderbot comes to appreciate Micky for another free soul. There are some open question in there once the story finishes.\nThe vexed generation - Magician 2.0 Being a programmer, I followed Magician 2.0 with an interest. I listen to all of the books in the series. It was fun at first, but then started to get a bit low in quality and repetitive. I don’t really care about the people in it, and I don’t really care about what’s happening to them. And since they are effectively immortal there is no real danger to them at all. In this episode however, there was real danger at the end. And while the protagonist kids were kind of annoying, and it could have been narrated better and I felt like some of the characters weren’t fully utilized, the story kind of still was fun to read. Definitely better than the previous one about dragons.\nHeaven’s River - Bobiverse The latest addition to the Bobiverse universe. This time there is civil war amongst the Bobs and, of course, there is a new species to explore. The species was interesting, and the talk about mega-structures was also interesting, but I found that the lack of coding, and fighting and technical stuff ( what caught me on the first couple of Bob adventures ) left me wanting for more and left me with an empty feeling at the end. I really missed the proves of Bob and couldn’t care less about Starfleet.\nArmada This one was interesting. It talks about the same premise as The Last Starfighter ( there is a reference to that in there ) which I found fascinating as a child. It is about the fact that an alien race uses video games to find the best soldier for a mission to save the universe. This time though the Earth is doing the recruiting and it is against an alien invasion. And much like Ender’s Game, it turns out that the threat is real and that all fighting on the video game that our protagonist did was preparing him for this situation. The twist was pretty good too.\nPrometheus Up And Running This one has a larger post incoming.\nThat’s all for this month. Thanks for reading, Gergely.\n","title":"Reader’s Digest 2021-01","uri":"/2021/02/01/readers-digest/"},{"content":"Intro Welcome. This is a longer post about how to deploy a Go backend with a React frontend on Kubernetes as separate entities. Instead of the usual compiled together single binary Go application, we are going to separate the two. Why? Because usually a React frontend is just a “static” SPA app with very little requirements in terms of resources, while the Go backend does most of the leg work, requiring a lot more resources.\nPart two of this will contain scaling, utilization configuration, health probes, readiness probes, and how to make sure our application can run multiple instances without stepping on each other’s toes.\nNote: This isn’t going to be a Kubernetes guide. Some knowledge is assumed.\nSummary This post details a complex setup of an infrastructure with a second part coming on scaling and how to make your application scalable in the first place by doing idempotent transactions or dealing with locking and several instances of the same application not stepping on each other’s foot.\nThis, part one, details how to deploy traditional REST + Frontend based application in Go + React, but not bundled together as a single binary, instead having the backend separate from the frontend. They key in doing so is explained at the Ingress section when talking about routing specific URIs to the backend and frontend services.\nIf you are familiar with Kubernetes and infrastructure setup, feel free to skip ahead to that section. Otherwise, enjoy the drawings or the writing or both.\nTechnology The SPA app will be handled by Serve while the Go backend will use Echo. The database will be Postgres.\nWe are going to apply some best practices using Network Policies to cordon off traffic that we don’t want to go outside.\nWe will set up HTTPS using cert-manager and let’s encrypt. We’ll be using nginx as ingress provider.\nCode All, or most of the code, including the application can be found here:\nStaple. The application is a simple reading list manager with user handling, email sending and lots of database access.\nLet’s get to it then!\nKubernetes Provider Let’s start with the obvious one. Where do you would like to create your Kubernetes cluster?\nThere are four major providers now-a-days. AWS EKS, GCP GKE, Azure AKS and DigitalOcean DKE. Personally, I prefer DO because, it’s a lot cheaper than the others. The downside is that DO only provides ReadWriteOnce persistent volumes. This gets to be a problem when we are trying to update and the new Pod can’t mount the volume because it’s already taken by the existing one. This can be solved by a good ol NFS instance. But that’s another story.\nAWS' was late to the party and their solution is quite fragile and the API is terrible. GCP is best in terms of technicalities, api, handling, and updates. Azure is surprisingly good, however, the documentation is most of the times out of date or even plain incorrect at some places.\nSetup Basics To setup your Kubernetes instance, follow DigitalOcean’s Kubernetes Getting Started guide. It’s really simple. When you have access to the cluster via kubectl I highly recommend using this tool: k9s.\nIt’s a flexible and quite handy tool for quick observations, logs, shells to pods, edits and generally following what’s happening to your cluster.\nNow that we are all set with our own little cluster, it’s time to have some people move in. First, we are going to install cert-manager.\nNote: I’m not going to use Helm because I think it’s unnecessary in this setting. We aren’t going to install these things in a highly configurable way and updating with helm is a pain in the butt. For example, for cert-manager the update with helm takes several steps, whilst updating with a plain yaml file is just applying the next version of the yaml file.\nI’m not going to explain how to install cert-manager or nginx. I’ll link to their respective guides because frankly, they are simple to follow and work out of the box.\nTo install nginx, simply apply the yaml file located here: DigitalOcean Nginx.\nTo install cert-manager follow this guide: https://cert-manager.io/docs/installation/kubernetes/. Follow the regular manifest install part, then ignore the Helm part and proceed with verification and then install your issuer. I used a simple ACME/http01 issuer from here: https://cert-manager.io/docs/configuration/acme/http01/\nNote: That acme configuration contains the staging url. This is to test that things are working. Once you are sure that everything is wired up correctly, switch that url to this one: https://acme-v02.api.letsencrypt.org/directory -\u003e prod url. For example:\napiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-prod spec: acme: # The ACME server URL server: https://acme-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: your@email.com # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-prod # Enable the HTTP-01 challenge provider solvers: - http01: ingress: class: nginx Note: I’m using a ClusterIssuer because I have multiple domains and multiple namespaces.\nThat’s it. Cert-manager and nginx should be up and running. Later on, we will create our own ingress rules.\nDomain Next, you’ll need a domain to bind too. There are a gazillion domain providers out there like no-ip, GoDaddy, HostGator, Shopify and so on. Choose one which is available to you or has the best prices.\nThere are some good guides on how to choose a domain and where to create it. For example: 5 things to watch out for when buying a domain.\nThe application Alright, let’s put together the application.\nStructure Every piece of our infrastructure will be laid out in yaml files. I believe in infrastructure as code. If you run a command you will most likely forget about it, unless it’s logged and / or is replayable.\nThis is the structure I’m using:\n. ├── LICENSE ├── README.md ├── certificate_request │ └── certificate_request.yml ├── configmaps │ └── staple_initdb_script.yaml ├── database │ ├── staple_db_deployment.yaml │ ├── staple_db_network_policy.yaml │ ├── staple_db_pvc.yaml │ └── staple_db_service.yaml ├── namespace │ └── staple_namespace.yaml ├── primer.sql ├── rbac ├── secrets │ ├── staple_db_password.yaml │ └── staple_mg_creds.yaml ├── staple-backend │ ├── staple_deployment.yaml │ └── staple_service.yaml └── staple-frontend ├── staple_deployment.yaml └── staple_service.yaml One other possible combination is, if you have multiple applications:\n. ├── README.md ├── applications │ ├── confluence │ │ ├── db │ │ │ ├── db_deployment.yaml │ │ │ └── db_service.yaml │ │ ├── deployment │ │ │ └── deployment.yaml │ │ ├── pvc │ │ │ └── confluence_app_pvc.yaml │ │ └── service │ │ └── service.yaml │ ├── gitea │ │ ├── config │ │ │ ├── app.ini │ │ │ └── gitea_config_map.yaml │ │ ├── db │ │ │ ├── gitea_db_deployment.yaml │ │ │ ├── gitea_db_network_policy.yaml │ │ │ ├── gitea_db_pvc.yaml │ │ │ └── gitea_db_service.yaml │ │ ├── deployment │ │ │ └── gitea_deployment.yaml │ │ ├── pvc │ │ │ └── gitea_app_pvc.yaml │ │ └── service │ │ └── gitea_service.yaml ├── cronjobs │ ├── cronjob1 │ │ ├── Dockerfile │ │ ├── README.md │ │ ├── go.mod │ │ ├── go.sum │ │ ├── cron.yaml │ │ └── main.go ├── ingress │ ├── example1 │ │ ├── example1_ingress_resource.yaml │ │ └── gitea_ssh_configmap.yaml │ ├── example2 │ │ └── example2_ingress_resource.yaml │ ├── lets-encrypt-issuer.yaml │ └── nginx │ ├── nginx-ingress-controller-deployment.yaml │ └── nginx-ingress-controller-service.yaml └── namespaces ├── example1_namespace.yaml ├── example2_namespace.yaml Namespace Before we begin, we’ll create a namespace for our application to properly partition all our entities.\nTo create a namespace we’ll use this yaml example_namespace.yaml:\napiVersion: v1 kind: Namespace metadata: name: example Apply this with kubectl -f apply example_namespace.yaml.\nThe Database Deploying a Postgres database on Kubernetes is actually really easy. You need five things to have a basic, but relatively secure installation.\nSecret The secret contains our password and our database user. In postgres, if you define a user using POSTGRES_USER postgres will create the user and a database with the user’s name. This could come from Vault too, but the Kubernetes secret is usually enough since it should be a closed environment anyways. But for important information I would definitely use an admission policy and some vault secret goodness. (Maybe another post?)\nOur secret looks like this: database_secret.yaml\napiVersion: v1 kind: Secret metadata: name: staple-db-password namespace: staple data: POSTGRES_PASSWORD: cGFzc3dvcmQxMjM= # This creates a user and a db with the same name. POSTGRES_USER: c3RhcGxl To generate the base64 code for a password and a user, use:\necho -n \"password123\" | base64 echo -n \"username\" | base64 …and paste the result in the respective fields. Once done, apply with kubectl -f apply database_secret.yaml.\nDeployment The deployment which configures our database. Looks something like this (database_deployment.yaml):\napiVersion: apps/v1 kind: Deployment metadata: namespace: staple name: staple-db spec: replicas: 1 selector: matchLabels: app: staple-db template: metadata: name: staple-db labels: app: staple-db spec: containers: - name: postgres image: postgres:11 env: - name: POSTGRES_USER value: staple - name: POSTGRES_PASSWORD valueFrom: secretKeyRef: name: staple-db-password key: POSTGRES_PASSWORD volumeMounts: - mountPath: /var/lib/postgresql/data subPath: data # important so it gets mounted correctly name: staple-db-data - mountPath: /docker-entrypoint-initdb.d/staple_initdb.sql subPath: staple_initdb.sql name: bootstrap-script volumes: - name: staple-db-data persistentVolumeClaim: claimName: do-storage-staple-db - name: bootstrap-script configMap: name: staple-initdb-script Note the two volume mounts.\nThe first one makes sure that our data isn’t lost when the database pod itself restarts. It creates a mount to a persistent volume which is defined a few lines below by persistentVolumeClaim. subPath is important in this case otherwise you’ll end up with a lost\u0026found folder.\nThe second mount is a postgres specific initialization file. Postgres will run that sql file when it starts up. I’m using it to create my application’s schema.\ncreate database staples; create table users (email varchar(255), password text, confirm_code text, max_staples int); create table staples (name varchar(255), id serial, content text, created_at timestamp, archived bool, user_email varchar(255)); And it comes from a configmap which looks like this:\napiVersion: v1 kind: ConfigMap metadata: name: staple-initdb-script namespace: staple labels: app: staple data: staple_initdb.sql: create table users (email varchar(255), password text, confirm_code text, max_staples int); create table staples (name varchar(255), id serial, content text, created_at timestamp, archived bool, user_email varchar(255)); Network Policy Network policies are important if you value your privacy. They restrict a PODs communication to a certain namespace OR even to between applications only. By default I like to deny all traffic and then slowly open the valve until everything works.\nKudos if you know who this is. (mind my terrible drawing capabilities)\nWe’ll use a basic network policy which will restrict the DB to talk to anything BUT the backend. Nothing else will be able to talk to this Pod.\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: staple-db-network-policy namespace: staple spec: podSelector: matchLabels: app: staple-db policyTypes: - Ingress - Egress ingress: - from: - podSelector: matchLabels: app: staple ports: - protocol: TCP port: 5432 egress: - to: - podSelector: matchLabels: app: staple ports: - protocol: TCP port: 5432 The important bit here is the podSelector part. The label will be the label used by the application deployment. This will restrict the Pod’s incoming and outgoing traffic to that of the application Pod including denying internet traffic.\nPVC The persistent volume claim definition is straight forward:\napiVersion: v1 kind: PersistentVolumeClaim metadata: namespace: staple name: do-storage-staple-db spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: do-block-storage 10 gigs should be enough anything.\nService The service will expose the database deployment to our cluster.\nOur service is fairly basic:\nkind: Service apiVersion: v1 metadata: namespace: staple name: staple-db-service spec: ports: - port: 5432 selector: app: staple-db clusterIP: None That’s done with the database. Next up is the backend.\nThe backend The backend itself is written in a way that it doesn’t require a persistent storage so we can skip that part. It only needs three pieces. A secret, a deployment definition and the service exposing the deployment.\nSecret First, we create a secret which contains Mailgun credentials.\napiVersion: v1 kind: Secret metadata: name: staple-mg-creds namespace: staple data: MG_DOMAIN: cGFzc3dvcmQxMjM= MG_API_KEY: cGFzc3dvcmQxMjM= Database connection The connection settings are handled through the same secret which is used to spin up the DB itself. We have to only mount that here too and we are good.\nDeployment Which brings us to the deployment. This is a bit more involved.\napiVersion: apps/v1 kind: Deployment metadata: namespace: staple name: staple-app labels: app: staple spec: replicas: 1 selector: matchLabels: app: staple template: metadata: labels: app: staple app.kubernetes.io/name: staple app.kubernetes.io/instance: staple spec: containers: - name: staple image: skarlso/staple:v0.1.0 imagePullPolicy: IfNotPresent resources: requests: memory: \"500Mi\" cpu: \"250m\" limits: memory: \"1000Mi\" cpu: \"500m\" env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: DB_PASSWORD valueFrom: secretKeyRef: name: staple-db-password key: POSTGRES_PASSWORD - name: MG_DOMAIN valueFrom: secretKeyRef: name: staple-mg-creds key: MG_DOMAIN - name: MG_API_KEY valueFrom: secretKeyRef: name: staple-mg-creds key: MG_API_KEY args: - --staple-db-hostname=staple-db-service.cronohub.svc.cluster.local:5432 - --staple-db-username=staple - --staple-db-database=staple - --staple-db-password=$(DB_PASSWORD) - --mg-domain=$(MG_DOMAIN) - --mg-api-key=$(MG_API_KEY) ports: - name: staple-port containerPort: 9998 There are a few important points here and I won’t explain them all, like the resource restrictions, which you should be familiar with by now. I’m using a mix of 12factor app’s environment configuration and command line arguments for the application configuration. The app itself is not using os.Environ but the args.\nThe args point to the cluster local dns of the database, some db settings, and the mailgun credentials.\nIt also exposes the container port 9998 which is Echo’s default port.\nNow all we need is the service.\nService Without much fanfare:\nkind: Service apiVersion: v1 metadata: namespace: staple name: staple-service labels: app: staple app.kubernetes.io/name: staple app.kubernetes.io/instance: staple spec: selector: app: staple app.kubernetes.io/name: staple app.kubernetes.io/instance: staple ports: - name: service-port port: 9998 targetPort: staple-port And with this, the backend is done.\nThe frontend The frontend, similarly to the backend, does not require a persistent volume. We can skip that one too.\nIn fact it only needs two things, a deployment and a service, and that’s all. It uses serve to host the static files. Honestly, that could also be a Go application serving the static content or anything that can serve static files.\nDeployment apiVersion: apps/v1 kind: Deployment metadata: namespace: staple name: staple-frontend labels: app: staple-frontend spec: replicas: 1 selector: matchLabels: app: staple-frontend template: metadata: labels: app: staple-frontend app.kubernetes.io/name: staple-frontend app.kubernetes.io/instance: staple-frontend spec: containers: - name: staple-frontend image: skarlso/staple-frontend:v0.0.9 imagePullPolicy: IfNotPresent resources: requests: memory: \"500Mi\" cpu: \"250m\" limits: memory: \"1000Mi\" cpu: \"500m\" env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: REACT_APP_STAPLE_DEV_HOST value: \"\" ports: - name: staple-front containerPort: 5000 Service And the service:\nkind: Service apiVersion: v1 metadata: namespace: staple name: staple-front-service labels: app: staple-frontend app.kubernetes.io/name: staple-frontend app.kubernetes.io/instance: staple-frontend spec: selector: app: staple-frontend app.kubernetes.io/name: staple-frontend app.kubernetes.io/instance: staple-frontend ports: - name: staple-front port: 5000 targetPort: staple-front And with that the backend and frontend are wired together and ready to receive traffic.\nAll pods should be up and running without problems at this point. If you have any trouble deploying things, please don’t hesitate to leave a question in the comments.\nIngress Fantastic. Now, our application is running. We just need to expose it and route traffic to it. The backend has the api route /rest/api/v1/. The frontend has the route syntax /login, /register and a bunch of others. The key here is that all of them are under the same domain name but based on the URI we need to direct one request to the backend the other to the frontend.\nThis is done via nginx’s routing logic using regex. In an nginx config this would be the location part. It’s imperative that the order of the routing is from more specific towards more general Because we need to catch the specific URIs first.\nIngress Resource To do this, we will create something called an Ingress Resource. Note that this is Nginx’s ingress resource and not Kubernetes'. There is a difference.\nI suggest reading up on that link about the ingress resource because it reads quite well and will explain how it works and fits into the Kubernetes environment.\nGot it? Good. We’ll create one for staple.app domain:\napiVersion: extensions/v1beta1 kind: Ingress metadata: namespace: staple name: staple-app-ingress annotations: kubernetes.io/ingress.class: \"nginx\" cert-manager.io/cluster-issuer: \"letsencrypt-prod\" cert-manager.io/acme-challenge-type: http01 nginx.ingress.kubernetes.io/rewrite-target: /$1 # this is important spec: tls: - hosts: - staple.app secretName: staple-app-tls rules: - host: staple.app http: paths: - backend: serviceName: staple-service servicePort: ss-port # 9998 path: /(rest/api/1.*) - host: staple.app http: paths: - backend: serviceName: staple-front-service servicePort: sfs-port # 5000 path: /(.*) Let’s take a look at what’s going on here. The first thing to catch the eye are the annotations. These are configuration settings for nginx, cert-manager and Kubernetes. We have the cluster issuer’s name. The challenge type, which we decided should be http01, and the most important part, the rewrite-target setting. This will use the first capture group as a base after the host.\nWith this rewrite rule in place, the paths values need to provide a capture group. The first in line will see everything that goes to the urls like: staple.app/rest/api/1/token, staple.app/rest/api/1/staples, staple.app/rest/api/1/user, etc. The first part of the url is the host staple.app, second part is /(rest/api/1/.*) for which the result is that group number one ($1) will be rest/api/1/token. Nginx now sees that we have a backend route for that and will send this URI along to the service. Our service picks it up and will match that URI to the router configuration.\nIf there is a request like, staple.app/login, which is our frontend’s job to pick up, the first rule will not catch it because the regex isn’t matching, so it falls through to the second one, which is the frontend service that is using a “catch all” regex. Like ip tables, we go from specific to more general.\nEnding words And that’s it. If everything works correctly, then the certificate service wired up the https certs and we should be able ping the rest endpoint under https://staple.app/rest/api/1/token and log in to the app in the browser using https://staple.app.\nStay tuned for the second part where we’ll scale the thing up!\nThanks for reading! Gergely.\n","title":"How to deploy a Go (Golang) backend with a React frontend separately on Kubernetes - Part One","uri":"/2020/07/23/kubernetes-deploy-golang-react-apps-separately-part1/"},{"content":"Intro Hi folks.\nThis time, I would like to talk a little bit about code reviews.\nHow do you do code reviews? Don’t hesitate to share it in the comments.\nHow do I do code reviews? Well read on if you would like to know.\nThe Top Down approach If I’m dealing with a small code change, a couple of lines here and there in the odd file first, I’ll try to understand why the review is there? What was it trying to achieve? What’s the goal of the change? Is there a ticket/issue I can read for background info? Or an RFC?\nUnderstanding the goal of the change will let you know how to read the change. I usually also scribble down some notes and my expectations to see if the change meets them or does something completely different. And if it’s different, maybe my expectations were wrong.\nIn any case, I will have a framework to start with. It’s important to understand why the change is there in the first place. I cannot stress this enough.\nLogical follow If the change is large, the top down approach will simply not work. You will loose track of why the change is and your logical big picture image will fade into nothingness after a hundred lines.\nIn Github at least, what I would do to approach this, is close all views and just have a general sense first how big the change is, and what files changed (after I understand why the change is there and what is it trying to change and / or solve). Once I have a feel for the structure I would look for changes which are trivial. For example parameter changes of a function. I would expect that in that case there will be a lot of changes at places where that function was called. I would review those and then go on.\nIf there is any, I would look for an entry point into the change. Is there a new handler? A new API? A new method? Did an API change? If so, did that change propagate all the way through the API’s implementation?\nIf it’s a huge number of deletes, I would look for the deleted code in the whole codebase. Did they miss something? Was that code referenced in another section of the code or possibly in another service? In that case, do a search on the whole organization on all repositories if you believe that that makes sense.\nIf it’s concurrent code… are they syncing it up at some point? Are they releasing the lock? Is the lock happening at the right place? In Go for example, you can get a lock and then defer w.Lock.Unlock() it. This makes for a convenient way of “forgetting” about the lock acquire. But this is counterproductive in some cases.\nImagine you have a function which acquires the lock in the begin. Then does a for loop which takes a couple of seconds but doesn’t actually use the map or the value you were trying to protect. In that case you are taking the lock but aren’t actually using it. There was no point in acquiring it at the beginning of the function.\nGeneral order There are a LOT of things one can review in a PR. Minute things and a myriad if small to big logical problems and ramifications. It’s not possible to list them all. So here are some general rules I would follow:\nSyntax The first thing I would do is look through the syntax and follow this mnemonic: BUD. B(ottleneck), U(nnecessary code), D(uplicate work). Spotting these is usually easy but it can happen that the change is subtle. Bottlenecks are often embedded loops in loops or a very sneaky recursion. Unnecessary code is sometimes harder to spot. This is duplicate code which could be extracted. It can be subtle because it’s likely that only a small thing changes and at first glance it’s not trivial how to extract the rest of the code around that small thing. Maybe it can be a function (if your language supports functions as first class citizens) which could close over a value and change it multiple times.\nDuplicate work is when a loop is calculating something over and over but it’s actually the same thing or we already have that information and it’s not likely that it would change so it can be reused. These kind of problems are solved through caching or simply just do it once, store the result, then pass it around. Candidates for this could be multiple calls to the same api for the same information which didn’t change in between.\nGeneral language guidelines General language syntax and guidelines adherence comes next. In Go this is trivial, since we have a plethora of tools available to us, devs, in the form of static analyzers like, fmt, golint, goimport etc. But in the absence there is usually a good guide at hand how a language is supposed to look like.\nWorkplace / Project guideline adherence This could arguably come before the general adherence. Whichever suits you better. Or maybe your workplace / environment the code is in (this could also be an open source project) is different from the general guidelines. That is okay, as long as it’s sensible. You could try changing it if you think it’s too far from how a language is supposed to look like but that usually doesn’t work. Especially if the in-place guidelines are already there for years.\nGenerally though, it’s better to follow whatever style/code/whim the current environment is doing. If changing something always look around how that looks like in the code you are working in and then follow that style. These could be things like, variable naming, comment semantics, logical flow of the code, structuring (like where the code should go and how it should look like (yes, look like(sometime aesthetics matter))).\nCould it be done concurrently As a cherry on top, I would try to determine if the work that is being done, could be done in a thread / go routine. In Go, go routines are cheap and very easy to make. It’s also easy to abuse them of course, but it doesn’t hurt to think asynchronously. Especially in a distributed environment. Which brings me to the next point.\nIn a distributed environment timing is key If this change is in an environment which has many services and is generally distributed your first though should immediately be, how those this affect the rest of the services and what timing issues could arise. If there is a delete operation, what about another service calling a create or a get on the same resource at the same time? What if it’s a create but another service also calls create with the same values? Is the data eventually consistent or strongly consistent? How does that affect the runtime? Is the change in a frequently called code segment which is usually under heavy load? Did the change change the way that is handled? Did it slow it down or speed it up? Did it trade the slowdown for strong consistency? Is strong consistency really needed in that service which would justify the slowdown?\nLike I said… a myriad of things…\nI’ll stop here for now.\nConclusion I hope this made sense. If you disagree with this approach or have a different guideline of reviewing, please don’t hesitate it to share it!\nAs always, Thanks for reading! Gergely.\n","title":"How to do a good code review","uri":"/2020/05/11/good-code-reviews/"},{"content":"Intro Hi folks.\nToday I would like to share a quick “fix” for a problem I’ve seen popping up here and there.\nThat is, if you have a react frontend which is a SPA app but you still want refresh to work. What do I mean by that? Consider the following…\nThe problem You have a SPA app with a react router which navigates the user around. The app calls to a backend api which serves content of some kind. You have the following routes…. login, signup, reset, archive.\nIf your app is compiled with your backend, as it usually is, then something like: https://app.com/login will not work unless it’s also defined on the backend serving some content.\nSo but what should the content be in this case?\nThe structre For that, let’s first look at the strucute of the app. Consider the following directory tree:\n. ├── Dockerfile ├── LICENSE ├── Makefile ├── README.md ├── build ├── cmd │ └── root.go ├── frontend │ ├── LICENSE │ ├── README.md │ ├── build │ ├── package-lock.json │ ├── package.json │ ├── public │ ├── src │ └── yarn.lock ├── go.mod ├── go.sum ├── img ├── internal └── pkg For this, the frontend contains a build dir in which the generated react frontend static files plus compiled JavaScript libraries are. In this directory there also is a index.html file which does the actual heavy lifting in terms of routing.\nThe Go backend therefor must only route to index.html on certain endpoints.\nIn Go to build and deploy a single binary containing the static assets here in, you can use something like go.rice or assetfs which generate a Go file for you which contains all the data in an easily accessible way.\nI’ll be using go.rice.\nThe solution To summarize, all you have to do is route every route in your router.js file to index.html in Go. But how? Well, like this…\nConsider this appliction: Staple. This is a react frontend go backend application which builds a frontend asset then packages it up with go.rice, builds a Docker container and deploys the whole thing to a Kubernetes cluster. But this is the interesting part which handles the index routing:\nIn routes.go (contains the mapped routes from under Router.js):\npackage pkg // These routes must match the routes under frontend/Routes.js var routes = []string{ \"/login\", \"/archive\", \"/staples/new\", \"/reset\", \"/signup\", \"/settings\", } Once we have a list of routes to map…\nIn server.go (which is starting up the server and generates the handlers…)\n// ... code which sets up the api routes... after every handler has been estabilished... \t// Setup front-end if not in production mode. \tif !config.Opts.DevMode { // This path needs to be relative from this files package's location. \tstaticAssets, err := rice.FindBox(\"../frontend/build\") if err != nil { log.Fatal(\"Cannot find assets in production\") return err } // Register handler for static assets  assetHandler := http.FileServer(staticAssets.HTTPBox()) // Open the index.html file as a *File for reading the content out of it.  // This is a virtual file handled by go.rice. \tindex, err := staticAssets.Open(\"index.html\") if err != nil { config.Opts.Logger.Error().Err(err).Msg(\"Failed to find index.html content.\") return err } // Set up the main point as a static file server \te.GET(\"/\", echo.WrapHandler(assetHandler)) // Set up routes to index.html for all routes under Routes.js. Index.html will handle the routing any further. \tfor _, r := range routes { e.GET(r, indexServer(r, index)) } e.GET(\"/favicon.ico\", echo.WrapHandler(assetHandler)) e.GET(\"/site.webmanifest\", echo.WrapHandler(assetHandler)) e.GET(\"/static/css/*\", echo.WrapHandler(http.StripPrefix(\"/\", assetHandler))) e.GET(\"/static/js/*\", echo.WrapHandler(http.StripPrefix(\"/\", assetHandler))) e.GET(\"/static/media/*\", echo.WrapHandler(http.StripPrefix(\"/\", assetHandler))) } What is indexServer in this you might ask? Well, fret no longer, I shall show you:\n// indexServer takes a name and the contents of the virtual file index.html gathered up by go.rice // and serves its content via http.ServeContent under the given name. func indexServer(name string, file *rice.File) echo.HandlerFunc { return func(c echo.Context) error { stat, _ := file.Stat() http.ServeContent(c.Response().Writer, c.Request(), name, stat.ModTime(), file) return nil } } The key points are the name, which will be the route and the file which is the index.html content which contains the logic to route based on the request. All that will be handled. And if a new route comes along, simple add it to the list, recompile and you are done!\nConclusion In summary, you let your index.html file handle the routing as you would normally do. Just you need to make your backend aware of that fact. Now refreshing the page will work as you’d expect.\nThank you for reading, Gergely.\n","title":"How to Make SPA refresh work with a Go backend","uri":"/2020/02/17/making-spa-refresh-work-with-go-backend/"},{"content":"Intro Hi folks.\nThis is a continuation of the previous post about my Kubernetes infrastructure located here. The two remaining points are to deploy Athens Go proxy and setting up monitoring.\nAthens Let’s start with Athens.\nFirst of all if you are a helm user, Athens has an awesome set of helm charts which you can use to deploy it in your cluster. Located here.\nI prefer to deploy my own config files, but that’s me. So here is my preferred way of deploying Athens.\nSince this is also a subdomain of the previously created powerhouse namespace we are going to use that.\nPVC We are going to need a PersistentVolumeClaim for Athens so it can store all the things forever.\napiVersion: v1 kind: PersistentVolumeClaim metadata: namespace: powerhouse name: athens-storage spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: do-block-storage Claim is very boring. Which means it just works.\nDeployment This is more interesting. Athens provides a lot of possibilities for the deployment. I’m just deploying the barest possible here. Which means, no user auth, no private repository support, ssh key configuration, etc… It’s a plain proxy installation.\napiVersion: apps/v1 kind: Deployment metadata: namespace: powerhouse name: athens-app labels: app: athens-proxy spec: replicas: 1 selector: matchLabels: app: athens-proxy template: metadata: labels: app: athens-proxy app.kubernetes.io/name: athens-proxy app.kubernetes.io/instance: athens-proxy spec: containers: - name: athens-proxy image: gomods/athens:v0.6.0 livenessProbe: httpGet: path: \"/healthz\" port: 3000 readinessProbe: httpGet: path: \"/readyz\" port: 3000 env: - name: ATHENS_GOGET_WORKERS value: \"3\" - name: ATHENS_STORAGE_TYPE value: \"disk\" - name: ATHENS_DISK_STORAGE_ROOT value: /var/lib/athens ports: - containerPort: 3000 name: athens-http volumeMounts: - name: athens-data mountPath: /var/lib/athens subPath: athens volumes: - name: athens-data persistentVolumeClaim: claimName: athens-storage Fun fact. The name of the app must not be just plain athens because that will result in an error: too many colons in address.\nThe issue is here: https://github.com/gomods/athens/issues/1038#issuecomment-457145658 Basically it’s because of the name used for the environment properties inside the container.\nService Now, let’s expose it.\nkind: Service apiVersion: v1 metadata: namespace: powerhouse name: athens-service labels: app: athens-proxy app.kubernetes.io/name: athens-proxy app.kubernetes.io/instance: athens-proxy spec: selector: app: athens-proxy app.kubernetes.io/name: athens-proxy app.kubernetes.io/instance: athens-proxy ports: - port: 80 targetPort: 3000 Ingress I’m using port 80 here because it’s convenient. But if you use any other port, don’t forget to alter your ingress to forward to that port and service.\napiVersion: extensions/v1beta1 kind: Ingress ... spec: tls: ... - hosts: - athens.powerhouse.com secretName: athens-cronohub-tls rules: ... - host: athens.powerhouse.com http: paths: - backend: serviceName: athens-service servicePort: 1234 path: / And that’s it! If you now visit https://athens.powerhouse.com it should say \"Welcome to The Athens Proxy\".\nNow, if you set this proxy with export GOPROXY=https://athens.powerhouse.com it should start to cache modules. It’s a fantastic proxy with a lot of capabilities. I encourage you to check it out and drop by it’s slack channel on Gopher slack called Athens.\nMonitoring Monitoring is a huge topic so I’m not going to talk about how to monitor or what. That is described in great many of posts. I especially recommend reading sysdig’s 6 part post on doing monitoring with Prometheus and Grafana and what to monitor and the four golden signals and whatnot. Starting here and here.\nPrometheus I’m going to deploy Prometheus. Prometheus is a monitoring tool which sits inside your cluster and gathers data about running pods, nodes, services, whatever you expose and wants to send data to it. It can also alert on things and can be integrated with tools like Graphana for nice front-end and metrics. Prometheus itself uses PromQL as its query language to gather data from different sources and do time series analytics and much much more.\nPlease visit the website and documentation for more details. It’s the defacto monitoring tool for Kubernetes. Again, I’m going to do a very basic installation of Prometheus. So basic in fact, that I don’t even have a PVC for it, because I don’t care at this point about retaining data.\nNamespace Let’s create it’s own namespace.\napiVersion: v1 kind: Namespace metadata: name: monitoring labels: name: monitoring Config Prometheus Server config is massive. I don’t expect you to pick up on everything in this thing, but I would encourage you to at least try to find out what these setting do… Our config yaml file contains the configuration file for Prometheus which we’ll later set up via a command line argument. It’s called prometheus.yml.\napiVersion: v1 kind: ConfigMap metadata: name: prometheus-server-conf labels: name: prometheus-server-conf namespace: monitoring data: prometheus.yml: |-global: scrape_interval: 5s evaluation_interval: 5s scrape_configs: - job_name: 'kubernetes-apiservers' kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: 'kubernetes-nodes' scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics - job_name: 'kubernetes-pods' kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name - job_name: 'kubernetes-cadvisor' scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor - job_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name Mostly it’s just setting up what Prometheus should monitor and how. The important bits are the labels. How this is going to work is, that we will annotate the resources we want Prometheus to see. Which is pretty cool. Basically we will just alter a pod to include an annotation and it will begin monitoring it. No need to install anything anywhere or restart anything. Just add an annotation and bamm, you’re done.\nRBAC Prometheus needs permissions to access resources in the cluster such as API end-points and gathering data about the cluster itself. We will provide it with this permission through Role Based Access Control.\nWe’ll create a service account which Prometheus can use. We want it to access the whole cluster so we’ll use a ClusterRole.\napiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: prometheus rules: - apiGroups: [\"\"] resources: - nodes - nodes/proxy - services - endpoints - pods verbs: [\"get\", \"list\", \"watch\"] - apiGroups: - extensions resources: - ingresses verbs: [\"get\", \"list\", \"watch\"] - nonResourceURLs: [\"/metrics\"] verbs: [\"get\"] --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: prometheus roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheus subjects: - kind: ServiceAccount name: default namespace: monitoring This will give access to monitor the following resources: nodes, nodes/proxy, services, endpoints and pods. The action are get, list, watch. No modifications.\nWe’ll also allow Prometheus to watch ingresses for data traffic and allow it to do get requests to non-resource endpoint /metrics.\nDeployment Now, the deployment is actually pretty easy.\napiVersion: extensions/v1beta1 kind: Deployment metadata: name: prometheus-deployment namespace: monitoring spec: replicas: 1 template: metadata: labels: app: prometheus-server spec: containers: - name: prometheus image: prom/prometheus:v2.2.1 args: - \"--config.file=/etc/prometheus/prometheus.yml\" - \"--storage.tsdb.path=/prometheus/\" ports: - containerPort: 9090 volumeMounts: - name: prometheus-config-volume mountPath: /etc/prometheus/ - name: prometheus-storage-volume mountPath: /prometheus/ volumes: - name: prometheus-config-volume configMap: defaultMode: 420 name: prometheus-server-conf - name: prometheus-storage-volume emptyDir: {} The two interesting things here are the two arguments. The config file, which we include through the configMap and the storage. Which I’m not bind mounting.\nService Let’s expose Prometheus. Now, this may come as a surprise if you don’t know anything about Prometheus, but this is an in cluster monitoring tool. It’s usually not supposed to be accessed directly, but through tools like Graphana or used by tools like Alerting or traefik as a reverse proxy. As such, Prometheus does not support authentication or authorization or user management of any kind. That is usually taken care of by a reverse proxy or other means written about here and here.\nAs such, we can do a number of things. We can expose it as a NodePort service for example:\napiVersion: v1 kind: Service metadata: name: prometheus-service namespace: monitoring annotations: prometheus.io/scrape: 'true' prometheus.io/port: '9090' spec: selector: app: prometheus-server type: NodePort ports: - port: 8080 targetPort: 9090 nodePort: 30000 Or we just port forward the pod like this:\nk port-forward pods/prometheus-deployment-6bf45557bd-qc6t6 9090:9090 -n monitoring And access it by simply opening the url: http://127.0.0.1:9090.\nPrometheus Once you open it, you should see something like this, after running a small query:\nAdding in Resources to monitor In order to add a resource to monitor simply insert these annotations:\nannotations: prometheus.io/scrape: 'true' prometheus.io/port: '9090' Done.\nBonus Round – Graphana We deployed Athens and Prometheus to monitor our cluster. We don’t have anything before Prometheus that would be fancy, but installing Graphana is actually pretty easy. You can follow the instructions here.\nA very easy way of looking at some nice metrics without worrying about anything like users and such, is running a Graphana instance in docker on your local machine with:\ndocker run -d -p 3000:3000 grafana/grafana … and while you are forwarding the Prometheus end-point you navigate to your Graphana instance by opening 127.0.0.1:3000 and install a Prometheus data-point like this:\nAfter that navigate to a new dashboard and select a simple PromQL metric to see if it’s working. You should see something like this:\nNow you can create a new dashboard add a PVC to our Prometheus instance and enjoy all the metrics you can store!\nConclusion And this is it folks. Everything is installed and we can monitor things now. If you give Prometheus a PVC you can build some pretty awesome time series graphs too and see how your cluster behaves over time.\nThank you for reading! Gergely.\n","title":"Using a Kubernetes based Cluster for Various Services with auto HTTPS - Part 2","uri":"/2019/10/15/kubernetes-cluster-part2/"},{"content":"Intro Hi folks.\nSo there is this workshop from Dave Cheney.\nAnd I thought I’d draw a sort of summary of that workshop.\nRight-click-\u003eOpen Image for higher resolution.\nCheers, Gergely.\n","title":"Summary of Practical Go workshop from Dave Cheney","uri":"/2019/10/10/practical-go-summary/"},{"content":"Intro One morning I woke up and tried to access my gitea just to find that it wasn’t running.\nI checked my cluster and found that the whole thing was dead as meat. I quickly jumped in and ran k get pods -A to see what’s going on. None of my services worked.\nWhat immediately struck my eye was a 100+ pods of my fork_updater cronjob. The fork_updater cronjob which runs once a month, looks like this:\napiVersion: batch/v1beta1 kind: CronJob metadata: name: fork-updater namespace: fork-updater spec: schedule: \"* * 1 * *\" jobTemplate: spec: template: spec: volumes: - name: fork-updater-ssh-key secret: secretName: fork-updater-ssh-key defaultMode: 256 # yaml spec does not support octal mode containers: - name: fork-updater imagePullPolicy: IfNotPresent image: skarlso/repo-updater:1.0.4 env: - name: GIT_TOKEN valueFrom: secretKeyRef: name: fork-updater-secret key: GIT_TOKEN volumeMounts: - name: fork-updater-ssh-key mountPath: \"/etc/secret\" readOnly: true restartPolicy: OnFailure Inherently there is nothing wrong with this at first glance. But on a second glance, the problem is restartPolicy: Always. For whatever the reason, the cronjob died when it started up. The restart policy then… restarted the cronjob, which failed again really fast. Then it scheduled a new one and a new one and a new one… and I had 100+ containers pending and running and creating.\nAt that point the cluster was basically DDOSd into oblivion. Once the other resources started to die ( since this was a private cluster and I didn’t bother to set up restrictions on resources ) the cronjob hogged even more and it basically blocked everything else from being able to run. It overwhelmed the scheduler.\nLovevly that.\nThis is how you could potentionally kill a cluster which doesn’t have any resource limits and restrictions set up.\nGergely.\n","title":"How I killed my entire Kubernetes cluster","uri":"/2019/10/01/killing-kubernetes-cluster/"},{"content":"Intro Hi folks.\nToday, I would like to show you how my infrastructure is deployed and managed. Spoiler alert, I’m using Kubernetes to do that.\nI know… What a twist!\nLet’s get to it.\nWhat What services am I running exactly? Here is a list I’m running at the time of this writing:\n Athens Go Proxy Gitea The Lounge (IRC bouncer) Two CronJobs  Fork Updater IDLE RPG online checker   My WebSite (gergelybrautigam.com) Monitoring  And it’s really simple to add more.\nWhere My cluster is deployed at DigitalOcean using two droplets each 1vCPU and 2GB RAM.\nWhat Not This isn’t going to be a production grade cluster. What I don’t include in here:\nRBAC for various services and users Since I’m the only user of my cluster I didn’t create any kind of access limits / users or such. You are free to create them though. The only role based auth that’s going on is for Prometheus.\nI’m not using any third party things which require access to the API.\nResource limitation I’m the sole user of my things. I’m not really scaling my gitea up or down based on usage and as such, I did not define things like:\n Resource limits Nodes with certain capabilities Affinities and Taints – which means, everything can run anywhere  Readiness Liveliness Most of by deploys and services don’t have these except for Athens.\nHow Okay, with that out of the way, let’s get into the hows of things…\nBeginning The most important thing that you need to do in order to use Kubernetes is Containerizing all the things.\nSince Kubernetes is a container orchestration tool, without containers it’s pretty useless.\nAs a driver, I’m going to use Docker. Kubernetes can use anything that’s OCI compatible, which means if you would like to use runc as a container engine, you can do that. I’d like to keep my sanity though.\nExample To show you what I mean… I have a cronjob which is running every month. It gathers all my forks on github and updates them with the latest from their parents. This a small ruby script located here: Fork Updater. How do we run this? It requires two things. First, a token. We pass that currently as an environment property. It could be in a file in a vault or a secret mounted in as a file it doesn’t matter. Currently, it’s an environment property. The second thing is more subtle.\nI’m pushing the changes back into my remote forks. I’m doing this via SSH. So, we need a key in there too. How we’ll get that in there, I’ll talk about later when we are talking about how to set this cron job up. For now though, the container needs to look for a key in a specific location because we don’t want to over-mount /root/.ssh/ and we also don’t want to use an initContainer to copy over an SSH key (because it’s mounted in as a symlink (but that’s a different issue all together)). Also, we certianly do NOT want to have a key in the container.\nTo achieve this, we simply set up a config file for SSH like this one:\nHost github.com IdentityFile /etc/secret/id_rsa /etc/secret will be the destination of the ssh key we create.\nAnd we also need to have a known_hosts file, otherwise git clone will complain. We also bake this into the container. Why? Why not generate that on the fly? Because I want it to fail in case there is something wrong or there is a MIMA going on etc.\nAll this translated into a Dockerfile looks like this:\n# We are using alpine for a minimalistic imageFROMalpine:latestRUN apk --no-cache add ca-certificatesRUN apk update# Openssh is needed for the SSH commandRUN apk --no-cache add ruby vim curl git build-base ruby-dev openssh openssh-client# Setup dependencies for the fork ruby fileRUN gem install octokit logger multi_json json --no-ri --no-rdocRUN mkdir /dataWORKDIR/data# Setup some data about the committerRUN git config --global user.name \"Fork Updater\"RUN git config --global user.email \"email@email.com\"RUN mkdir -p /root/.ssh# Get the host config for github.comRUN ssh-keyscan github.com \u003e\u003e /root/.ssh/known_hosts# Setup the SSH configCOPY ./config /root/.sshCOPY ./fork_updater.rb .CMD [\"ruby\", \"/data/fork_updater.rb\"]That’s it. Now our updater is containerized and ready to be deployed as a cronjob on a kube cluster. Oh, and we also need to create the SSH key like this:\nkubectl create secret generic ssh-key-secret --from-file=ssh-privatekey=/path/to/.ssh/id_rsa Before we Begin There are two things we will need though to set up for our cluster before we even begin adding the first service. And that’s an ingress with a load-balancer and cert-manager.\nCert-Manager Now, you have the option of installing cert-manager via helm, or via the provided kube config yaml file. I STRONGLY recommend using the config yaml file because the upgrading process with helm is a hell of a lot dirtier / failure prone than simply applying a new yaml file with a different version in it.\nEither way, to install cert-manager follow this simple guide: Cert-manager Install Manual.\nIngress An Ingress is a must. This is the component which manages external access to the services which we will define. Like a proxy before your http server. This component will handle the hostname based routing between our services.\nI’m using nginx ingress, although there are a couple of implementations out there.\nTo install nginx ingress, follow their guide here: Installing Nginx-Ingress.\nFrom Easy to Complicated Alright. Now that we have the prereqs out of the way, let’s get our hands dirty. I’ll start with the easiest of them all, my web site, and then will progress towards the more complicated ones, like Gitea and Athens, which require a lot more fiddling and have more moving parts.\nMy Website The site, located here: Gergely’s Domain; is a really simple, static, Hugo based website. It contains nothing fancy, no real Javascript magic, has a simple list of things I’ve done and who I am.\nIt’s powered / served by an nginx instance running on port 9090 define by a very simple Dockerfile:\nFROMgolang:latest as builderRUN apt-get update \u0026\u0026 apt install -y git make vim hugoRUN mkdir -p /opt/websiteRUN git clone https://github.com/Skarlso/gergelybrautigam /opt/websiteWORKDIR/opt/websiteRUN makeFROMnginx:latestRUN mkdir -p /var/www/html/gergelybrautigamWORKDIR/var/www/html/gergelybrautigamCOPY --from=builder /opt/website/public .COPY 01_gergelybrautigam /etc/nginx/sites-available/RUN mkdir -p /etc/nginx/sites-enabled/RUN ln -s /etc/nginx/sites-available/01_gergelybrautigam /etc/nginx/sites-enabled/01_gergelybrautigamEasy as goblin pie. Nginx has a command set like this CMD [\"nginx\", \"-g\", \"daemon off;\"] and exposes port 80.\nThe deployment In order to deploy this in the cluster, I created a deployment as follows:\napiVersion: apps/v1 kind: Deployment metadata: name: gb-deployment namespace: gergely-brautigam labels: app: gergelybrautigam annotations: prometheus.io/scrape: 'true' prometheus.io/port: '9090' spec: replicas: 1 selector: matchLabels: app: gergelybrautigam template: metadata: labels: app: gergelybrautigam annotations: prometheus.io/scrape: 'true' prometheus.io/port: '9090' spec: containers: - name: gergelybrautigam image: skarlso/gergelybrautigam:v0.0.26 ports: - containerPort: 9090 The metadata section defines information about the deployment. It’s name is gb-deployment. The namespace in which this sits is called gergely-brautigam and it has some labels to it so Prometheus monitoring tool can discover the pod.\nIt’s running a single replica, has a bunch of more metadata and template settings, and finally the container spec which defines the image, and the exposed container port on which the application is running.\nNow we need a service to expose this deployment.\nThe service The service is also simple. It looks like this:\nkind: Service apiVersion: v1 metadata: namespace: gergely-brautigam name: gb-service spec: selector: app: gergelybrautigam ports: - port: 80 targetPort: 9090 Again, nothing fancy here, just a simple service exposing a port to a different port on the front-end side. This service will be picked up by our previously created routing facility.\nIngress Now that we have the service we need to expose it to the domain. I have the domain gergelybrautigam.com and I already pointed it at the LoadBalancer’s IP which was created by the nginx ingress controller.\nWe only want one LoadBalancer, but we have multiple hostnames. We can achieve that by creating an Ingress resource in the namespace our service is in like this:\napiVersion: extensions/v1beta1 kind: Ingress metadata: namespace: gergely-brautigam name: gergely-brautigam-ingress annotations: kubernetes.io/ingress.class: \"nginx\" certmanager.k8s.io/cluster-issuer: \"letsencrypt-prod\" certmanager.k8s.io/acme-challenge-type: http01 nginx.ingress.kubernetes.io/rewrite-target: / spec: tls: - hosts: - gergelybrautigam.com secretName: gergelybrautigam-tls rules: - host: gergelybrautigam.com http: paths: - backend: serviceName: gb-service servicePort: 80 Remember, we already have the nginx ingress resource in the default namespace when we installed the controller previously. That is the main entrypoint. We are taking advantage of the rewrite-target annotation. That is our key to success nginx.ingress.kubernetes.io/rewrite-target: /. The rest is basic routing. We’ll have something like this in the other namespace to.\nAnd with that, our website is done and it should be working under HTTPS. Cert-manager should have picked it up and generated a certificate for it. Let’s check.\nRunning k get certs -n gergely-brautigam you should see something like this:\n $ k get certs -n gergely-brautigam NAME READY SECRET AGE gergelybrautigam-tls True gergelybrautigam-tls 86d If there is a problem, just describe the cert resource and look for the generated challenge and if it was successful or not. The challenge contains mostly good error messages.\nIRC bouncer That wasn’t too bad, right? Let’s do something a bit more complex this time. We are going to deploy The lounge irc bouncer.\nIt’s actually quite easy to do but can be daunting to look at at first.\nThe container Lucky for us, the bouncer already provides a container located here: The Lounge Docker Hub.\nWe just need two things. To expose the port 9000 and to give it something called a PersistentVolume. What’s a persistent volume? Well, look it up here: Kubernetes Persistent Volumes.\nTL;DR: We need to preserve data. Containers are ephemeral in nature. Meaning if there is a problem we usually just delete the pod. Which means that all data will be lost. But we need persistence in this case because we’ll have user data and user information which we would like to persist between pods. That’s what a volume is for.\nIt will be mounted into the pod so we can point the bouncer to use that location for data management.\nPVC With that, this is how our PersistentVolumeClaim will look like:\napiVersion: v1 kind: PersistentVolumeClaim metadata: namespace: powerhouse name: do-storage-irc spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: do-block-storage DigitalOcean provides a block storage implementation for this claim so we use that storage class do-block-storage.\nDeployment With that, this is how the deployment will look like:\napiVersion: apps/v1 kind: Deployment metadata: namespace: powerhouse name: irc-app labels: app: irc spec: replicas: 1 selector: matchLabels: app: irc template: metadata: labels: app: irc app.kubernetes.io/name: irc app.kubernetes.io/instance: irc spec: containers: - name: irc image: thelounge/thelounge:3.1.1 ports: - containerPort: 9000 name: irc-http volumeMounts: - mountPath: /var/opt/thelounge subPath: thelounge name: irc-data readOnly: false volumes: - name: irc-data persistentVolumeClaim: claimName: do-storage-irc Short and sweet. The important bits are the labels, those are used by cert-manager and ingress to find the right deployment, and the volumeMounts. We mount into the /var/opt/thelounge folder because that’s the main configuration location. The subPath is important for a correct mounting.\nThe service Alright, with the deployment in place, let’s take a look at the service.\nkind: Service apiVersion: v1 metadata: namespace: powerhouse name: irc labels: app: irc app.kubernetes.io/name: irc app.kubernetes.io/instance: irc spec: selector: app: irc app.kubernetes.io/name: irc app.kubernetes.io/instance: irc ports: - name: http port: 9000 targetPort: irc-http Again, very boring stuff. Boring is good. Boring is predictable. We expose port 9000 to the named targetPort called irc-http which we defined in the above deployment.\nNow, I have a domain in which these things are running, let’s call it powerhouse.com (because I’m tired of example.com). And I have multiple services in this namespace too, so I’ll call the namespace here, powerhouse and put this irc service in there. This also means that the ingress resource for this namespace will contain a couple more routings, because my powerhouse namespace will also contain my gitea and Athens proxy installation.\nWe can, however, take a peak at the ingress resource here and now… because I hate suspense.\napiVersion: extensions/v1beta1 kind: Ingress metadata: namespace: powerhouse name: powerhouse-ingress annotations: kubernetes.io/ingress.class: \"nginx\" certmanager.k8s.io/cluster-issuer: \"letsencrypt-prod\" certmanager.k8s.io/acme-challenge-type: http01 nginx.ingress.kubernetes.io/rewrite-target: / spec: tls: - hosts: - irc.powerhouse.com secretName: irc-powerhouse-tls - hosts: - gitea.powerhouse.com secretName: gitea-powerhouse-tls - hosts: - athens.powerhouse.com secretName: athens-powerhouse-tls rules: - host: irc.powerhouse.com http: paths: - backend: serviceName: irc servicePort: 9000 path: / - host: gitea.powerhouse.com http: paths: - backend: serviceName: gitea servicePort: 3000 path: / - host: athens.powerhouse.com http: paths: - backend: serviceName: athens-service servicePort: 80 path: / We can see that I have multiple paths pointing to three different subdomains with different ports. These ports will be routed to by nginx ingress. Meaning you DO NOT OPEN THESE ON YOUR LOADBALANCER. These will all be accessible from 443/HTTPS. Expect for gitea’s SSH port later on.\nWith these in place, cert-manager should pick it up and provide a certificate for it.\nSide track – debugging If there is a problem and we can’t reach TheLounge we need to debug. I use the following tool to access Kubernetes resources: K9S. It’s a neat CLI tool to look at kube resources in an interactive way and not having to type in a bunch of commands. Never the less, I will also paste those in here.\nTo look at the pods that should have been created, type:\nk get pods -n powerhouse Should see something like this:\nNAME READY STATUS RESTARTS AGE athens-app-857749c59c-lmjnb 1/1 Running 0 6d3h gitea-app-6974fb995b-pn2vv 1/1 Running 0 9d gitea-db-59758fbcd9-4562c 1/1 Running 0 9d irc-app-5f87688f98-dqsvb 1/1 Running 0 9d You can see that my other services are running fine. And there is IRC as well. Now if there would be any kind of problem we could access the Pods information be describing the pod with:\nk describe pod/irc-app-5f87688f98-dqsvb -n powerhouse Which will provide a bunch of information about the pod. But the pod could be absolutely fine, yet our service could be down. (We didn’t define any liveliness or readiness probs after all).\nWe can verify that by taking a peak in the container (also, check if our mounting was successful). Since this is just a container, exec works similar to docker exec.\n $ k exec -it irc-app-5f87688f98-dqsvb -n powerhouse /bin/bash root@irc-app-5f87688f98-dqsvb:/# Should give us a prompt. We can now look at logs, check out the configuration folder etc.\nIn k9s you would simply select the right namespace, select the pod, hit d for describe or s for shell. Done.\nGitea Now, we have IRC running. Let’s try deploying Gitea. This takes a tiny bit more fiddling though.\nRequirements Gitea requires the following things to be present:\n The gitea app configuration file (this can be done via environment properties though) A DB A PersistentVolume SSH Port for SSH based git clones instead of simple https  DB We shall begin with the simplest of them, the DB. At this point we could go with the DigitalOcean managed Postgres installation, but I didn’t want to put that on the bill as well. So I choose to simply put my DB into a container and deploy it within the cluster.\nThis is actually quite simple. The DB will be a separate deployment / application in the same namespace as the Gitea app. It will also contain a network policy, since the DB doesn’t need internet access and the internet shouldn’t be able to access it.\nIn fact the only thing that should be able to access the DB is the Gitea application itself which we will be able to restrict via the usage of… Labels!\nDeployment But first, take a look at the deployment of a Postgres 11 pod:\napiVersion: apps/v1 kind: Deployment metadata: namespace: powerhouse name: gitea-db spec: replicas: 1 selector: matchLabels: app: gitea-db template: metadata: name: gitea-db labels: app: gitea-db spec: containers: - name: postgres image: postgres:11 env: - name: POSTGRES_USER value: gitea - name: POSTGRES_PASSWORD valueFrom: secretKeyRef: name: gitea-db-password key: password - name: POSTGRES_DB value: gitea volumeMounts: - mountPath: /var/lib/postgresql/data subPath: data # important so it gets mounted properly name: git-db-data volumes: - name: git-db-data persistentVolumeClaim: claimName: do-storage-gitea-db Okay, there are a lot of things going on here, but the three things we need to note are the following:\nlabels: app: gitea-db Our network policy will look for this label to identify the pods which fell under its rule.\n- name: POSTGRES_PASSWORD valueFrom: secretKeyRef: name: gitea-db-password key: password The database password will come from a secret.\nvolumeMounts: - mountPath: /var/lib/postgresql/data subPath: data # important so it gets mounted properly name: git-db-data volumes: - name: git-db-data persistentVolumeClaim: claimName: do-storage-gitea-db We also need a persistent volume otherwise the data will be lost on each pod restart.\n--- apiVersion: v1 kind: PersistentVolumeClaim metadata: namespace: powerhouse name: do-storage-gitea-db spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: do-block-storage Service We also need a Service so Gitea will be able to reach it. This isn’t public though so a NodePort is enough with no clusterIP.\nkind: Service apiVersion: v1 metadata: namespace: powerhouse name: gitea-db-service spec: ports: - port: 5432 selector: app: gitea-db clusterIP: None In order to reach this DB we can use a URL like this now from the Gitea app: gitea-db-service.powerhouse.svc.cluster.local:5432.\nNetworkPolicy We want the Gitea app to be able to reach it. Which means in-out to the Gitea app and nothing else.\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: gitea-db-network-policy namespace: powehouse spec: podSelector: matchLabels: app: gitea-db policyTypes: - Ingress - Egress ingress: - from: - podSelector: matchLabels: app: gitea ports: - protocol: TCP port: 5432 egress: - to: - podSelector: matchLabels: app: gitea ports: - protocol: TCP port: 5432 We can test this now by exec-ing into the Pod of the DB deployment and trying to ping google.com for example. It should be denied. Yet later, when we deploy our Gitea app, that should be able to talk to the DB instance.\nSecret Finally, we have a Secret which contains our db password base64 encoded.\napiVersion: v1 kind: Secret metadata: namespace: cronohub name: gitea-db-password type: Opaque data: password: Z2l0ZWE= That says password123. To get it, you can run something like echo -n \"password123\" | base64.\nGitea App ini Huh, with that done, we can go on with the application ini file. This can be configured via environment properties but once you get over a dozen configuration entries, it’s just easier to use an app.ini. My app ini is large, so I won’t post it here. I could mount it in as a file, but that proved to be difficult or not work at all properly because Gitea is running under a different user than root. Also, once the mount happened the fact the gitea was trying to write into it caused problems. Mounting as a different user didn’t work out either, so I’m using an InitContainer to do the job. They are there for that reason. And it was actually a hell of a lot simpler than doing file mounting.\nThe app.ini is defined as a ConfigMap like this:\nkubectl create configmap gitea-app-ini --from-file=app.ini --namespace powerhouse This was done from the folder where my app.ini was residing.\nDeployment Now comes the big gun. The Gitea deployment file. This is how it looks like in all its glory:\napiVersion: apps/v1 kind: Deployment metadata: namespace: cronohub name: gitea-app labels: app: gitea spec: replicas: 1 selector: matchLabels: app: gitea template: metadata: labels: app: gitea app.kubernetes.io/name: gitea app.kubernetes.io/instance: gitea spec: initContainers: - name: init-disk image: busybox:latest command: - /bin/chown - 1000:1000 # we set the gid and uid of the user for gitea. - /data volumeMounts: - name: git-data mountPath: \"/data\" readOnly: false - name: init-app-ini image: busybox:latest command: ['sh', '-c', 'mkdir -p /data/gitea/conf/; cp /data/app.ini /data/gitea/conf'] volumeMounts: - name: git-data mountPath: \"/data\" readOnly: false - name: gitea-app-ini-conf mountPath: /data/app.ini subPath: app.ini readOnly: false containers: - name: gitea image: gitea/gitea:1.9.2 env: - name: DB_PASSWD valueFrom: secretKeyRef: name: gitea-db-password key: password - name: DB_TYPE valueFrom: configMapKeyRef: name: gitea-config-map key: DB_TYPE - name: DB_HOST valueFrom: configMapKeyRef: name: gitea-config-map key: DB_HOST - name: DB_NAME valueFrom: configMapKeyRef: name: gitea-config-map key: DB_NAME - name: DB_USER valueFrom: configMapKeyRef: name: gitea-config-map key: DB_USER ports: - containerPort: 3000 name: gitea-http - containerPort: 22 name: gitea-ssh volumeMounts: - mountPath: /data name: git-data readOnly: false volumes: - name: git-data persistentVolumeClaim: claimName: do-storage-gitea - name: gitea-app-ini-conf configMap: name: gitea-app-ini The important bit is the initContainer section. What’s happening here? We mount the app.ini file to the init container under /data. The awesome part about the initContainer is that the real container will have access to the file system the init container created.\nSo we take that file, fix the permissions on it and copy it to the right location under /data/gitea/conf for the Gitea app to work with.\nDone!\nAnd the configMap is simple:\napiVersion: v1 kind: ConfigMap metadata: namespace: powerhouse name: gitea-config-map data: APP_COLOR: blue APP_MOD: prod DB_TYPE: postgres DB_HOST: \"gitea-db-service.cronohub.svc.cluster.local:5432\" DB_NAME: gitea DB_USER: gitea SSH Normally, Ingress only allows HTTP based traffic control. But what would an ingress be without also regular TCP based routing?\nBut it’s not trivial. Nginx Ingress provides a documentation for this here: Exposing TCP and UDP services. What does that mean in practice?\nYou see we are also exposing port 22 on the container:\n- containerPort: 22 name: gitea-ssh I choose to differentiate my SSH port for Gitea from port 22 because that’s just cumbersome to get done right. Gitea provides an explanation on how to do port 22 forwarding in a docker container with a custom git command which forwards commands to the container itself. This is all just plain too much to worry about.\nI have this in the app.ini:\nSSH_PORT = \u003cport of my choosing\u003e And then this in the Service definition:\nkind: Service apiVersion: v1 metadata: namespace: powerhouse name: gitea labels: app: gitea app.kubernetes.io/name: gitea app.kubernetes.io/instance: gitea spec: selector: app: gitea app.kubernetes.io/name: gitea app.kubernetes.io/instance: gitea ports: - name: http port: 3000 targetPort: gitea-http - name: ssh port: \u003cport of my choosing\u003e targetPort: gitea-ssh protocol: TCP And then, we edit the nginx-controller deployment like this:\nkubectl edit deployment.apps/nginx-ingress-controller And add this line --tcp-services-configmap=cronohub/gitea-ssh-service to the container’s args field:\ncontainers: - args: - /nginx-ingress-controller - --default-backend-service=default/nginx-ingress-default-backend - --election-id=ingress-controller-leader - --ingress-class=nginx - --configmap=default/nginx-ingress-controller - --tcp-services-configmap=powerhouse/gitea-ssh-service One more thing is that we have to open that port on the load balancer as well to get traffic to it. To that end, edit the nginx ingress service as well:\nkubectl edit services/nginx-ingress-controller And add the exposed port:\n- name: ssh port: \u003cport of my choosing\u003e protocol: TCP targetPort: \u003cport of my choosing\u003e There will probably be a nodePort section in there on the other ports. Ignore that for your change.\nAlso, if you are doing the nginx installation by hand, just add this or save the yaml file from those deployments like this:\nkubectl get service/nginx-ingress-controller -o yaml \u003e nginx-ingress-controller.yaml So you can deploy / modify it later on.\nFinished Gitea And with that, visit gitea.powerhouse.com and it should work including HTTPS and SSH!\nYou can now clone repositories like this: git clone ssh://git@gitea.powerhouse.com:1234/user/awesome_project.git after you created your user.\nUser creation is done by using the gitea admin CLI tool described here: Gitea Documentation.\nIt is important to note that we don’t use latest anywhere. It’s just not good if you are trying to update a service later on. We could set ImagePolicy to AlwaysPull but that’s just not a good thing to do if you have a 2 gig image. Always use version and policy imagePullPolicy: IfNotPresent to save yourself some bandwidth.\nIdle Checker Let’s create a last resource, then we’ll call it a day.\nThe idle RPG is a cool little game that you play by… not playing. At all. If you play, you get penalties. Here is a cool resource to start: Idle RPG. It looks something like this:\n21:56 \u003c@IdleBot\u003e Verily I say unto thee, the Heavens have burst forth, and the blessed hand of God carried ganome 0 days, 03:52:11 toward level 45. 21:56 \u003c@IdleBot\u003e ganome reaches next level in 0 days, 01:49:16. 22:02 \u003c@IdleBot\u003e himuraken, the level 77 Mage Of BitFlips, is now online from nickname himuraken. Next level in 11 days, 10:35:53. 22:14 \u003c@IdleBot\u003e Nechayev, Sundance, and simple [2011/2347] have team battled HeavyPodda, Sixbierehomme, and L [1417/2717] and won! 0 days, 06:14:54 is removed from their clocks. 22:18 \u003c@IdleBot\u003e canton7 saw an episode of Ally McBeal. This terrible calamity has slowed them 0 days, 05:10:53 from level 85. 22:18 \u003c@IdleBot\u003e canton7 reaches next level in 2 days, 00:21:36. 22:26 \u003c@IdleBot\u003e Tor [4/1142] has challenged Brainiac [232/817] in combat and lost! 3 days, 23:06:05 is added to Tor's clock. 22:26 \u003c@IdleBot\u003e Tor reaches next level in 39 days, 23:39:35. It could happen that by some misfortune the bouncer gets restarted and it doesn’t log you back in. Or you simply just lose connection and you don’t re-connect. That is unacceptable because the point is to be present. Otherwise you don’t play. So you need an early warning in case you are offline. Luckily, IdleRPG provides an XML based endpoint to get which contains your status.\nFrom there, I’m using mailgun with a registered domain to send me an email in case my status is offline. For that, here is a small Go program IdleRPG Checker Go Code.\nTo put that into a Docker container, here is a Dockerfile:\nFROMgolang:latest as buildRUN go get -v github.com/sirupsen/logrus \u0026\u0026 \\  go get -v github.com/mailgun/mailgun-goCOPY ./main.go /code/WORKDIR/codeRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o /idlerpg-checker .FROMalpine:latestRUN apk --no-cache add ca-certificatesCOPY --from=build /idlerpg-checker /idlerpg-checkerRUN echo \"v0.0.1\" \u003e\u003e .versionENTRYPOINT [\"/idlerpg-checker\"]And the corresponding cronjob resource definition:\napiVersion: batch/v1beta1 kind: CronJob metadata: name: idle-checker namespace: idle-checker spec: schedule: \"*/20 * * * *\" jobTemplate: spec: template: spec: containers: - name: idle-checker image: skarlso/idle-checker imagePullPolicy: IfNotPresent env: - name: MG_API_KEY valueFrom: secretKeyRef: name: idle-rpg-secret key: MG_API_KEY - name: MG_DOMAIN valueFrom: secretKeyRef: name: idle-rpg-secret key: MG_DOMAIN args: ['-username', 'username', '-email', 'user@powerhouse.com'] restartPolicy: OnFailure Aaaand, the secret for the API key:\napiVersion: v1 kind: Secret metadata: namespace: idle-checker name: idle-rpg-secret type: Opaque data: MG_API_KEY: asdf= MG_DOMAIN: asdf== Done. Huh. This will run every 20 minutes and check if the user with username username is online. If not, it will send an email to the given email address. Your levels are safe.\nClosing words Phew. This has been quite the ride. The post is now really long, so I will split the rest out into a Part 2. That is, Athens and Monitoring.\nThank you for reading this far!\nCheers, Gergely.\n","title":"Using a Kubernetes based Cluster for Various Services with auto HTTPS","uri":"/2019/09/21/kubernetes-cluster/"},{"content":"I had a lot of fun using Procreate to re-draw the architecture image I’ve drawn for my distribute face recognition application detailed in this post Distributed Face-Recognition App.\nWithout much fanfare, here is the drawing:\nThanks, Gergely.\n","title":"Updated Face-recog architecture drawing","uri":"/2019/09/19/updated-face-recog-drawing/"},{"content":"Intro Hi folks!\nToday I would like to write about a metric that I read in a book called Clean Architecture from Robert Cecil Martin ( Uncle Bob ).\nAbstract The metrics I mean are Efferent and Afferent coupling in packages. So you, dear reader, don’t have to navigate away from this page, here are the descriptions pasted in:\n  Afferent couplings (Ca): The number of classes in other packages that depend upon classes within the package is an indicator of the package’s responsibility. Afferent couplings signal inward. (Affected by this package) (Fan-In).\n  Efferent couplings (Ce): The number of classes in other packages that the classes in this package depend upon is an indicator of the package’s dependence on externalities. Efferent couplings signal outward. (Effecting this package) (Fan-Out).\n  These metrics used together will indicate the stability / instability of each package in a project.\nMetric Usage What does it mean if the package is stable vs. unstable? Let’s take a closer look.\nUnstable If the instability metric comes out as 1 or close to 1, that means that the package is unstable. It means that there are only packages which this package is depending upon and nothing, or only 1 or 2, packages depend on it. This infers two things:\n The package is easy to change since there is nothing depending on the behavior explicitly The package is volatile since it depends on a lot of out side things  The first one is self-explanatory. The second one has ramifications. These ramifications are that there are a lot of packages that could cause bugs in this package. Ideally, a package with instability 1 or high, requires a large test coverage to ensure that no bugs seep in.\nStable On the other spectrum lies the indicator for a stable package. If this metric is 0 or close to 0, the package is said to be stable. A stable package resists change because it has a lot of depending packages. The depending packages lock this package in place, meaning we can’t change the package easily. Ideally this is the package that would contain business logic for example, or code which does not change often.\nAppliance in Go ecosystem The book was using mostly Java or C/C++ for examples and dealt with classes describing these metrics. Especially the Abstractness of a package which calculates as ratio of abstract classes + interfaces vs concrete classes and implementations. This isn’t that easy to define in Go. Not impossible though and we could still get something close enough. Something like, count interfaces + structs vs implementations of said interfaces with function receivers and functions.\nThe easier of these is the coupling metrics. I think we can define them since Go also has import statements. Go doesn’t have classes, but it’s enough if we calculate the number of packages that said package depends upon and are depended upon by. Should be close enough.\nTool If there is a project with a lot of packages and files, it would be quite difficult to calculate things using your hands… Hence, Effrit. This tool, at the writing of this post, only calculates the stability metric for now. If given a parameter like -p effrit it will only calculate the Fan-Out metrics considering project packages. If no project name is given, it will also calculate not project packages (for example cobra or aws sdk) as Efferent. Usage is really simple. Navigate to the root of the project and run effrit scan -p \u003cprojectname\u003e.\nApplying the tool Let’s see with a real example on using the tool and what to do with the metrics it provides.\nI have a project called Furnace. Running the tool on it I get the following stats:\n.\nWhat do these means?\nIt means, that hopefully, command packages have a high coverage and that config packages don’t require change that often. The coverage count for aws command package is:\ncoverage: 74.7% of statements\nThat is pretty good. I think it’s covered well enough for now.\nOn to the config package. This is the whole file:\npackage config import ( \"os/user\" \"path/filepath\" \"github.com/go-furnace/go-furnace/handle\" ) // Spinners is a collection os spinner types var Spinners = []string{`←↖↑↗→↘↓↙`, `▁▃▄▅▆▇█▇▆▅▄▃`, `┤┘┴└├┌┬┐`, `◰◳◲◱`, `◴◷◶◵`, `◐◓◑◒`, `⣾⣽⣻⢿⡿⣟⣯⣷`, `|/-\\`} // WAITFREQUENCY global wait frequency default. var WAITFREQUENCY = 1 // STACKNAME is the default name for a stack. var STACKNAME = \"FurnaceStack\" // SPINNER is the index of which spinner to use. Defaults to 7. var SPINNER = 7 // Path retrieves the main configuration path. func Path() string { // Get configuration path \tusr, err := user.Current() handle.Error(err) return filepath.Join(usr.HomeDir, \".config\", \"go-furnace\") } Not a lot of stuff in there. But it’s using the handle package. Hence the 0.2. Luckily, we also have some coverage to take care of that.\nThe handle is pretty stable. Let’s take a peak inside:\npackage handle import \"log\" // LogFatalf is used to define the fatal error handler function. In unit tests, this is used to // mock out fatal errors so we can test for them. var LogFatalf = log.Fatalf // Error extracts the if err != nil check. If the given error is not nil it will call // the defined fatal error handler function. func Error(err error) { if err != nil { Fatal(\"Error occurred:\", err) } } // Fatal is a wrapper for LogFatalf function. It's used to throw a Fatal in case it needs to. func Fatal(s string, err error) { LogFatalf(s, err) } Basic logic to take care of errors in Furnace. Last time I changed this file was… a year ago. Yeah, I think it’s doing fine.\nConclusion And that’s it. Hopefully this is an interesting metric to use to define what packages may need refactoring, or need to be repurposed because they are too rigid. If a packages is stable, aka. hard to change but must undergo changes frequently, it may be time to refactor and introduce a mediator or a liaison package. If a package is unstable and has a lot of bugs, we might want to refactor it and inverse it’s dependencies. This is called the Dependency Inversion Principle, DIP. Which is also described in the same book. However it’s not always bad if a package is unstable. Maybe it contains code which needs to change frequently. It’s a database schema code. Or an algorithm which requires constant tweaking. And that is fine. Just make sure it’s covered well enough.\nThe principles that these metrics are based on are: SAP and SDP. Stable Abstraction Principle and Stable Dependencies Principle. These are also described in the same book, Clean Architecture. A highly recommend it. Applying these principles could help maintain the project’s stability and it’s dependencies.\nThank you for reading, Gergely.\n","title":"Efferent and Afferent metrics in Go","uri":"/2019/04/21/efferent-and-afferent-metrics-in-go/"},{"content":"Hi folks!\nToday’s post is a retrospective. I would like to gather some thoughts about living with the new parser that I wrote for JsonPath.\nAfter a little over a year, some interesting problems surfaced that I thought I’d share for people who also would like to endeavor on this path. Let’s begin.\nPreviously About, two years ago, I took over managing / fixing / improving this ruby gem: Json Parser. It’s a json parser in ruby. Amongst other problems, it used eval in the background to evaluate expressions. It was a security risk to use this gem to its full extent. Something had to be done about that.\nI proceeded to write a semi-language parser which replaced eval that can be found here: Parser. The basic intention was to replace the bare minimum of the eval behavior, and so, it was lacking some serious logic. That got improved as time went by.\nThis is a one year retrospective on living with a self-written parser. Enjoy some of the quirks I faced so you don’t have to.\nAST AST is short for Abstract Syntax Tree. It’s a data structure that is ideal for representing and parsing language syntax. All major lexers use some kind of AST in the background like this old Ruby language parser gem: Whitequark Parser. This parser is used by projects like Rubocop and line coverage reports. It’s usage is not trivial right out of the box. But as you move along you get a firm grasp of true potential.\nI decided to not use that parser a year ago mainly because I thought it’s too much for what I’m trying to accomplish. Maybe I was right, maybe not. I tried to play with Parser recently but it’s none trivial nature and lack of documentation makes it cumbersome to use.\nThe first problems What was then the first trouble that arose after I replaced eval? The parser back then was dumbed down a lot. The bug I faced was a simple infinite loop. The parser works like a lexer. It identifies tokens of certain type and tries to parse them into variables. This lexing had an error.\n- elsif t = scanner.scan(/(\\s+)?'?(\\w+)?[.,]?(\\w+)?'?(\\s+)?/) # @TODO: At this point I should trim somewhere... + elsif t = scanner.scan(/(\\s+)?'?.*'?(\\s+)?/) It was caught by this Json Path:\n$.acceptNewTasks.[?(@.taskEndpoint == \"mps/awesome\")].lastTaskPollTime The culprit was the / character. The tokenizer wasn’t prepared…\nEval would have no problem but the parser is using strict regex-s. This is where an AST would have had more luck.\nNumbers The second problem was the fact that the parser is using strings. Who would have thought that the string 2.0 in fact does not equal to string 2? In Ruby the simplest way of making sure a variable is a Number is by casting the variable to Number or Float. In case it’s not a Number we rescue and move on.\nel = Float(el) rescue el Incidentally this also solved the problem where the json path contained a number but since everything is a string this, also did not equal: '1' == 1.\nSince first the string needed to be a Number.\nSupporting regexes Next came supported operators. The parser only supported the basic operators: \u003c\u003e=. It was missing =~ from this. Which meant people who would use regexes to filter JSON would no longer be able to do so. This was only a tiny modification actually:\nFirst, the operator filter needed to be aware…\n- elsif t = scanner.scan(/(\\s+)?[\u003c\u003e=][=\u003c\u003e]?(\\s+)?/) + elsif t = scanner.scan(/(\\s+)?[\u003c\u003e=][=~]?(\\s+)?/) With that done, we just .to_regexp it with the power of ruby and send would automatically pick it up. And of course test coverage.\nRegression Once the parser was introduced I knew that it would create problems, since eval did many things that the parser could not handle. And they started to arrive slowly. One-by-one.\nBooleans Aka, the story of true == 'true'… Inherently working with strings here makes it difficult to detect when the type boolean is meant or a string which happens to say true. This one was easy to solve as well in the end:\noperand = if t == 'true' true elsif t == 'false' false else operator.to_s.strip == '=~' ? t.to_regexp : t.gsub(%r{^'|'$}, '').strip # We also handle regexp here. end Ignoring the regex part, this was all it needed.\nSyntax Some smaller tid-bits here and there also started to crop up. Things that eval did not mind at all, but my poor Parser couldn’t handle. The regex started out tightly tied. This meant that certain characters weren’t properly detected. Characters like the underscore, or @ or /… All these weren’t picked up by my tight regexp. I had to widen it a bit using .* at certain places.\nNumber formatting Formatting and comparing numbers gave me a lot of headache. I had to detect whether I’m dealing with a number or a string parsed as a number or a number but that was converted into string or a string that happened to be a number. Geez…\nI ended up making it simple like this:\nel = Float(el) rescue el operand = Float(operand) rescue operand Basically everything is a number. Doesn’t matter where it came from, what it was in the past… It’s a number if it can be converted. This, of course, also means that a test like this one fails:\ndef test_number_match json = { channels:[ { elem: 1, }, { elem: '1' } ] }.to_json assert_equal [{ 'elem' =\u003e 1 }], JsonPath.on(json, \"$..channels[?(@.elem == 1)]\") end Both will match… Even though you’d expect it only to match one. Luckily though… this is exactly how http://jsonpath.com/ works as well. An AST would detect that it’s a number type… But since I’m parsing strings here, that would be almost impossible a feat to accomplish in a nice manner.\nGroups And finally, the biggest one… Groups in conditions. A query like this one for example:\n$..book[?((@['author'] == 'Evelyn Waugh' || @['author'] == 'Herman Melville' \u0026\u0026 (@['price'] == 33 || @['price'] == 9))] Something like this was never parsed correctly. Since the parser didn’t understand grouping and order of evaluation. Let’s break it down. How do we get from a monstrous like that one above to something that can be handled? We take it one group at a time.\nParentheses As a first step, we make sure that the parentheses match. It’s possible that someone didn’t pay attention and left out a closing parentheses. Now, there are a couple of way of doing that in Ruby, but I went for the most plain blatant one.\ndef check_parenthesis_count(exp) return true unless exp.include?(\"(\") depth = 0 exp.chars.each do |c| if c == '(' depth += 1 elsif c == ')' depth -= 1 end end depth == 0 end A basic depth counter. We do this first, to avoid parsing an invalid query.\nBreaking it down Next we break down this complex thing into a query that makes more sense to the parser. To do that, we take each group and extract the operation in them and replace it with the value they provide. Meaning a query like the one above essentially should look like this:\n((false || false) \u0026\u0026 (false || true)) Neat. This is handled by this code segment: Parser.\nThe parsing function is called over and over again until there are no parentheses left in the expression. Aka, a single true or false or number remains.\nNow, who can spot an issue with that? The function bool_or_exp is there to return a float or a boolean value. If it returns a float, we still \u0026\u0026= -it together with the result… Thus, if there is a query like this one for example:\n$..book[?(@.length-5 \u0026\u0026 @.type == 'asdf')] This would fail horribly. Which means, asking for a specific index in a json in a grouped expression is not supported at the moment.\nReturn Value The parser doesn’t just return a bool value and call it a day. It also returns indexes like you read above. Indexes in cases when there is a query that returns the location of an item in the node and not if the node contains something or matches some data. For example:\n$..book[(@.length-5)] Returns the length-5-th book.\nOutstanding issues Right now there are two outstanding issues. The one mentioned above, where you can’t nest indexes and true/false notations. And the other is a submitted issue in which it’s described that it’s not possible to use something like this:\n$.phoneNumbers[?(@[0].type == 'home')] Which basically boils down to the fact that Jsonpath can’t handle nested lists like these:\n{ \"phoneNumbers\": [ [{ \"type\" : \"iPhone\", \"number\": \"0123-4567-8888\" }], [{ \"type\" : \"home\", \"number\": \"0123-4567-8910\" }] ] } That isn’t actually the problem of the parser, but Jsonpath itself.\nConclusion Like a good marriage, living with a Parser is a lot of compromise and ironing out edges and working on making it better for both parties involved. I have no doubt that there are more bugs in this code, but I’m trying to keep it concise and clear to read as much as possible.\nI hope this was as fun to read as it was to write.\nThank you for reading,\nGergely.\n","title":"Living with a new Parser for a year","uri":"/2019/04/12/living-with-a-parser/"},{"content":"Intro Hi folks.\nToday I thought I show you how you can use Github Actions to deploy a hugo based blog like this one.\nLet’s dive in.\nActions What are actions? If you read the above linked document they are basically steps performed in containers based on some events that happened with your repository. Events can be such as pushing, creating a PR or creating/closing an issue etc.\nWe need an even on a push.\nActions is in beta right now so much of the documentation has some gaps but they are fairly okay. I recommend reading through this one carefully: Developer Guide. This describes for example accessing the environment. That is important because we will need to access the generated content from one action in the next action.\nDockerfile Each action requires a Dockerfile which will be used to create a container to run this particular action in. The Dockerfile uses LABELS to mark a container. It is recommended to create an ENTRYPOINT in the Dockerfile that can work with CMDs passed in from the action.\nFor example my pusher container has the ability to push into any repository thanks to using arguments for the entrypoint.sh script.\nWe’ll see that later on.\nBlog actions Let’s look at the two actions in detail which we’ll be using.\nBuilder First, we need to build the blog. This is accomplished pretty much the same as I wrote earlier in the travis blog part but with a little extra information.\nThis is the Dockerfile:\nFROMgolang:latestLABEL \"name\"=\"Hugo Builder\"LABEL \"maintainer\"=\"Gergely Brautigam \u003cgergely@gergelybrautigam.com\u003e\"LABEL \"version\"=\"0.1.0\"LABEL \"com.github.actions.name\"=\"Go Builder\"LABEL \"com.github.actions.description\"=\"Build a hugo blog\"LABEL \"com.github.actions.icon\"=\"package\"LABEL \"com.github.actions.color\"=\"#E0EBF5\"RUN \\  apt-get update \u0026\u0026 \\  apt-get install -y ca-certificates openssl git \u0026\u0026 \\  update-ca-certificates \u0026\u0026 \\  rm -rf /var/lib/aptRUN go get github.com/gohugoio/hugoCOPY entrypoint.sh /entrypoint.shENTRYPOINT [\"/entrypoint.sh\"]Pretty simple. The entrypoint script looks like this:\n#!/bin/bash  set -e set -x set -o pipefail if [[ -z \"$GITHUB_WORKSPACE\" ]]; then echo \"Set the GITHUB_WORKSPACE env variable.\" exit 1 fi if [[ -z \"$GITHUB_REPOSITORY\" ]]; then echo \"Set the GITHUB_REPOSITORY env variable.\" exit 1 fi root_path=\"$GITHUB_WORKSPACE\" echo \"Root path is: ${root_path}\" blog_path=\"$GITHUB_WORKSPACE/.blog\" echo \"Blog path is: ${blog_path}\" mkdir -p \"$blog_path\" mkdir -p \"$root_path\" cd \"$root_path\" echo \"Preparing to build blog\" hugo --theme hermit echo \"Building is done. Copying over generated files\" cp -R public/* \"$blog_path\"/ echo \"Copy is done.\" exit 0 The interesting parts here are GITHUB_WORKSPACE and GITHUB_REPOSITORY. The workspace is where the repository is located at.\nThis is the place where we will copy our built blog files. Since this is a mount basically on the local build machine the next action which comes along will see the folder .blog. This is how we pass artifacts between actions.\nThis action can be found here: Hugo Blog Builder Action.\nPublisher Once the building finishes successfully we can push it to the new location.\nDockerfile is similar to the one above in every regard. Except for the name and that it doesn’t need Hugo and the command…\nFROMgolang:latestLABEL \"name\"=\"Hugo Pusher\"LABEL \"maintainer\"=\"Gergely Brautigam \u003cgergely@gergelybrautigam.com\u003e\"LABEL \"version\"=\"0.1.0\"LABEL \"com.github.actions.name\"=\"Go Pusher\"LABEL \"com.github.actions.description\"=\"Push a hugo blog\"LABEL \"com.github.actions.icon\"=\"package\"LABEL \"com.github.actions.color\"=\"#E0EBF5\"RUN \\  apt-get update \u0026\u0026 \\  apt-get install -y ca-certificates openssl git \u0026\u0026 \\  update-ca-certificates \u0026\u0026 \\  rm -rf /var/lib/aptCOPY entrypoint.sh /entrypoint.shENTRYPOINT [\"/entrypoint.sh\"]CMD [\"Skarlso/skarlso.github.io.git\"]Why do we require the CMD? Let’s take a look at the script.\n#!/bin/bash  set -e set -x if [[ -z \"$GITHUB_WORKSPACE\" ]]; then echo \"Set the GITHUB_WORKSPACE env variable.\" exit 1 fi if [[ -z \"$GITHUB_REPOSITORY\" ]]; then echo \"Set the GITHUB_REPOSITORY env variable.\" exit 1 fi setup_git() { repo=$1 git config --global user.email \"bot@github.com\" git config --global user.name \"Github Actions\" git init echo \"Starting to clone blog repository\" git remote add origin https://\"${PUSH_TOKEN}\"@github.com/\"${repo}\" \u003e /dev/null 2\u003e\u00261 git pull origin master echo \"Cloning is done\" ls -l } commit_website_files() { git add . git commit -am \"Github Action Build ${GITHUB_SHA}\" } upload_files() { git push --quiet --set-upstream origin master } echo \"Beginning publishing workflow\" repo=$1 if [ -z \"${repo}\" ]; then echo \"Repo must be defined.\" exit 1 fi echo \"Using repository ${repo}to push to\" mkdir /opt/publish \u0026\u0026 cd /opt/publish blog_path=\"$GITHUB_WORKSPACE/.blog\" echo \"Blog is located at: ${blog_path}\" ls -l \"${blog_path}\" echo \"Setting up git\" setup_git \"${repo}\" cp -R \"${blog_path}\"/* . echo \"Copied over generated content from blog path. Committing.\" commit_website_files echo \"Committed. Pushing.\" upload_files echo \"All done.\" exit 0 Now this is a lot more involved. I’m leaving as many echos in here as possible for esae of debugging.\nThe interesting part in here is the repo=$1. This is why we need CMD specified. But this is what makes this Action a bit more flexible too. It can push anywhere it has access to.\nThis action can be found here: Hugo Blog Publisher Action.\nThe Workflow file How does this all fit together? You have to create a workflow file which looks something like this:\nworkflow \"Publish Blog\" { on = \"push\" resolves = [\"blog-publisher\"] } action \"blog-builder\" { uses = \"skarlso/blog-builder@master\" secrets = [\"GITHUB_TOKEN\"] } action \"blog-publisher\" { uses = \"skarlso/blog-publisher@master\" needs = [\"blog-builder\"] secrets = [\"GITHUB_TOKEN\", \"PUSH_TOKEN\"] } This is located in your repositroy under .github/main.workdflow. Notice the secrets. GITHUB_TOKEN is created for you by Github. This is a basic token which lets you access the github API. But it can’t be used for pushing code. Thus, we need another token. This can be defined under your repository / settings / secrets. Once you have a token, add a new secret called PUSH_TOKEN and… done.\nEverything should be ready to go.\nLocation of the actions Now, I read the doc and should have been possible to have these actions in the repositroy itself. However, I faced some problems with that setup so I ended up having actions in their respectice repository. That’s why uses is set up to be skarlso/\u003caction-name\u003e@branch.\nConclusion On a push now the blog is built and published. If a step fails it won’t be published. It’s actually a lot faster than my travis build was.\nThank you for reading,\nGergely.\n","title":"Deploy a Hugo Blog Github Actions","uri":"/2019/03/19/deploy-hugo-blog-github-actions/"},{"content":"Intro Good afternoon folks.\nToday, I would like to talk a little bit about Cronohub. It’s a Python application which you can use to archive anything from anywhere to anywhere. It uses plugins to archive this versatility.\nLet me show you some of its features.\nMain Usage Cronohub is a python application which uses the power and ease of usage of Python to give the user a framework. This Framework can then be used to implement concrete functionality in the form of plugins.\nThe plugins provide the essential working logic for Cronohub. Cronohub itself, is basically a Hub for these plugins.\nWhat can Cronohub be actually used for? Say, you have a gazillion Github repositories you would like to archive to a S3 bucket. Or SCP to another server. Or to ownCloud… You would use cronohub as such:\n❯ cronohub -s {github,gitlab} -t {s3,owncloud,scp} It is this simple. What’s going on here then? -s tells Cronohub to use a source plugin called github where as -t tells it to use a target plugin called s3 or owncloud or an scp operation.\nUnder the hood What happens then is that the source plugin downloads the repositories for a given user. The plugins themselves can require certain configuration options to be provided. Like environment properties. To get the Help of a plugin you can simply ask Cronohub like this:\n❯ cronohub help --source-help github Help (github source plugin): - Environment Property: CRONO_GITHUB_TOKEN: a token with access to listing repositories for a given user. - File that filters the list of repositories to archive. If not present, all will be archived. ~/.config/cronohub/configurations/github/.repo_list It will display help information for that plugin.\nOnce the source plugin finished downloading the requested repositories to a given location it will pass on a list of files to the target plugin for archiving. The target plugin takes this list of files and will add a timestamp to the file and upload them with the requested operation.\nThe plugins take care of parallelization or threading if this makes it faster. For example the github plugin downloads at most 5 archive files concurrently.\nPlugins What are plugins then? The plugins adhere to an ABC. An Abstract Base Class. This class defines the abilities and structure of a plugin. It looks like this:\nfrom abc import ABCMeta, abstractmethod class CronohubSourcePlugin(metaclass=ABCMeta): \"\"\" This is the basic definition of a CronoHub plugin. \"\"\" @abstractmethod def validate(self): ... @abstractmethod def help(self): ... @abstractmethod def fetch(self): \"\"\" Returns a tuple (str, str) where there first parameter is the name of the archive and the second is the location as a full path. Exp: (\"my-project-12345\", \"/home/user/projects/my-project/my-project.tar.gz\") \"\"\" ... Validation will be called before the plugin can be used. This method can be used to validate settings for a plugin, for example if a token is provided for the github plugin. Or a bucket name is defined for the S3 plugin etc.\nHelp will display information just like we saw above and fetch will actually perform the downloading or fetching of files to later archive.\nPlugins are located in this repository: Cronohub plugins.\nHopefully, at some point I’ll finish https://cronohub.org and then there will be an online repository for these.\nPlugins are located at ~/.config/cronohub/plugins/{target,source}. Each plugin must take care of its own dependencies via a requirements.txt file.\nRight now, there are only a few plugins available.\n   Source Plugins Target Plugins     Github S3   Gitlab SCP   Local ownCloud    No-Op    More are hopefully on the way. Local is simply gathering a list of files from a folder. And no-op is what it says. It’s good for debugging a source plugin as it doesn’t do anything else but displays the files it got from the source plugin.\nInstallation Installing is dead trivial. Simply run:\npip install cronohub … and that’s it. It’s published on pypi.org.\nContribution Plugin contributions are heartily welcomed!\nAnd that’s it.\nThank you for reading,\nGergely.\n","title":"Cronohub: Archive from anywhere to anywhere","uri":"/2019/03/19/cronohub/"},{"content":"Intro Hi folks.\nI’ve been using the Hugo build for wercker for a long time now. Recent problems occurred though where I did not understand at first what the problem was. It was quite difficult to debug since I did not have too much insight on the wercker build itself. Turned out that I deleted the GITHUB token that the process was using. However, the error message was telling me that a function failed to load some other function. Which was totally unrelated.\nThus, I thought that I’m going to shift away from this outside medium to a different one that I’m already familiar with and have greater control over.\nHence, Travis. Incidentally, since I will no longer be dependend on a third party component (which was the image wercker was using), I’ll be able to switch away from this build platform easily. For example, to CircleCI.\nI’m using github pages, but without the whole convoluted submodule init, different branch stuff. I find that that simply adds unnecessary complexity to the whole thing. I’m keeping the source and the website in a different repository.\nThe steps are simple:\n Get the source Generate the content locally using hugo Setup Git Get the source for the generated web site Copy in the newly generated code Push the code up to git  Sounds simple… In fact it’s so simple, it’s three files.\nTravis The travis modification is such:\nlanguage: go install: - go get github.com/gohugoio/hugo - sudo apt-get install -y git script: - .travis/build.sh after_success: - cd ${TRAVIS_BUILD_DIR} \u0026\u0026 .travis/push.sh Easy, it’s a standard Go based travis file. There are two things here which stand out. The scripts section and the after_success section. Why after_success? Because if we made a mistake, we don’t want to destroy the website. Thus we only push in case build.sh was successful.\nBuilding In this light, building the blog is simple. In fact the whole script is such:\n#!/bin/sh  set -e set -x mkdir /opt/blog git clone --recurse-submodules https://github.com/Skarlso/blogsource.git /opt/app echo Build started on `date` cd /opt/app hugo --theme hermit cp -R public/* /opt/blog For clone, --recurse-submodules is required because the theme is a submodule. Once this script runs successfull, we can push the new version of the site.\nPushing Pushing is a bit more involved. There are four steps involved in this process.\nSetup git First, we set up git to use some specific name so we know where the push came from.\nsetup_git() { git config --global user.email \"travis@travis-ci.org\" git config --global user.name \"Travis CI\" git init git remote add origin https://${GITHUB_TOKEN}@github.com/Skarlso/skarlso.github.io.git \u003e /dev/null 2\u003e\u00261 git pull origin master } Github token is a secret environment property. We also pull the blog source in this step.\nCopy Then copy everything from the built site’s public folder (which was we already copied to /opt/blog) to this folder.\ncp -R /opt/blog/* . Commit the changes commit_website_files() { git add . git commit -am \"Travis build: $TRAVIS_BUILD_NUMBER\" } This is extracted for clarity.\nPushing the changes upload_files() { git push --quiet --set-upstream origin master } The script in it’s entirety here: push.sh\nAnd that’s it. The site is changed and updated. This can be executed in any environment and the only requirement is hugo and git being present. If you still prefer the branch method of Github pages, this is easily altered to checkout the right branch and push the changes from there.\nNo dependency on anything. Just how I like my build processes.\nThanks for reading!\nGergely.\nAppendix push.sh #!/bin/sh  set -e set -x setup_git() { git config --global user.email \"travis@travis-ci.org\" git config --global user.name \"Travis CI\" git init git remote add origin https://${GITHUB_TOKEN}@github.com/Skarlso/skarlso.github.io.git \u003e /dev/null 2\u003e\u00261 git pull origin master } commit_website_files() { git add . git commit -am \"Travis build: $TRAVIS_BUILD_NUMBER\" } upload_files() { git push --quiet --set-upstream origin master } mkdir /opt/publish \u0026\u0026 cd /opt/publish setup_git cp -R /opt/blog/* . commit_website_files upload_files ","title":"Deploy a Hugo Blog with Travis on Git Push","uri":"/2019/03/18/hugo-blog-with-travis-deployment/"},{"content":"Hi folks.\nFollowing a long search and reading lots of debates and possibilities of doing SSH within Go, I was shocked to see that not a great many tools and people use SSH with host key verification. What I usually see is this:\nHostKeyCallback: ssh.InsecureIgnoreHostKey() This is terrible. Now, I realise that doing HostKeyVerification can be tedious, but don’t fear. It’s actually easy now that the Go team provided the knownhosts subpackage in their crypto SSH package located here: KnownHosts.\nThis part in particular is interesting: New.\nUsing new with a known_hosts file a code can be written like this one to verify host keys:\npackage main import ( \"bytes\" \"fmt\" \"io/ioutil\" \"log\" \"golang.org/x/crypto/ssh\" kh \"golang.org/x/crypto/ssh/knownhosts\" ) func main() { user := \"user\" address := \"192.168.0.17\" command := \"uptime\" port := \"9999\" key, err := ioutil.ReadFile(\"/Users/user/.ssh/id_rsa\") if err != nil { log.Fatalf(\"unable to read private key: %v\", err) } // Create the Signer for this private key. \tsigner, err := ssh.ParsePrivateKey(key) if err != nil { log.Fatalf(\"unable to parse private key: %v\", err) } hostKeyCallback, err := kh.New(\"/Users/user/.ssh/known_hosts\") if err != nil { log.Fatal(\"could not create hostkeycallback function: \", err) } config := \u0026ssh.ClientConfig{ User: user, Auth: []ssh.AuthMethod{ // Add in password check here for moar security. \tssh.PublicKeys(signer), }, HostKeyCallback: hostKeyCallback, } // Connect to the remote server and perform the SSH handshake. \tclient, err := ssh.Dial(\"tcp\", address+\":\"+port, config) if err != nil { log.Fatalf(\"unable to connect: %v\", err) } defer client.Close() ss, err := client.NewSession() if err != nil { log.Fatal(\"unable to create SSH session: \", err) } defer ss.Close() // Creating the buffer which will hold the remotly executed command's output. \tvar stdoutBuf bytes.Buffer ss.Stdout = \u0026stdoutBuf ss.Run(command) // Let's print out the result of command. \tfmt.Println(stdoutBuf.String()) } Here is the whole thing as a Gist.\nPlease try and avoid using Insecure host keys. It is easier, but can harm so much. Software like these: Man in The Middle Proxy thrive in an environment that doesn’t do it, or doesn’t in other ways mitigate this problem.\nBe wise and be safe. Thanks for reading, Gergely.\n","title":"Go SSH with Host Key Verification","uri":"/2019/02/17/go-ssh-with-host-key-verification/"},{"content":"Intro If you don’t know what go-plugin is, don’t worry, here is a small introduction on the subject matter:\nBack in the old days when Go didn’t have the plugin package, HashiCorp was desperately looking for a way to use plugins.\nIn the old days, Lua plus Go wasn’t really a thing yet, and to be honest, nobody wants to write Lua ( joking!).\nAnd thus Mitchell had this brilliant idea of using RPC over the local network to serve a local interface as something that could easily be implemented with any other language that supported RPC. This sounds convoluted but has many benefits! For example, your code will never crash because of a plugin and the ability to use any language to implement a plugin. Not just Go.\nIt has been a battle-hardened solution for years now and is being actively used by Terraform, Vault, Consule, and especially Packer. All using go-plugin in order to provide a much needed flexibility. Writing a plugin is easy. Or so they say.\nIt can get complicated quickly, for example, if you are trying to use GRPC. You can lose sight of what exactly you’ll need to implement, where and why; or utilizing various languages or using go-plugins in your own project and extending your CLI with pluggable components.\nThese are all nothing to sneeze at. Suddenly you’ll find yourself with hundreds of lines of code pasted from various examples and yet nothing works. Or worse, it DOES work but you have no idea how. Then you find yourself needing to extend it with a new capability, or you find an elusive bug and can’t trace its origins.\nFear not. I’ll try to demystify things and draw a clear picture about how it works and how the pieces fit together.\nLet’s start at the beginning.\nBasic plugin Let’s start by writing a simple Go GRPC plugin. In fact, we can go through the basic example in the go-plugin’s repository which can be quite confusing when first starting out. We’ll go step-by-step, and the switch to GRPC will be much easier!\nBasic concepts Server In the case of plugins, the Server is the one serving the plugin’s implementation. This means the server will have to provide the implementation to an interface.\nClient The Client calls the server in order to execute the desired behaviour. The underlying logic will connect to the server running on localhost on a random higher port, call the wanted function’s implementation and wait for a response. Once the response is received provide that back to the calling Client.\nImplementation The main function Logger The plugins defined here use stdout in a special way. If you aren’t writing a Go based plugin, you will have to do that yourself by outputting something like this:\n1|1|tcp|127.0.0.1:1234|grpc We’ll come back to this later. Suffice to say the framework will pick this up and will connect to the plugin based on the output. In order to get some output back, we must define a special logger:\n// Create an hclog.Logger \tlogger := hclog.New(\u0026hclog.LoggerOptions{ Name: \"plugin\", Output: os.Stdout, Level: hclog.Debug, }) NewClient // We're a host! Start by launching the plugin process. \tclient := plugin.NewClient(\u0026plugin.ClientConfig{ HandshakeConfig: handshakeConfig, Plugins: pluginMap, Cmd: exec.Command(\"./plugin/greeter\"), Logger: logger, }) defer client.Kill() What is happening here? Let’s see one by one:\nHandshakeConfig: handshakeConfig,: This part is the handshake configuration of the plugin. It has a nice comment as well.\n// handshakeConfigs are used to just do a basic handshake between // a plugin and host. If the handshake fails, a user friendly error is shown. // This prevents users from executing bad plugins or executing a plugin // directory. It is a UX feature, not a security feature. var handshakeConfig = plugin.HandshakeConfig{ ProtocolVersion: 1, MagicCookieKey: \"BASIC_PLUGIN\", MagicCookieValue: \"hello\", } The ProtocolVersion here is used in order to maintain compatibility with your current plugin versions. It’s basically like an API version. If you increase this, you will have two options. Don’t accept lower protocol versions nor switch to the version number and use a different client implementation for a lower version than for a higher version. This way you will maintain backwards compatibility.\nThe MagicCookieKey and MagicCookieValue are used for a basic handshake which the comment is talking about. You have to set this ONCE for your application. Never change it again, for if you do, your plugins will no longer work. For uniqueness sake, I suggest using UUID.\nCmd is one of the most important parts about a plugin. Basically how plugins work is that they boil down to a compiled binary which is executed and starts an RPC server. This is where you will have to define the binary which will be executed and does all this. Since this is all happening locally, (please keep in mind that Go-plugins only support localhost, and for a good reason), these binaries will most likely sit next to your application’s binary or in a pre-configured global location. Something like: ~/.config/my-app/plugins. This is individual for each plugin of course. The plugins can be autoloaded via a discovery function given a path and a glob.\nAnd last but not least is the Plugins map. This map is used in order to identify a plugin called Dispense. This map is globally available and must stay consistent in order for all the plugins to work:\n// pluginMap is the map of plugins we can dispense. var pluginMap = map[string]plugin.Pluglin\t\"greeter\": \u0026example.GreeterPlugin{}, } You can see that the key is the name of the plugin and the value is the plugin.\nWe then proceed to create an RPC client:\n// Connect via RPC \trpcClient, err := client.Client() if err != nil { log.Fatal(err) } Nothing fancy about this one…\nNow comes the interesting part:\n// Request the plugin \traw, err := rpcClient.Dispense(\"greeter\") if err != nil { log.Fatal(err) } What’s happening here? Dispense will look in the above created map and search for the plugin. If it cannot find it, it will throw an error at us. If it does find it, it will cast this plugin to an RPC or a GRPC type plugin. Then proceed to create an RPC or a GRPC client out of it.\nThere is no call yet. This is just creating a client and parsing it to a respective representation.\nNow comes the magic:\n// We should have a Greeter now! This feels like a normal interface \t// implementation but is in fact over an RPC connection. \tgreeter := raw.(example.Greeter) fmt.Println(greeter.Greet()) Here we are type asserting our raw GRPC client into our own plugin type. This is so we can call the respective function on the plugin! Once that’s done we will have a {client,struct,implementation} that can be called like a simple function.\nThe implementation right now comes from greeter_impl.go, but that will change once protoc makes an appearance.\nBehind the scenes, go-plugin will do a bunch of hat tricks with multiplexing TCP connections as well as a remote procedure call to our plugin. Our plugin then will run the function, generate some kind of output, and will then send that back for the waiting client.\nThe client will then proceed to parse the message into a given response type and will then return it back to the client’s callee.\nThis concludes main.go for now.\nThe Interface Now let’s investigate the Interface. The interface is used to provide calling details. This interface will be what defines our plugins’ capabilities. How does our Greeter look like?\n// Greeter is the interface that we're exposing as a plugin. type Greeter interface { Greet() string } This is pretty simple. It defines a function which will return a string typed value.\nNow, we will need a couple of things for this to work. Firstly we need something which defines the RPC workings. go-plugin is working with net/http inside. It also uses something called Yamux for connection multiplexing, but we needn’t worry about this detail.\nImplementing the RPC details looks like this:\n// Here is an implementation that talks over RPC type GreeterRPC struct { client *rpc.Client } func (g *GreeterRPC) Greet() string { var resp string err := g.client.Call(\"Plugin.Greet\", new(interface{}), \u0026resp) if err != nil { // You usually want your interfaces to return errors. If they don't, \t// there isn't much other choice here. \tpanic(err) } return resp } Here the GreeterRPC struct is an RPC specific implementation that will handle communication over RPC. This is Client in this setup.\nIn case of gRPC, this would look something like this:\n// GRPCClient is an implementation of KV that talks over RPC. type GreeterGRPC struct{ client proto.GreeterClient } func (m *GreeterGRPC) Greet() (string, error) { s, err := m.client.Greet(context.Background(), \u0026proto.Empty{}) return s, err } What is happening here? What’s Proto and what is GreeterClient? GRPC uses Google’s protoc library to serialize and unserialize data. proto.GreeterClient is generated Go code by protoc. This code is a skeleton for which implementation detail will be replaced on run time. Well, the actual result will be used and not replaced as such.\nBack to our previous example. The RPC client calls a specific Plugin function called Greet for which the implementation will be provided by a Server that will be streamed back over the RPC protocol.\nThe server is pretty easy to follow:\n// Here is the RPC server that GreeterRPC talks to, conforming to // the requirements of net/rpc type GreeterRPCServer struct { // This is the real implementation \tImpl Greeter } Impl is the concrete implementation that will be called in the Server’s implementation of the Greet plugin. Now we must define Greet on the RPCServer in order for it to be able to call the remote code. This looks like this:\nfunc (s *GreeterRPCServer) Greet(args interface{}, resp *string) error { *resp = s.Impl.Greet() return nil } This is all still boilerplate for the RPC works. Now comes plugin. For this, the comment is actually quite good too:\n// This is the implementation of plugin.Plugin so we can serve/consume this // // This has two methods: Server must return an RPC server for this plugin // type. We construct a GreeterRPCServer for this. // // Client must return an implementation of our interface that communicates // over an RPC client. We return GreeterRPC for this. // // Ignore MuxBroker. That is used to create more multiplexed streams on our // plugin connection and is a more advanced use case. type GreeterPlugin struct { // Impl Injection \tImpl Greeter } func (p *GreeterPlugin) Server(*plugin.MuxBroker) (interface{}, error) { return \u0026GreeterRPCServer{Impl: p.Impl}, nil } func (GreeterPlugin) Client(b *plugin.MuxBroker, c *rpc.Client) (interface{}, error) { return \u0026GreeterRPC{client: c}, nil } What does this mean? So, remember: GreeterRPCServer is the one calling the actual implementation while Client is receiving the result of that call. The GreeterPlugin has the Greeter interface embedded just like the RPCServer. We will use the GreeterPlugin as a struct in the plugin map. This is the plugin that we will actually use.\nThis is all still common stuff. These things will need to be visible for both. The plugin’s implementation will use the interface to see what it needs to implement. The Client will use it see what to call and what API is available. Like, Greet.\nHow does the implementation look like?\nThe Implementation In a completely separate package, but which still has access to the interface definition, this plugin could be something like this:\n// Here is a real implementation of Greeter type GreeterHello struct { logger hclog.Logger } func (g *GreeterHello) Greet() string { g.logger.Debug(\"message from GreeterHello.Greet\") return \"Hello!\" } We create a struct and then add the function to it which is defined by the plugin’s interface. This interface, since it’s required by both parties, could well sit in a common package outside of both programs. Something like a SDK. Both code could import it and use it as a common dependency. This way we have separated the interface from the plugin and the calling client.\nThe main function could look something like this:\nlogger := hclog.New(\u0026hclog.LoggerOptions{ Level: hclog.Trace, Output: os.Stderr, JSONFormat: true, }) greeter := \u0026GreeterHello{ logger: logger, } // pluginMap is the map of plugins we can dispense. var pluginMap = map[string]plugin.Plugin{ \"greeter\": \u0026example.GreeterPlugin{Impl: greeter}, } logger.Debug(\"message from plugin\", \"foo\", \"bar\") plugin.Serve(\u0026plugin.ServeConfig{ HandshakeConfig: handshakeConfig, Plugins: pluginMap, }) Notice two things that we need. One is the handshakeConfig. You can either define it here, with the same cookie details as you defined in the client code, or you can extract the handshake information into the SDK. This is up to you.\nThen the next interesting thing is the plugin.Serve method. This is where the magic happens. The plugins open up a RPC communication socket and over a hijacked stdout, broadcasts its availability to the calling Client in this format:\nCORE-PROTOCOL-VERSION | APP-PROTOCOL-VERSION | NETWORK-TYPE | NETWORK-ADDR | PROTOCOL For Go plugins, you don’t have to concern yourself with this. go-plugin takes care of all this for you. For non-Go versions, we must take this into account. And before calling serve, we need to output this information to stdout.\nFor example, a Python plugin must deal with this himself. Like this:\n# Output information print(\"1|1|tcp|127.0.0.1:1234|grpc\") sys.stdout.flush() For GRPC plugins, it’s also mandatory to implement a HealthChecker.\nHow would all this look like with GRPC?\nIt gets slightly more complicated but not too much. We need to use protoc to create a protocol description for our implementation, and then we will call that. Let’s look at this now by converting the basic greeter example into GRPC.\nGRPC Basic plugin The example that’s under GRPC is quite elaborate and perhaps you don’t need the Python part. I will focus on the basic RPC example into a GRPC example. That should not be a problem.\nThe API First and foremost, you will need to define an API to implement with protoc. For our basic example, the protoc file could look like this:\nsyntax = \"proto3\"; package proto; message GreetResponse { string message = 1; } message Empty {} service GreeterService { rpc Greet(Empty) returns (GreetResponse); } The syntax is quite simple and readable. What this defines is a message, which is a response, that will contain a message with the type string. The service defines a service which has a method called Greet. The service definition is basically an interface for which we will be providing the concrete implementation through the plugin.\nTo read more about protoc, visit this page: Google Protocol Buffer.\nGenerate the code Now, with the protoc definition in hand, we need to generate the stubs that the local client implementation can call. That client call will then, through the remote procedure call, call the right function on the server which will have the concrete implementation at the ready. Run it and return the result in the specified format. Because the stub needs to be available by both parties, (the client AND the server), this needs to live in a shared location.\nWhy? Because the client is calling the stub and the server is implementing the stub. Both need it in order to know what to call/implement.\nTo generate the code, run this command:\nprotoc -I proto/ proto/greeter.proto --go_out=plugins=grpc:proto I encourage you to read the generated code. Much will make little sense at first. It will have a bunch of structs and defined things that the GRPC package will use in order to server the function. The interesting bits and pieces are:\nfunc (m *GreetResponse) GetMessage() string { if m != nil { return m.Message } return \"\" } Which will get use the message inside the response.\ntype GreeterServiceClient interface { Greet(ctx context.Context, in *Empty, opts ...grpc.CallOption) (*GreetResponse, error) } This is our ServiceClient interface which defines the Greet function’s topology.\nAnd lastly, this guy:\nfunc RegisterGreeterServiceServer(s *grpc.Server, srv GreeterServiceServer) { s.RegisterService(\u0026_GreeterService_serviceDesc, srv) } Which we will need in order to register our implementation for the server. We can ignore the rest.\nThe interface Much like the RPC, we need to define an interface for the client and server to use. This must be in a shared place as both the server and the client need to know about it. You could put this into an SDK and your peers could just get the SDK and implement some function for define and done. The interface definition in the GRPC land could look something like this:\n// Greeter is the interface that we're exposing as a plugin. type Greeter interface { Greet() string } // This is the implementation of plugin.GRPCPlugin so we can serve/consume this. type GreeterGRPCPlugin struct { // GRPCPlugin must still implement the Plugin interface \tplugin.Plugin // Concrete implementation, written in Go. This is only used for plugins \t// that are written in Go. \tImpl Greeter } func (p *GreeterGRPCPlugin) GRPCServer(broker *plugin.GRPCBroker, s *grpc.Server) error { proto.RegisterGreeterServer(s, \u0026GRPCServer{Impl: p.Impl}) return nil } func (p *GreeterGRPCPlugin) GRPCClient(ctx context.Context, broker *plugin.GRPCBroker, c *grpc.ClientConn) (interface{}, error) { return \u0026GRPCClient{client: proto.NewGreeterClient(c)}, nil } With this we have the Plugin’s implementation for hashicorp what needed to be done. The plugin will call the underlying implementation and serve/consume the plugin. We can now write the GRPC part of it.\nPlease note that proto is a shared library too where the protocol stubs reside. That needs to be somewhere on the path or in a separate SDK of some sort, but it must be visible.\nWriting the GRPC Client Firstly we define the grpc client struct:\n// GRPCClient is an implementation of Greeter that talks over RPC. type GRPCClient struct{ client proto.GreeterClient } Then we define how the client will call the remote function:\nfunc (m *GRPCClient) Greet() string { ret, _ := m.client.Greet(context.Background(), \u0026proto.Empty{}) return ret.Message } This will take the client in the GRPCClient and will call the method on it. Once that’s done we will return to the result Message property which will be Hello!. proto.Empty is an empty struct; we use this if there is no parameter for a defined method or no return value. We can’t just leave it blank. protoc needs to be told explicitly that there is no parameter or return value.\nWriting the GRPC Server The server implementation will also be similar. We call Impl here which will have our concrete plugin implementation.\n// Here is the gRPC server that GRPCClient talks to. type GRPCServer struct { // This is the real implementation \tImpl Greeter } func (m *GRPCServer) Greet( ctx context.Context, req *proto.Empty) *proto.GreeterResponse { v := m.Impl.Greet() return \u0026proto.GreeterResponse{Message: v} } And we will use the protoc defined message response. v will have the response from Greet which will be Hello! provided by the concrete plugin’s implementation. We then transform that into a protoc type by setting the Message property on the GreeterResponse struct provided by the automatically generated protoc stub code.\nEasy, right?\nWriting the plugin itself The whole thing looks much like the RPC implementation with just a few small modifications and changes. This can sit completely outside of everything, or can even be provided by a third party implementor.\n// Here is a real implementation of KV that writes to a local file with // the key name and the contents are the value of the key. type Greeter struct{} func (Greeter) Greet() error { return \"Hello!\" } func main() { plugin.Serve(\u0026plugin.ServeConfig{ HandshakeConfig: shared.Handshake, Plugins: map[string]plugin.Plugin{ \"greeter\": \u0026shared.GreeterGRPCPlugin{Impl: \u0026Greeter{}}, }, // A non-nil value here enables gRPC serving for this plugin... \tGRPCServer: plugin.DefaultGRPCServer, }) } Calling it all in the main Once all that is done, the main function looks the same as RPC’s main but with some small modifications.\n// We're a host. Start by launching the plugin process. \tclient := plugin.NewClient(\u0026plugin.ClientConfig{ HandshakeConfig: shared.Handshake, Plugins: shared.PluginMap, Cmd: exec.Command(\"./plugin/greeter\"), AllowedProtocols: []plugin.Protocol{plugin.ProtocolGRPC}, }) The NewClient now defines AllowedProtocols to be ProtocolGRPC. The rest is the same as before calling Dispense and value hinting the plugin to the correct type then calling Greet().\nConclusion This is it. We made it! Now our plugin works over GRPC with a defined API by protoc. The plugin’s implementation can live where ever we want it to, but it needs some shared data. These are:\n The generated code by protoc The defined plugin interface The GRPC Server and Client  These need to be visible by both the Client and the Server. The Server here is the plugin. If you are planning on making people be able to extend your application with go-plugin, you should make these available as a separate SDK. So people won’t have to include your whole project just to implement an interface and use protoc. In fact, you could also extract the protoc definition into a separate repository so that your SDK can also pull it in.\nYou will have three repositories:\n Your application; The SDK providing the interface and the GRPC Server and Client implementation; The protoc definition file and generated skeleton ( for Go based plugins).  Other languages will have to generate their own protoc code, and includ it into the plugin; like the Python implementation example located here: Go-plugin Python Example. Also, read this documentation carefully: non-go go-plugin. This document will also clarify what 1|1|tcp|127.0.0.1:1234|grpc means and will dissipate the confusion around how plugins work.\nLastly, if you would like to have an in-depth explanation about how go-plugin came to be, watch this video by Mitchell:\ngo-plugin explanation video.\nI must warn you though- it’s an hour long. But worth the watch!\nThat’s it. I hope this has helped to clear the confusion around how to use go-plugin.\nHappy plugging!\nGergely.\n","title":"Extensive tutorial on go-plugin.","uri":"/2018/10/29/go-plugin-tutorial/"},{"content":"Hi.\nA quick update, but a very important and interesting one hopefully. Furnace just got a massive boost to its plugin system.\nI’m using HashiCorp’s Go-Plugins system now to handle plugins. This means one of two things that are interesting to the plugin author.\nOne, plugins can be written in any language which is supported by Furnace and supports GRPC. Currently this means that plugins can be written in the following languages:\n  Go\n  Python\n  Ruby\n  Adding new plugins is easy and I’m open for suggestions in which language to provide next if the need arrises.\nTo find out more, please read the README on Furnace about plugins located here: Furnace Plugin System.\nI hope to see a bunch of nice plugins pop up here and there if please are interested in writing them. I’m listing a couple of possibilities like, notification after create, or resource cleanup or even preventing the stack from creating in the first place with a pre-create check for permissions / resource availability / funds constraints.\nHave fun writing plugins and making Furnace more powerful then ever.\nI’m planning on providing some basic plugins that could be used out of the box. Those will probably be in Go though.\nThanks, Gergely.\n","title":"Furnace with a new Plugin System","uri":"/2018/09/17/furnace-plugin-update/"},{"content":"Hi.\nToday, I would like to write about a little tool I put together in Rust.\nIt’s called gotp. I’m calling it gotp mainly because of crates.io. I published it there as well, you can find it under this link: crates.io/gotp.\nThe purpose is clear. It’s a totp generator I wrote a while ago in C++ but now it’s in rust. It can generate a token and save it into an account file that is AES encrypted. The password is never saved, so it’s secure enough to use it.\nOne of it’s properties it will have over the c++ implementation is it’s safe, it uses a proper IV and once I’m done, it will also support encryption via PGP.\nThat way a password will no longer be asked, but the gpg-agent will be used instead.\nThis will give a much needed usability boost in which a password will no longer be asked for constantly.\nThere is also a possibility to place the account file into DropBox or Google Drive and share it between your own devices. This way your home laptop will also have the same account as your work laptop making it essentially an Authy like service on the CLI.\nEnjoy, and keep an eye out for the PGP update. For more information in usage, please read the README.\nThank you! Gergely.\n","title":"TOTP generator with account handling for multiple tokens","uri":"/2018/09/13/gotp/"},{"content":"Hi folks.\nToday’s is a quick tip for keeping your forks updated.\nIf you are like me, and have at least a 100 forks in your repository because: * You would like to contribute at some point * Save it for yourself because you are afraid that it disappears * Would like to make modifications for your own benefit * Whatever the reason\n…then you probably have a lot of trouble keeping them updated and making sure you always see the latest change.\nUpstream can change a lot especially if it’s a busy repository.\nFret not. Help is here. This little ruby script will solve your troubles:\n#!/usr/bin/env ruby require 'octokit' require 'logger' @logger = Logger.new(\"output.log\") def update_fork(repo) repo_name = repo.name # clone the repository -- octokit doesn't provide this feature as it's a github api library @logger.info(\"cloning into #{repo.ssh_url}\") system(\"git clone #{repo.ssh_url}#{repo_name}\") # setup upstream for updating @logger.info(\"setup upstream to #{repo.parent.ssh_url}\") system(\"cd #{repo_name}\u0026\u0026 git remote add upstream #{repo.parent.ssh_url}\") # do the update @logger.info(\"doing the update with push\") system(\"cd #{repo_name}\u0026\u0026 git fetch upstream \u0026\u0026 git rebase upstream/master \u0026\u0026 git push origin\") ensure # ensure that the folder is cleaned up @logger.info(\"cleanup: removing the repository folder\") system(\"rm -fr #{repo_name}\") end client = Octokit::Client.new(:access_token =\u003e ENV['GIT_TOKEN'], per_page: 100) repos = client.repos({}, query: {type: 'owner', sort: 'asc'}) # Go through all the pages and add them to the list of repositories. repos.concat(client.last_response.rels[:next].get.data) repos = repos.select{ |r| r.fork } @logger.info(\"going to update '#{repos.length}' repositories\") repos.each do |repo| # get the repositories information @logger.info(\"updating #{repo.name}\") r = client.repository(repo.id) update_fork(r) end This script is also available as a Gist located here.\nPut this into a cron job, or a Jenkins job on a schedule and you should be good to go.\nNote two things: First: ENV['GIT_TOKEN'] this should be set to a token which you can acquire by navigating to tokens. Add a token which has repo access.\nSecond: Obviously this script will push to your local repository. So wherever you run this, make sure git is set-up and can push to your repository via SSH. This script is using ssh_url for the repositories. It won’t ask for a username or a password.\nThat’s it. Enjoy and keep updating.\nThanks for reading\nGergely.\n","title":"Keep your git forks updated all the time","uri":"/2018/06/08/fork-updater/"},{"content":"Intro Alright folks. Settle in and get comfortable. This is going to be a long, but hopefully, fun ride.\nI’m going to deploy a distributed application with Kubernetes. I attempted to create an application that I thought resembled a real world app. Obviously I had to cut some corners due to time and energy constraints.\nMy focus will be on Kubernetes and deployment.\nShall we delve right in?\nThe Application TL;DR The application itself consists of six parts. The repository can be found here: Kube Cluster Sample.\nIt’s a face recognition service which identifies images of people, comparing them to known individuals. A simple frontend displays a table of these images whom they belong to. This happens by sending a request to a receiver. The request contains a path to an image. This image can sit on an NFS somewhere. The receiver stores this path in the DB (MySQL) and sends a processing request to a queue. The queue uses: NSQ. The request contains the ID of the saved image.\nAn Image Processing service is constantly monitoring the queue for jobs to do. The processing consists of the following steps: taking the ID; loading the image; and finally, sending the image to a face recognition backend written in Python via gRPC. If the identification is successful, the backend will return the name of the image corresponding to that person. The image_processor then updates the image’s record with the person’s ID and marks the image as “processed successfully”. If identification is unsuccessful, the image will be left as “pending”. If there was a failure during identification, the image will be flagged as “failed”.\nFailed images can be retried with a cron job, for example:\nSo how does this all work? Let’s check it out .\nReceiver The receiver service is the starting point of the process. It’s an API which receives a request in the following format:\ncurl -d '{\"path\":\"/unknown_images/unknown0001.jpg\"}' http://127.0.0.1:8000/image/post In this instance, the receiver stores the path using a shared database cluster. The entity will then receive an ID from the database service. This application is based on the model where unique identification for Entity Objects is provided by the persistence layer. Once the ID is procured, the receiver will send a message to NSQ. At this point in the process, the receiver’s job is done.\nImage Processor Here is where the excitement begins. When Image Processor first runs it creates two Go routines. These are…\nConsume This is an NSQ consumer. It has three integral jobs. Firstly, it listens for messages on the queue. Secondly, when there is a message, it appends the received ID to a thread safe slice of IDs that the second routine processes. And lastly, it signals the second routine that there is work to be do. It does this through sync.Condition.\nProcessImages This routine processes a slice of IDs until the slice is drained completely. Once the slice is drained, the routine suspends instead of sleep-waiting on a channel. The processing of a single ID can be seen in the following linear steps:\n Establish a gRPC connection to the Face Recognition service (explained under Face Recognition) Retrieve the image record from the database Setup two functions for the Circuit Breaker  Function 1: The main function which runs the RPC method call Function 2: A health check for the Ping of the circuit breaker   Call Function 1 which sends the path of the image to the face recognition service. This path should be accessible by the face recognition service. Preferably something shared like an NFS If this call fails, update the image record as FAILED PROCESSING If it succeeds, an image name should come back which corresponds to a person in the db. It runs a joined SQL query which gets the corresponding person’s ID Update the Image record in the database with PROCESSED status and the ID of the person that image was identified as  This service can be replicated. In other words, more than one can run at the same time.\nCircuit Breaker A system in which replicating resources requires little to no effort, there still can be cases where, for example, the network goes down, or there are communication problems of any kind between two services. I like to implement a little circuit breaker around the gRPC calls for fun.\nThis is how it works:\nAs you can see, once there are 5 unsuccessful calls to the service, the circuit breaker activates, not allowing any more calls to go through. After a configured amount of time, it will send a Ping call to the service to see if it’s back up. If that still errors out, it will increase the timeout. If not, it opens the circuit, allowing traffic to proceed.\nFront-End This is only a simple table view with Go’s own html/template used to render a list of images.\nFace Recognition Here is where the identification magic happens. I decided to make this a gRPC based service for the sole purpose of its flexibility. I started writing it in Go but decided that a Python implementation would be much sorter. In fact, excluding the gRPC code, the recognition part is approximately 7 lines of Python code. I’m using this fantastic library which contains all the C bindings to OpenCV. Face Recognition. Having an API contract here means that I can change the implementation anytime as long as it adheres to the contract.\nPlease note that there exist a great Go library OpenCV. I was about to use it but they had yet to write the C bindings for that part of OpenCV. It’s called GoCV. Check them out! They have some pretty amazing things, like real-time camera feed processing that only needs a couple of lines of code.\nThe python library is simple in nature. Have a set of images of people you know. I have a folder with a couple of images named, hannibal_1.jpg, hannibal_2.jpg, gergely_1.jpg, john_doe.jpg. In the database I have two tables named, person, person_images. They look like this:\n+----+----------+ | id | name | +----+----------+ | 1 | Gergely | | 2 | John Doe | | 3 | Hannibal | +----+----------+ +----+----------------+-----------+ | id | image_name | person_id | +----+----------------+-----------+ | 1 | hannibal_1.jpg | 3 | | 2 | hannibal_2.jpg | 3 | +----+----------------+-----------+ The face recognition library returns the name of the image from the known people which matches the person on the unknown image. After that, a simple joined query -like this- will return the person in question.\nselect person.name, person.id from person inner join person_images as pi on person.id = pi.person_id where image_name = 'hannibal_2.jpg'; The gRPC call returns the ID of the person which is then used to update the image’s ‘person` column.\nNSQ NSQ is a nice little Go based queue. It can be scaled and has a minimal footprint on the system. It also has a lookup service that consumers use to receive messages, and a daemon that senders use when sending messages.\nNSQ’s philosophy is that the daemon should run with the sender application. That way, the sender will send to the localhost only. But the daemon is connected to the lookup service, and that’s how they achieve a global queue.\nThis means that there are as many NSQ daemons deployed as there are senders. Because the daemon has a minuscule resource requirement, it won’t interfere with the requirements of the main application.\nConfiguration In order to be as flexible as possible, as well as making use of Kubernetes’s ConfigSet, I’m using .env files in development to store configurations like the location of the database service, or NSQ’s lookup address. In production- and that means the Kubernetes’s environment- I’ll use environment properties.\nConclusion for the Application And that’s all there is to the architecture of the application we are about to deploy. All of its components are changeable and coupled only through the database, a queue and gRPC. This is imperative when deploying a distributed application due to how updating mechanics work. I will cover that part in the Deployment section.\nDeployment with Kubernetes Basics What is Kubernetes?\nI’m going to cover some of the basics here. I won’t go too much into detail- that would require a whole book like this one: Kubernetes Up And Running. Also, if you’re daring enough, you can have a look through this documentation: Kubernetes Documentation.\nKubernetes is a containerized service and application manager. It scales easily, employs a swarm of containers, and most importantly, it’s highly configurable via yaml based template files. People often compare Kubernetes to Docker swarm, but Kubernetes does way more than that! For example: it’s container agnostic. You could use LXC with Kubernetes and it would work the same way as you using it with Docker. It provides a layer above managing a cluster of deployed services and applications. How? Let’s take a quick look at the building blocks of Kubernetes.\nIn Kubernetes, you’ll describe a desired state of the application and Kubernetes will do what it can to reach that state. States could be something such as deployed; paused; replicated twice; and so on and so forth.\nOne of the basics of Kubernetes is that it uses Labels and Annotations for all of its components. Services, Deployments, ReplicaSets, DaemonSets, everything is labelled. Consider the following scenario. In order to identify what pod belongs to what application, a label is used called app: myapp. Let’s assume you have two containers of this application deployed; if you would remove the label app from one of the containers, Kubernetes would only detect one and thus would launch a new instance of myapp.\nKubernetes Cluster For Kuberenetes to work, a Kubernetes cluster needs to be present. Setting that up might be a tad painful, but luckily, help is on hand. Minikube sets up a cluster for us locally with one Node. And AWS has a beta service running in the form of a Kubernetes cluster in which the only thing you need to do is request nodes and define your deployments. The Kubernetes cluster components are documented here: Kubernetes Cluster Components.\nNodes A Node is a worker machine. It can be anything- from a vm to a physical machine- including all sorts of cloud provided vms.\nPods Pods are a logically grouped collection of containers, meaning one Pod can potentially house a multitude of containers. A Pod gets its own DNS and virtual IP address after it has been created so Kubernetes can load balancer traffic to it. You rarely need to deal with containers directly. Even when debugging, (like looking at logs), you usually invoke kubectl logs deployment/your-app -f instead of looking at a specific container. Although it is possible with -c container_name. The -f does a tail on the log.\nDeployments When creating any kind of resource in Kubernetes, it will use a Deployment in the background. A deployment describes a desired state of the current application. It’s an object you can use to update Pods or a Service to be in a different state, do an update, or rollout new version of your app. You don’t directly control a ReplicaSet, (as described later), but control the deployment object which creates and manages a ReplicaSet.\nServices By default a Pod will get an IP address. However, since Pods are a volatile thing in Kubernetes, you’ll need something more permanent. A queue, mysql, or an internal API, a frontend; these need to be long running and behind a static, unchanging IP or preferably a DNS record.\nFor this purpose, Kubernetes has Services for which you can define modes of accessibility. Load Balanced, simple IP or internal DNS.\nHow does Kubernetes know if a service is running correctly? You can configure Health Checks and Availability Checks. A Health Check will check whether a container is running, but that doesn’t mean that your service is running. For that, you have the availability check which pings a different endpoint in your application.\nSince Services are pretty important, I recommend that you read up on them later here: Services. Advanced warning though, this document is quite dense. Twenty four A4 pages of networking, services and discovery. It’s also vital to decide whether you want to seriously employ Kubernetes in production.\nDNS / Service Discovery If you create a service in the cluster, that service will get a DNS record in Kubernetes provided by special Kubernetes deployments called kube-proxy and kube-dns. These two provide service discover inside a cluster. If you have a mysql service running and set clusterIP: none, then everyone in the cluster can reach that service by pinging mysql.default.svc.cluster.local. Where:\n mysql – is the name of the service default – is the namespace name svc – is services cluster.local – is a local cluster domain  The domain can be changed via a custom definition. To access a service outside the cluster, a DNS provider has to be used, and Nginx (for example), to bind an IP address to a record. The public IP address of a service can be queried with the following commands:\n NodePort – kubectl get -o jsonpath=\"{.spec.ports[0].nodePort}\" services mysql LoadBalancer – kubectl get -o jsonpath=\"{.spec.ports[0].LoadBalancer}\" services mysql  Template Files Like Docker Compose, TerraForm or other service management tools, Kubernetes also provides infrastructure describing templates. What that means is that you rarely need to do anything by hand.\nFor example, consider the following yaml template which describes an nginx Deployment:\napiVersion: apps/v1 kind: Deployment #(1) metadata: #(2) name: nginx-deployment labels: #(3) app: nginx spec: #(4) replicas: 3 #(5) selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: #(6) - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 This is a simple deployment in which we do the following:\n (1) Define the type of the template with kind (2) Add metadata that will identify this deployment and every resource that it would create with a label (3) (4) Then comes the spec which describes the desired state  (5) For the nginx app, have 3 replicas (6) This is the template definition for the containers that this Pod will contain  nginx named container nginx:1.7.9 image (docker in this case) exposed ports      ReplicaSet A ReplicaSet is a low level replication manager. It ensures that the correct number of replicates are running for a application. However, Deployments are at a higher level and should always manage ReplicaSets. You rarely need to use ReplicaSets directly unless you have a fringe case in which you want to control the specifics of replication.\nDaemonSet Remember how I said Kubernetes is using Labels all the time? A DaemonSet is a controller that ensures that at daemonized application is always running on a node with a certain label.\nFor example: you want all the nodes labelled with logger or mission_critical to run an logger / auditing service daemon. Then you create a DaemonSet and give it a node selector called logger or mission_critical. Kubernetes will look for a node that has that label. Always ensure that it will have an instance of that daemon running on it. Thus everyone running on that node will have access to that daemon locally.\nIn case of my application, the NSQ daemon could be a DaemonSet. Make sure it’s up on a node which has the receiver component running by labelling a node with receiver and specifying a DaemonSet with a receiver application selector.\nThe DaemonSet has all the benefits of the ReplicaSet. It’s scalable and Kubernetes manages it; which means, all life cycle events are handled by Kube ensuring it never dies, and when it does, it will be immediately replaced.\nScaling It’s trivial to scale in Kubernetes. The ReplicaSets take care of the number of instances of a Pod to run- as seen in the nginx deployment with the setting replicas:3. It’s up to us to write our application in a way that allows Kubernetes to run multiple copies of it.\nOf course the settings are vast. You can specify which replicates must run on what Nodes, or on various waiting times as to how long to wait for an instance to come up. You can read more on this subject here: Horizontal Scaling and here: Interactive Scaling with Kubernetes and of course the details of a ReplicaSet which controls all the scaling made possible in Kubernetes.\nConclusion for Kubernetes It’s a convenient tool to handle container orchestration. Its unit of work are Pods and it has a layered architecture. The top level layer is Deployments through which you handle all other resources. It’s highly configurable. It provides an API for all calls you make, so potentially, instead of running kubectl you can also write your own logic to send information to the Kubernetes API.\nIt provides support for all major cloud providers natively by now and it’s completely open source. Feel free to contribute! And check the code if you would like to have a deeper understanding on how it works: Kubernetes on Github.\nMinikube I’m going to use Minikube. Minikube is a local Kubernetes cluster simulator. It’s not great in simulating multiple nodes though, but for starting out and local play without any costs, it’s great. It uses a VM that can be fine tuned if necessary using VirtualBox and the likes.\nAll of the kube template files that I’ll be using can be found here: Kube files.\nNOTE If, later on, you would like to play with scaling but notice that the replicates are always in Pending state, remember that minikube employs a single node only. It might not allow multiple replicas on the same node, or just plainly ran out of resources to use. You can check available resources with the following command:\nkubectl get nodes -o yaml Building the containers Kubernetes supports most of the containers out there. I’m going to use Docker. For all the services I’ve built, there is a Dockerfile included in the repository. I encourage you to study them. Most of them are simple. For the go services, I’m using a multi stage build that has been recently introduced. The Go services are Alpine Linux based. The Face Recognition service is Python. NSQ and MySQL are using their own containers.\nContext Kubernetes uses namespaces. If you don’t specify any, it will use the default namespace. I’m going to permanently set a context to avoid polluting the default namespace. You do that like this:\n❯ kubectl config set-context kube-face-cluster --namespace=face Context \"kube-face-cluster\" created. You have to also start using the context once it’s created, like so:\n❯ kubectl config use-context kube-face-cluster Switched to context \"kube-face-cluster\". After this, all kubectl commands will use the namespace face.\nDeploying the Application Overview of Pods and Services:\nMySQL The first Service I’m going to deploy is my database.\nI’m using the Kubernetes example located here Kube MySQL which fits my needs. Please note that this file is using a plain password for MYSQL_PASSWORD. I’m going to employ a vault as described here Kubernetes Secrets.\nI’ve created a secret locally as described in that document using a secret yaml:\napiVersion: v1 kind: Secret metadata: name: kube-face-secret type: Opaque data: mysql_password: base64codehere I created the base64 code via the following command:\necho -n \"ubersecurepassword\" | base64 And, this is what you’ll see in my deployment yaml file:\n... - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: kube-face-secret key: mysql_password ... Another thing worth mentioning: It’s using a volume to persist the database. The volume definition is as follows:\n... volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql ... volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pv-claim ... presistentVolumeClain is key here. This tells Kubernetes that this resource requires a persistent volume. How it’s provided is abstracted away from the user. You can be sure that Kubernetes will provide a volume that will always be there. It is similar to Pods. To read up on the details, check out this document: Kubernetes Persistent Volumes.\nDeploying the mysql Service is done with the following command:\nkubectl apply -f mysql.yaml apply vs create. In short, apply is considered a declarative object configuration command while create is imperative. What this means for now is that ‘create’ is usually for a one of tasks, like running something or creating a deployment. While, when using apply, the user doesn’t define the action to be taken. That will be defined by Kubernetes based on the current status of the cluster. Thus, when there is no service called mysql and I’m calling apply -f mysql.yaml it will create the service. When running again, Kubernetes won’t do anything. But if I would run create again it will throw an error saying the service is already created.\nFor more information, check out the following docs: Kubernetes Object Management, Imperative Configuration, Declarative Configuration.\nTo see progress information, run:\n# Describes the whole process kubectl describe deployment mysql # Shows only the pod kubectl get pods -l app=mysql Output should be similar to this:\n... Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u003cnone\u003e NewReplicaSet: mysql-55cd6b9f47 (1/1 replicas created) ... Or in case of get pods:\nNAME READY STATUS RESTARTS AGE mysql-78dbbd9c49-k6sdv 1/1 Running 0 18s To test the instance, run the following snippet:\nkubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -pyourpasswordhere GOTCHA: If you change the password now, it’s not enough to re-apply your yaml file to update the container. Since the DB is persisted, the password will not be changed. You have to delete the whole deployment with kubectl delete -f mysql.yaml.\nYou should see the following when running a show databases.\nIf you don't see a command prompt, try pressing enter. mysql\u003e mysql\u003e mysql\u003e show databases; +--------------------+ | Database | +--------------------+ | information_schema | | kube | | mysql | | performance_schema | +--------------------+ 4 rows in set (0.00 sec) mysql\u003e exit Bye You’ll also notice that I’ve mounted a file located here: Database Setup SQL into the container. MySQL container automatically executes these. That file will bootstrap some data and the schema I’m going to use.\nThe volume definition is as follows:\nvolumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql - name: bootstrap-script mountPath: /docker-entrypoint-initdb.d/database_setup.sql volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pv-claim - name: bootstrap-script hostPath: path: /Users/hannibal/golang/src/github.com/Skarlso/kube-cluster-sample/database_setup.sql type: File To check if the bootstrap script was successful, run this:\n~/golang/src/github.com/Skarlso/kube-cluster-sample/kube_files master* ❯ kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -uroot -pyourpasswordhere kube If you don't see a command prompt, try pressing enter. mysql\u003e show tables; +----------------+ | Tables_in_kube | +----------------+ | images | | person | | person_images | +----------------+ 3 rows in set (0.00 sec) mysql\u003e This concludes the database service setup. Logs for this service can be viewed with the following command:\nkubectl logs deployment/mysql -f NSQ Lookup The NSQ Lookup will run as an internal service. It doesn’t need access from the outside, so I’m setting clusterIP: None which will tell Kubernetes that this service is a headless service. This means that it won’t be load balanced, and it won’t be a single IP service. The DNS will be based upon service selectors.\nOur NSQ Lookup selector is:\nselector: matchLabels: app: nsqlookup Thus, the internal DNS will look like this: nsqlookup.default.svc.cluster.local.\nHeadless services are described in detail here: Headless Service.\nBasically it’s the same as MySQL, just with slight modifications. As stated earlier, I’m using NSQ’s own Docker Image called nsqio/nsq. All nsq commands are there, so nsqd will also use this image just with a different command. For nsqlookupd, the command is:\ncommand: [\"/nsqlookupd\"] args: [\"--broadcast-address=nsqlookup.default.svc.cluster.local\"] What’s the --broadcast-address for, you might ask? By default, nsqlookup will use the hostname as broadcast address. When the consumer runs a callback it will try to connect to something like: http://nsqlookup-234kf-asdf:4161/lookup?topics=image. Please note that nsqlookup-234kf-asdf is the hostname of the container. By setting the broadcast-address to the internal DNS, the callback will be: http://nsqlookup.default.svc.cluster.local:4161/lookup?topic=images. Which will work as expected.\nNSQ Lookup also requires two ports forwarded: One for broadcasting and one for nsqd callback. These are exposed in the Dockerfile, and then utilized in the Kubernetes template. Like this:\nIn the container template:\nports: - containerPort: 4160 hostPort: 4160 - containerPort: 4161 hostPort: 4161 In the service template:\nspec: ports: - name: tcp protocol: TCP port: 4160 targetPort: 4160 - name: http protocol: TCP port: 4161 targetPort: 4161 Names are required by Kubernetes.\nTo create this service, I’m using the same command as before:\nkubectl apply -f nsqlookup.yaml This concludes nsqlookupd. Two of the major players are in the sack!\nReceiver This is a more complex one. The receiver will do three things:\n Create some deployments; Create the nsq daemon; Expose the service to the public.  Deployments The first deployment it creates is its own. The receiver’s container is skarlso/kube-receiver-alpine.\nNsq Daemon The receiver starts an nsq daemon. As stated earlier, the receiver runs an nsqd with it-self. It does this so talking to it can happen locally and not over the network. By making the receiver do this, they will end up on the same node.\nNSQ daemon also needs some adjustments and parameters.\nports: - containerPort: 4150 hostPort: 4150 - containerPort: 4151 hostPort: 4151 env: - name: NSQLOOKUP_ADDRESS value: nsqlookup.default.svc.cluster.local - name: NSQ_BROADCAST_ADDRESS value: nsqd.default.svc.cluster.local command: [\"/nsqd\"] args: [\"--lookupd-tcp-address=$(NSQLOOKUP_ADDRESS):4160\", \"--broadcast-address=$(NSQ_BROADCAST_ADDRESS)\"] You can see that the lookup-tcp-address and the broadcast-address are set. Lookup tcp address is the DNS for the nsqlookupd service. And the broadcast address is necessary, just like with nsqlookupd, so the callbacks are working properly.\nPublic facing Now, this is the first time I’m deploying a public facing service. There are two options. I could use a LoadBalancer since this API will be under heavy load. And if this would be deployed anywhere in production, then it should be using one.\nI’m doing this locally though- with one node- so something called a NodePort is enough. A NodePort exposes a service on each node’s IP at a static port. If not specified, it will assign a random port on the host between 30000-32767. But it can also be configured to be a specific port, using nodePort in the template file. To reach this service, use \u003cNodeIP\u003e:\u003cNodePort\u003e. If more than one node is configured, a LoadBalancer can multiplex them to a single IP.\nFor further information, check out this document: Publishing Services.\nPutting this all together, we’ll get a receiver-service for which the template for is as follows:\napiVersion: v1 kind: Service metadata: name: receiver-service spec: ports: - protocol: TCP port: 8000 targetPort: 8000 selector: app: receiver type: NodePort For a fixed nodePort on 8000 a definition of nodePort must be provided:\napiVersion: v1 kind: Service metadata: name: receiver-service spec: ports: - protocol: TCP port: 8000 targetPort: 8000 selector: app: receiver type: NodePort nodePort: 8000 Image processor The Image Processor is where I’m handling passing off images to be identified. It should have access to nsqlookupd, mysql and the gRPC endpoint of the face recognition service. This is actually quite a boring service. In fact, it’s not even a service at all. It doesn’t expose anything, and thus it’s the first deployment only component. For brevity, here is the whole template:\n--- apiVersion: apps/v1 kind: Deployment metadata: name: image-processor-deployment spec: selector: matchLabels: app: image-processor replicas: 1 template: metadata: labels: app: image-processor spec: containers: - name: image-processor image: skarlso/kube-processor-alpine:latest env: - name: MYSQL_CONNECTION value: \"mysql.default.svc.cluster.local\" - name: MYSQL_USERPASSWORD valueFrom: secretKeyRef: name: kube-face-secret key: mysql_userpassword - name: MYSQL_PORT # TIL: If this is 3306 without \" kubectl throws an error. value: \"3306\" - name: MYSQL_DBNAME value: kube - name: NSQ_LOOKUP_ADDRESS value: \"nsqlookup.default.svc.cluster.local:4161\" - name: GRPC_ADDRESS value: \"face-recog.default.svc.cluster.local:50051\" The only interesting points in this file are the multitude of environment properties that are used to configure the application. Note the nsqlookupd address and the grpc address.\nTo create this deployment, run:\nkubectl apply -f image_processor.yaml Face - Recognition The face recognition service does have a service. It’s a simple one. Only needed by image-processor. It’s template is as follows:\napiVersion: v1 kind: Service metadata: name: face-recog spec: ports: - protocol: TCP port: 50051 targetPort: 50051 selector: app: face-recog clusterIP: None The more interesting part is that it requires two volumes. The two volumes are known_people and unknown_people. Can you guess what they will contain? Yep, images. The known_people volume contains all the images associated to the known people in the database. The unknown_people volume will contain all new images. And that’s the path we will need to use when sending images from the receiver; that is wherever the mount point points too, which in my case is /unknown_people. Basically, the path needs to be one that the face recognition service can access.\nNow, with Kubernetes and Docker, this is easy. It can be a mounted S3 or some kind of nfs or a local mount from host to guest. The possibilities are endless (around a dozen or so). I’m going to use a local mount for the sake of simplicity.\nMounting a volume is done in two parts. Firstly, the Dockerfile has to specify volumes:\nVOLUME [ \"/unknown_people\", \"/known_people\" ]Secondly, the Kubernetes template needs add volumeMounts as seen in the MySQL service; the difference being hostPath instead of claimed volume:\nvolumeMounts: - name: known-people-storage mountPath: /known_people - name: unknown-people-storage mountPath: /unknown_people volumes: - name: known-people-storage hostPath: path: /Users/hannibal/Temp/known_people type: Directory - name: unknown-people-storage hostPath: path: /Users/hannibal/Temp/ type: Directory We also need to set the known_people folder config setting for the face recognition service. This is done via an environment property:\nenv: - name: KNOWN_PEOPLE value: \"/known_people\" Then the Python code will look up images, like this:\nknown_people = os.getenv('KNOWN_PEOPLE', 'known_people') print(\"Known people images location is: %s\" % known_people) images = self.image_files_in_folder(known_people) Where image_files_in_folder is:\ndef image_files_in_folder(self, folder): return [os.path.join(folder, f) for f in os.listdir(folder) if re.match(r'.*\\.(jpg|jpeg|png)', f, flags=re.I)] Neat.\nNow, if the receiver receives a request (and sends it off further down the line) similar to the one below…\ncurl -d '{\"path\":\"/unknown_people/unknown220.jpg\"}' http://192.168.99.100:30251/image/post …it will look for an image called unknown220.jpg under /unknown_people, locate an image in the known_folder that corresponds to the person in the unknown image and return the name of the image that matches.\nLooking at logs, you should see something like this:\n# Receiver ❯ curl -d '{\"path\":\"/unknown_people/unknown219.jpg\"}' http://192.168.99.100:30251/image/post got path: {Path:/unknown_people/unknown219.jpg} image saved with id: 4 image sent to nsq # Image Processor 2018/03/26 18:11:21 INF 1 [images/ch] querying nsqlookupd http://nsqlookup.default.svc.cluster.local:4161/lookup?topic=images 2018/03/26 18:11:59 Got a message: 4 2018/03/26 18:11:59 Processing image id: 4 2018/03/26 18:12:00 got person: Hannibal 2018/03/26 18:12:00 updating record with person id 2018/03/26 18:12:00 done And that concludes all of the services that we need to deploy.\nFrontend Lastly, there is a small web-app which displays the information in the db for convenience. This is also a public facing service with the same parameters as the receiver.\nIt looks like this:\nRecap We are now at the point in which I’ve deployed a bunch of services. A recap off the commands I’ve used so far:\nkubectl apply -f mysql.yaml kubectl apply -f nsqlookup.yaml kubectl apply -f receiver.yaml kubectl apply -f image_processor.yaml kubectl apply -f face_recognition.yaml kubectl apply -f frontend.yaml These could be in any order since the application does not allocate connections on start. (Except for image_processor’s NSQ consumer. But that re-tries.)\nQuery-ing kube for running pods with kubectl get pods should show something like this if there were no errors:\n❯ kubectl get pods NAME READY STATUS RESTARTS AGE face-recog-6bf449c6f-qg5tr 1/1 Running 0 1m image-processor-deployment-6467468c9d-cvx6m 1/1 Running 0 31s mysql-7d667c75f4-bwghw 1/1 Running 0 36s nsqd-584954c44c-299dz 1/1 Running 0 26s nsqlookup-7f5bdfcb87-jkdl7 1/1 Running 0 11s receiver-deployment-5cb4797598-sf5ds 1/1 Running 0 26s Running minikube service list:\n❯ minikube service list |-------------|----------------------|-----------------------------| | NAMESPACE | NAME | URL | |-------------|----------------------|-----------------------------| | default | face-recog | No node port | | default | kubernetes | No node port | | default | mysql | No node port | | default | nsqd | No node port | | default | nsqlookup | No node port | | default | receiver-service | http://192.168.99.100:30251 | | kube-system | kube-dns | No node port | | kube-system | kubernetes-dashboard | http://192.168.99.100:30000 | |-------------|----------------------|-----------------------------| Rolling update What happens during a rolling update?\nAs it happens during software development, change is requested/needed to some parts of the system. So what happens to our cluster if I change one of its components without breaking the others whilst also maintaining backwards compatibility with no disruption to user experience? Thankfully Kubernetes can help with that.\nWhat I don’t like is that the API only handles one image at a time. Unfortunately there is no bulk upload option.\nCode Currently, we have the following code segment dealing with a single image:\n// PostImage handles a post of an image. Saves it to the database // and sends it to NSQ for further processing. func PostImage(w http.ResponseWriter, r *http.Request) { ... } func main() { router := mux.NewRouter() router.HandleFunc(\"/image/post\", PostImage).Methods(\"POST\") log.Fatal(http.ListenAndServe(\":8000\", router)) } We have two options: Add a new endpoint with /images/post and make the client use that, or modify the existing one.\nThe new client code has the advantage in that it can fall back to submitting the old way if the new endpoint isn’t available. The old client code, however, doesn’t have this advantage so we can’t change the way our code works right now. Consider this: You have 90 servers and you do a slow paced rolling update that will take out servers one step at a time whilst doing an update. If an update lasts around a minute, the whole process will take around one and a half hours to complete, (not counting any parallel updates).\nDuring this time, some of your servers will run the new code and some will run the old one. Calls are load balanced, thus you have no control over which servers will be hit. If a client is trying to do a call the new way but hits an old server, the client will fail. The client can try and fallback, but since you eliminated the old version it will not succeed unless it, by mere chance, hits a server with the new code (assuming no sticky sessions are set).\nAlso, once all your servers are updated, an old client will not be able to use your service any longer.\nNow, you can argue that you don’t want to keep old versions of your code forever. And that’s true in a sense. That’s why we are going to modify the old code to simply call the new one with some slight augmentations. This way, once all clients have been migrated, the code can simply be deleted without any problems.\nNew Endpoint Let’s add a new route method:\n... router.HandleFunc(\"/images/post\", PostImages).Methods(\"POST\") ... Updating the old one to call the new one with a modified body looks like this:\n// PostImage handles a post of an image. Saves it to the database // and sends it to NSQ for further processing. func PostImage(w http.ResponseWriter, r *http.Request) { var p Path err := json.NewDecoder(r.Body).Decode(\u0026p) if err != nil { fmt.Fprintf(w, \"got error while decoding body: %s\", err) return } fmt.Fprintf(w, \"got path: %+v\\n\", p) var ps Paths paths := make([]Path, 0) paths = append(paths, p) ps.Paths = paths var pathsJSON bytes.Buffer err = json.NewEncoder(\u0026pathsJSON).Encode(ps) if err != nil { fmt.Fprintf(w, \"failed to encode paths: %s\", err) return } r.Body = ioutil.NopCloser(\u0026pathsJSON) r.ContentLength = int64(pathsJSON.Len()) PostImages(w, r) } Well, the naming could be better, but you should get the basic idea. I’m modifying the incoming single path by wrapping it into the new format and sending it over to the new endpoint handler. And that’s it! There are a few more modifications. To check them out, take a look at this PR: Rolling Update Bulk Image Path PR.\nNow, the receiver can be called in two ways:\n# Single Path: curl -d '{\"path\":\"unknown4456.jpg\"}' http://127.0.0.1:8000/image/post # Multiple Paths: curl -d '{\"paths\":[{\"path\":\"unknown4456.jpg\"}]}' http://127.0.0.1:8000/images/post Here, the client is curl. Normally, if the client is a service, I would modify it that in case the new end-point throws a 404 it would try the old one next.\nFor brevity, I’m not modifying NSQ and the others to handle bulk image processing; they will still receive it one by one. I’ll leave that up to you as homework ;)\nNew Image To perform a rolling update, I must create a new image first from the receiver service.\ndocker build -t skarlso/kube-receiver-alpine:v1.1 . Once this is complete, we can begin rolling out the change.\nRolling update In Kubernetes, you can configure your rolling update in multiple ways:\nManual Update If I was using a container version in my config file called v1.0, then doing an update is simply calling:\nkubectl rolling-update receiver --image:skarlso/kube-receiver-alpine:v1.1 If there is a problem during the rollout we can always rollback.\nkubectl rolling-update receiver --rollback It will set back the previous version. No fuss, no muss.\nApply a new configuration file The problem with by-hand updates is that they aren’t in source control.\nConsider this: Something has changed, A couple of servers got updated by hand to do a quick “patch fix”, but nobody witnessed it and it wasn’t documented. A new person comes along and does a change to the template and applies the template to the cluster. All the servers are updated, and then all of a sudden there is a service outage.\nLong story short, the servers which got updated are written over because the template doesn’t reflect what has been done manually.\nThe recommended way is to change the template in order to use the new version, and than apply the template with the apply command.\nKubernetes recommends that a Deployment with ReplicaSets should handle a rollout. This means there must be at least two replicates present for a rolling update. If less than two replicates are present then the update won’t work (unless maxUnavailable is set to 1). I increase the replica count in yaml. I also set the new image version for the receiver container.\nreplicas: 2 ... spec: containers: - name: receiver image: skarlso/kube-receiver-alpine:v1.1 ... Looking at the progress, this is what you should see :\n❯ kubectl rollout status deployment/receiver-deployment Waiting for rollout to finish: 1 out of 2 new replicas have been updated... You can add in additional rollout configuration settings by specifying the strategy part of the template like this:\nstrategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 Additional information on rolling update can be found in the below documents: Deployment Rolling Update, Updating a Deployment, Manage Deployments, Rolling Update using ReplicaController.\nNOTE MINIKUBE USERS: Since we are doing this on a local machine with one node and 1 replica of an application, we have to set maxUnavailable to 1; otherwise Kubernetes won’t allow the update to happen, and the new version will remain in Pending state. That’s because we aren’t allowing for a services to exist with no running containers; which basically means service outage.\nScaling Scaling is dead easy with Kubernetes. Since it’s managing the whole cluster, you basically just need to put a number into the template of the desired replicas to use.\nThis has been a great post so far, but it’s getting too long. I’m planning on writing a follow-up where I will be truly scaling things up on AWS with multiple nodes and replicas; plus deploying a Kubernetes cluster with Kops. So stay tuned!\nCleanup kubectl delete deployments --all kubectl delete services -all Final Words And that’s it ladies and gentlemen. We wrote, deployed, updated and scaled (well, not yet really) a distributed application with Kubernetes.\nIf you have any questions, please feel free to chat in the comments below. I’m happy to answer.\nI hope you’ve enjoyed reading this. I know it’s quite long; I was thinking of splitting it up multiple posts, but having a cohesive, one page guide is useful and makes it easy to find, save, and print.\nThank you for reading, Gergely.\n","title":"Kubernetes distributed application deployment with sample Face Recognition App","uri":"/2018/03/15/kubernetes-distributed-application/"},{"content":"Intro So I was at Go Budapest Meetup yesterday, where the brilliant Johan Brandhorst gave a talk about his project based on gRPC using gRPC-web + GopherJS + protobuf. He also has some Go contributions and check out his project here: Protobuf. It’s GopherJS Bindings for ProtobufJS and gRPC-Web.\nIt was interesting to see where these projects could lead and I see the potential in them. I liked the usage of Protobuf and gRPC, I don’t have THAT much experience with them. However after yesterday, I’m eager to find an excuse to do something with these libraries. I used gRPC indirectly, well, the result of it, when dealing with Google Cloud Platform’s API. Which is largely generated code through gRPC and protobuf.\nHe also presented a bi-directional stream communication between the gRPC-web client and the server which was an interesting feat to produce. It did involve the use of errgroup. Which is nice.\nI didn’t look THAT much into WebAssembly however, again, after yesterday, I will. He gave a shout out to WebAssembly developers that he is ready to tackle the Go bindings for WASM!\nIt was a good change of pace to look at some Go code being written, I’ll be sure to visit the meetup again, in about three months when the next one will come.\nMaybe, I’ll even give a talk if they are looking for speakers. ;)\nA huge thank you to Emarsys Budapest for organizing the event and bringing Johan to us for his talk.\nThanks, Gergely\n","title":"Go Budapest Meetup","uri":"/2018/02/06/go-budapest-meetup/"},{"content":"Intro Hi folks.\nToday, I would like demonstrate how to use Ansible in order to construct a server hosting multiple HTTPS domains with Nginx and LetsEncrypt. Are you ready? Let’s dive in.\nTL;DR What you will need There is really only one thing you need in order for this to work and that is Ansible. If you would like to run local tests without a remote server, than you will need Vagrant and VirtualBox. But those two are optional.\nWhat We Are Going To Set Up The setup is as follows:\nNagios We are going to have a Nagios with a custom check for pending security updates. That will run under nagios.example.com.\nHugo Website The main web site is going to be a basic Hugo site. Hugo is a static Go based web site generator. This Blog is run by it.\nWe are also going to setup NoIP which will provide the DNS for the sites.\nWiki The wiki is a plain, basic DokuWiki.\nHTTPS + Nginx And all the above will be hosted by Nginx with HTTPS provided by letsencrypt. We are going to set all these up with Ansible on top so it will be idempotent.\nRepository All of the playbooks and the whole thing together can be viewed here: Github Ansible Server Setup.\nAnsible I won’t be writing everything down to the basics about Ansible. For that you will need to go and read its documentation. But I will provide ample of clarification for using what I’ll be using.\nSome Basics Ansible is a configuration management tool which, unlike chef or puppet, isn’t master - slave based. It’s using SSH to run a set of instructions on a target machine. The instructions are written in yaml files and look something like this:\n--- # tasks file for ssh - name: Copy sshd_config copy: content=\"{{sshd_config}}\" dest=/etc/ssh/sshd_config notify: - SSHD Restart This is a basic Task which copies over an sshd_config file overwriting the one already being there. It can execute in priviliged mode if root password is provided or the user has sudo rights.\nIt works from so called hosts files where the server details are described. This is how a basic host file would look like:\n[local] 127.0.0.1 [webserver1] 1.23.4.5 Ansible will use these settings to try and access the server. To test if the connection is working, you can send a ping task like this:\nansible all -m ping Ansible uses variables for things that change. They are defined under each task’s subfolder called vars. Please feel free to change the varialbes there to your liking.\nSSH Access You can either define SSH information per host or per group or globally. In this example I have it under the groups wars called webserver1 like this (vars.yaml):\n--- # SSH sudo keys and pass ansible_become_pass: '{{vault_ansible_become_pass}}' ansible_ssh_port: '{{vault_ansible_ssh_port}}' ansible_ssh_user: '{{vault_ansible_ssh_user}}' ansible_ssh_private_key_file: '{{vault_ansible_ssh_private_key_file}}' home_dir: /root Further reading Further readings are:\n Servers For Hackers Ansible docs  Vault The vault is the place where we can keep secure information. This file is called vault and usually lives under either group_vars or host_vars. The preference is up to you.\nThis file is encrypted using a password you specify. You can have the vault password stored in the following ways:\n Store it on a secure drive which is encrypted and only mounted when the playbook is executed Store it on Keybase Store it on an encrypted S3 bucket Store it in a file next to the playbook which is never commited into source control  Either way, in the end, ansible will look for a file called .vault_password for when it’s trying to decrypt the file. You can define a different file in the ansible.cfg file using the vault_password_file option.\nYou can create a vault like this:\nansible-vault create vault If you are following along, you are going to need these variables in the vault:\nvault_ansible_become_pass: \u003cyour_sudo_password\u003e # if applicable vault_ansible_ssh_user: \u003cssh_user\u003e vault_ansible_ssh_private_key_file: /Users/user/.ssh/ida_rsa vault_nagios_password: supersecurenagiosadminpassword vault_nagios_username: nagiosadmin vault_noip_username: youruser@gmail.com vault_noip_password: \"SuperSecureNoIPPassword\" vault_nginx_user: \u003clocaluser\u003e You can always edit the vault later on with:\nansible-vault edit group_vars/webserver1/vault --vault-password-file=.vault_pass Tasks The following are a collection of tasks which execute in order. The end task, which is letsencrypt, relies on all the hosts being present and configured under Nginx. Otherwise it will throw an error that the host you are trying to configure HTTPS for, isn’t defined.\nNo-IP I’m choosing No-ip as a DNS provider because it’s cheap and the sync tool is easy to automate. To automate the CLI of No-IP, I’m using a package called expect. This looks something like this:\ncd {{home_dir}} wget http://www.no-ip.com/client/linux/noip-duc-linux.tar.gz mkdir -p noip tar zxf noip-duc-linux.tar.gz -C noip cd noip/* make /usr/bin/expect \u003c\u003cEND_SCRIPT spawn make install expect \"Please enter the login/email*\" { send \"{{noip_username}}\\r\" } expect \"Please enter the password for user*\" { send \"{{noip_password}}\\r\" } expect { \"Do you wish to have them all updated*\" { send \"y\" exp_continue } } expect \"Please enter an update interval*\" { send \"30\\r\" } expect \"Do you wish to run something at successful update*\" {send \"N\" } END_SCRIPT The interesting part is the command running expect. Basically, it’s expecting some kind of output which is outlined there. And has canned answers for those which it sends to the waiting command.\nTo Util or Not To Util So, there are small tasks, like installing vim and wget and such which could warrant the existance of a utils task. Utils task would install the packages that are used as convinience and don’t really relate to a singe task.\nYet I settled for the following. Each of my tasks has a dependency part. The given tasks takes care of all the packages it needs so they can be executed on their own as well as in unison.\nThis looks like this:\n# Install dependencies - name: Install dependencies apt: pkg=\"{{item}}\" state=installed with_items: - \"{{deps}}\" For which the deps variable is defined as follows:\n# Defined dependencies for letsencrypt task. deps: ['git', 'python-dev', 'build-essential', 'libpython-dev', 'libpython2.7', 'augeas-lenses', 'libaugeas0', 'libffi-dev', 'libssl-dev', 'python-virtualenv', 'python3-virtualenv', 'virtualenv'] This is much cleaner. And if a task is no longer needed, it’s dependencies will no longer be needed either in most of the cases.\nNagios I’m using Nagios 4 which is a real pain in the butt to install. Luckily, thanks to Ansiblei, I only ever had to figure it out once. Now I have a script for that. Installing Nagios demands several, smaller components to be installed. Thus our task uses import from outside tasks like this:\n- name: Install Nagios block: - include: create_users.yml # creates the Nagios user - include: install_dependencies.yml # installs Nagios dependencies - include: core_install.yml # Installs Nagios Core - include: plugin_install.yml # Installs Nagios Plugins - include: create_htpasswd.yml # Creates a password for Nagios' admin user - include: setup_custom_check.yml # Adds a custom check which is to check how many security updates are pending when: st.stat.exists == False The when is a check for a variable created by a file check.\n- stat: path: /usr/local/nagios/bin/nagios register: st It checks if Nagios is installed or not. If yes, skip.\nI’m not going to paste in here all the subtasks because that would be huge. You can check those out in the repository under Nagios.\nHugo Hugo is easy to install. Its sole requirement is Go. To install hugo you simply run apt-get install hugo. Setting up the site for me was just checking out the git repo and than execute hugo from the root folder like this:\nhugo server --bind=127.0.0.1 --port=8080 --baseUrl=https://example.com --appendPort=false --logFile hugo.log --verboseLog --verbose -v \u0026 Wiki I used DokuWiki because it’s a file based wiki so installation is basically just downloading the archive, extracting it and done. The only thing that’s needed for it, is php-fpm to run it and a few php modules which I’ll outline in the ansible playbook.\nThe VHOST file for DokuWiki is provided by them and looks like this:\nserver { server_name {{ wiki_server_name }}; root {{ wiki_root }}; index index.php index.html index.htm; client_max_body_size 2M; client_body_buffer_size 128k; location / { index doku.php; try_files $uri $uri/ @dokuwiki; } location @dokuwiki { rewrite ^/_media/(.*) /lib/exe/fetch.php?media=$1 last; rewrite ^/_detail/(.*) /lib/exe/detail.php?media=$1 last; rewrite ^/_export/([^/]+)/(.*) /doku.php?do=export_$1\u0026id=$2 last; rewrite ^/(.*) /doku.php?id=$1 last; } location ~ \\.php$ { try_files $uri =404; fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } location ~ /\\.ht { deny all; } location ~ /(data|conf|bin|inc)/ { deny all; } } Nginx Nginx install is through apt as well. Here, however, there is a bit of magic going on with templates. The templates provide the vhost files for the three hosts we will be running. This looks as follows:\n- name: Install vhosts block: - template: src=01_example.com.j2 dest=/etc/nginx/vhosts/01_example.com notify: - Restart Nginx - template: src=02_wiki.example.com.j2 dest=/etc/nginx/vhosts/02_wiki_example.com notify: - Restart Nginx - template: src=03_nagios.example.com.j2 dest=/etc/nginx/vhosts/03_nagios.example.com notify: - Restart Nginx Now, you might be wondering what notify is? It’s basically a handler that gets notified to restart nginx. The great part about it is that it does this only once, even if it was called multiple times. The handler looks like this:\n- name: Restart Nginx service: name: nginx state: restarted And lives under handlers sub-folder.\nWith this, Nginx is done and should be providing our sites under plain HTTP.\nLetsEncrypt Now comes the part where we enable HTTPS for all these three domains. Which is as follows:\n example.com wiki.example.com nagios.example.com  This is actually quiet simple now-a-days with certbot-auto. In fact, it will insert the configurations we need all by itself. The only thing for us to do is to specify what domains we have and what our challenge would be. Also, we have to pass in some variables for certbot-auto to run in a non-interactive mode. This looks as follows:\n- name: Generate Certificate for Domains shell: ./certbot-auto --authenticator standalone --installer nginx -d '{{ domain_example }}' -d '{{ domain_wiki }}' -d '{{ domain_nagios }}' --email example@gmail.com --agree-tos -n --no-verify-ssl --pre-hook \"sudo systemctl stop nginx\" --post-hook \"sudo systemctl start nginx\" --redirect args: chdir: /opt/letsencrypt And that’s that. The interesting and required part here is the pre-hook and post-hook. Without those it wouldn’t work because the ports that certbot is performing the challenge on would be taken already. This stops nginx, performs the challenge and generates the certs, and starts nginx again. Also note --redirect. This will force HTTPS on the sites and disables plain HTTP.\nIf all went well our sites should contain information like this:\nlisten 443 ssl; # managed by Certbot ssl_certificate /etc/letsencrypt/live/example.com-0001/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/example.com-0001/privkey.pem; # managed by Certbot include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot Test Run using Vagrant If you don’t want to run all this on a live server to test out, you can do either of these two things:\n Use a remote dedicated test server Use a local virtual machine with Vagrant  Here, I’m giving you an option for the later.\nIt’s possible for most of the things to be tested on a local Vagrant machine. Most of the time a Vagrant box is enough to test out installing things. A sample Vagrant file looks like this:\n# encoding: utf-8 # -*- mode: ruby -*- # vi: set ft=ruby : # Box / OS VAGRANT_BOX = 'ubuntu/xenial64' VM_NAME = 'ansible-practice' Vagrant.configure(2) do |config| # Vagrant box from Hashicorp config.vm.box = VAGRANT_BOX # Actual machine name config.vm.hostname = VM_NAME # Set VM name in Virtualbox config.vm.provider 'virtualbox' do |v| v.name = VM_NAME v.memory = 2048 end # Ansible provision config.vm.provision 'ansible_local' do |ansible| ansible.limit = 'all' ansible.inventory_path = 'hosts' ansible.playbook = 'local.yml' end end This interesting part here is the ansible provision section. It’s running a version of Ansible that is called ansible_local. It’s local, becuase it will be only on the VirtualBox. Meaning, you don’t have to have Ansible installed to test it on a vagrant box. Neat, huh?\nTo test your playbook, simply run vagrant up and you should see the provisioning happening.\nRoom for improvement And that should be all. Note that this setup isn’t quiet enterprise ready. I would add the following things:\nTests and Checks A ton of tests and checks if the commands that we are using are actually successful or not. If they aren’t make them report the failure.\nMultiple Domains If you happen to have a ton of domain names to set up, this will not be the most effective way. Right now letsencrypt creates a single certificate file for those three domains with -d and that’s not what you want with potentially hundreds of domains.\nIn that case, have a list to go through with with_items. Note that you’ll have to restart nginx on each line, because you don’t want one of them fail and stop the process entirely. Rather have a few fail but the rest still work.\nConclusion That’s it folks. Have fun setting up servers all over the place and enjoy the power of nginx and letsencrypt and not having to worry about adding another server into the bunch.\nThank you for reading, Gergely.\n","title":"Ansible + Nginx + LetsEncrypt + Wiki + Nagios","uri":"/2018/01/23/nginx-certbot-ansible/"},{"content":"Intro Hi folks.\nIn the past couple of months I’ve been slowly updating Furnace.\nThere are three major changes that happened. Let’s take a look at them, shall we?\nGoogle Cloud Platform Furnace now supports Google Cloud Platform (GCP). It provides the same API to handle GCP resource as with AWS. Namely, create, delete, status, update. I opted to leave out push because Google mostly works with git based repositories, meaning a push is literary just a push, than Google handles distributing the new code by itself.\nAll the rest of the commands should work the same way as AWS.\nDeployment Manager GCP has a similar service to AWS CloudFormations called Deployment Manager. The documentation is fairly detailed with a Bookshelf example app to deploy. Code and Templates can be found in their Git repositroy here: Deployment Manager Git Repository.\nSetting up GCP As the README of Furnace outlines…\n Please carefully read and follow the instruction outlined in this document: Google Cloud Getting Started. It will describe how to download and install the SDK and initialize cloud to a Project ID.\n  Take special attention to these documents:\n  Initializing GCloud Tools Authorizing Tools\n  Furnace uses a Google Key-File to authenticate with your Google Cloud Account and Project. In the future, Furnace assumes these things are properly set up and in working order.\n To initialize the client, it uses the following code:\nctx := context.Background() client, err := google.DefaultClient(ctx, dm.NdevCloudmanScope) The DefaultClient in turn, does the following:\n// FindDefaultCredentials searches for \"Application Default Credentials\". // // It looks for credentials in the following places, // preferring the first location found: // // 1. A JSON file whose path is specified by the // GOOGLE_APPLICATION_CREDENTIALS environment variable. // 2. A JSON file in a location known to the gcloud command-line tool. // On Windows, this is %APPDATA%/gcloud/application_default_credentials.json. // On other systems, $HOME/.config/gcloud/application_default_credentials.json. // 3. On Google App Engine it uses the appengine.AccessToken function. // 4. On Google Compute Engine and Google App Engine Managed VMs, it fetches // credentials from the metadata server. // (In this final case any provided scopes are ignored.) func FindDefaultCredentials(ctx context.Context, scope ...string) (*DefaultCredentials, error) { Take note on the order. This is how Google will authenticate your requests.\nRunning GCP Running gcp is largely similar to AWS. First, you create the necessary templates to your infrastructure. This is done via the Deployment Manager and it’s templating engine. The GCP templates are Python JINJA files. Examples are provided in the template directory. It’s a bit more complicated than the CloudFormation templates in that it uses outside templates plus schema files to configure dynamic details.\nIt’s all explained in these documents: Creating a Template Step-by-step and Creating a Basic Template.\nIt’s not trivial however. And using the API can also be confusing. The Google Code is just a generated Go code file using gRPC. But studying it may provide valuable insigth into how the API is structured. I’m also providing some basic samples that I gathered together and the readme does a bit more explaining on how to use them.\nYour First Stack Once you have everything set-up you’ll need a configuration file for Furnace. The usage is outlined more here YAML Configuration. The configuration file for GCP looks like this:\nmain: project_name: testplatform-1234 spinner: 1 gcp: template_name: google_template.yaml stack_name: test-stack Where project_name is the name you generate for your first billable Google Cloud Platform project. Template lives next to this yaml file and stack name must be DNS complient.\nOnce you have a project and a template setup, it’s as simple as calling ./furnace-gcp create or ./furnace-gcp create mycustomstack.\nDeleting Deleting happens with ./furnace-gcp delete or ./furnace-gcp delete mycustomstack. Luckily, as with AWS, this means that every resource created with the DeploymentManager will be deleted leaving no need for search and cleanup.\nProject Name vs. Project ID Unlike with AWS Google requires your stack name and project id to be DNS complient. This is most likely because all API calls and such contain that information.\nSeparate Binaries In order to mitigate some of Furnace’s size, I’m providing separate binaries for each service it supports.\nThe AWS binaries can be found in aws folder, and respectively, the Google Cloud Platform is located in gcp. Both are build-able by running make.\nIf you would like to run both with a single command, a top level make file is provided for your convinience. Just run make from the root. That will build all binaries. Later on, Digital Oceans will join the ranks.\nYAML Configuration Last but not least, Furnace now employs YAML files for configuration. However, it isn’t JUST using YAML files. It also employs a smart configuration pattern which works as follows.\nSince Furnace is a distributed binary file which could be running from any given location at any time. Because of that, at first I opted for a global configuration directory.\nNow, however, furnace uses a furnace configuration file named with the following pattern: .stackalias.furnace. Where stackname, or stack is the name of a custom stack you would like to create for a project. The content of this file is a single entry, which is the location, relative to this file, of the YAML configuration files for the given stack. For example:\nstacks/mydatabasestack.yaml This means, that in the directory called stacks there will a yaml configuration file for your database stack. The AWS config file looks like this:\nmain: stackname: FurnaceStack spinner: 1 aws: code_deploy_role: CodeDeployServiceRole region: us-east-1 enable_plugin_system: false template_name: cloud_formation.template app_name: furnace_app code_deploy: # Only needed in case S3 is used for code deployment code_deploy_s3_bucket: furnace_code_bucket # The name of the zip file in case it's on a bucket code_deploy_s3_key: furnace_deploy_app # In case a Git Repository is used for the application, define these two settings git_account: Skarlso/furnace-codedeploy-app git_revision: b89451234... The important part is the template_name. The template has to be next to this yaml file. To use this file, you simply call any of the AWS or GCP commands with an extra, optional parameter like this:\n./furnace-aws create mydatabase Note that mydatabase will translate to .mydatabase.furnace.\nThe intelligent part is, that this file could be placed anywhere in the project folder structure; because furnace, when looking for a config file, traverses backwards from the current execution directory up until /. Where root is not included in the search.\nConsider the following directory tree:\n├── docs │ ├── furnace-aws status mydatabase ├── stacks │ ├── mystack.template │ └── mystack.yaml └── .mydatabase.furnace\nYou are currently in your docs directory and would like to ask for the status of your database. You don’t have to move to the location of the setting file, just simply run the command from where you are. This only works if you are above the location of the file. If you would be below, furnace would say it can’t find the file. Because it only traverses upwards.\n.mydatabase.furnace here contains only a single entry stacks/mystack.yaml. And that’s it. This way, you could have multiple furnace files, for example a .database.furnace, .front-end.furnace and a .backend.furnace. All three would work in unison, and if want needs updating, simply run ./furnace-aws update backend. And done!\nClosing words As always, contributions are welcomed in the form of issues or pull requests. Questions anything, I tend to answer as soon as I can.\nAlways run the tests before submitting.\nThank you for reading. Gergely.\n","title":"Huge Furnace Update","uri":"/2018/01/13/furnace-massive-update/"},{"content":"Intro Hi All.\nToday I would like to write about an AWS finger practice.\nPreviously, I wrote about how I build and deploy my blog with Wercker. Since, I’m a cloud engineer and I dislike Oracle and it’s ever expending tenctacles into the abyss, I wanted to switch to use something else.\nMy build and deploy cycle is simple.\nCommit to Blogsource Repo -\u003e Wercker WebHook -\u003e Builds my blog using Hugo -\u003e Pushed to a Different Repository which my Github Blog.\nThat’s all.\nIt’s quiet possible to reproduce this on AWS without infering costs. Unless you publish like… a couple 100 posts / week.\nI’m going to use the following services: CloudFormation, AWS Lambda, CodeBuild, S3.\nTo deploy the below describe architecture in your account in us-east-1 region simply click this button: \nBEFORE doing that though you need the following created:\nHave a bucket for your lambda function. The lambda function can be found here:\nLambda Repository.\nZip up the lambda folder contents by doing this:\ncd lambda zip -r gitpusher.zip * aws s3 cp gitpusher.zip s3://your-lambda-bucket That’s it.\nTo read a description of the stack, please continue.\nTL;DR; The architecture I’m about to lay out is simple in its use and design. I tried not to complicate things, because I think the simpler something is, the less prone to failure it will be.\nIn its most basic form the flow is as follows:\n.\nYou push something into a repository you provide. CodeBuild has a webhook to this repository so on each commit it starts to build the blog. The build will use a so called buildspec.yaml file which describes how your blog should be built. Mine looks like this:\nversion: 0.2 phases: install: commands: - echo Installing required packages and Hugo - apt-get update - apt-get install -y git golang wget - wget -q https://github.com/gohugoio/hugo/releases/download/v0.31/hugo_0.31_Linux-64bit.deb -O /tmp/hugo.dep - dpkg -i /tmp/hugo.dep pre_build: commands: - echo Downloading source code - git clone https://github.com/Skarlso/blogsource.git /opt/app build: commands: - echo Build started on `date` - cd /opt/app \u0026\u0026 hugo --theme purehugo post_build: commands: - echo Build completed on `date` artifacts: files: - /opt/app/public/**/* When it’s finished, CodeBuild will upload everything in the public folder as a zip to a bucket. The bucket has a lambda attached which triggers on putObject event with the extension .zip. It downloads the archive, extracts it and pushes it to another repository, which is the repository for the blog.\nAnd done! That’s it. For an architecture overview, please read on.\nArchitecture Now, we are going to use CloudFormation stack to deploy these resources. Because we aren’t animals to create them by hand, yes?\nAn overview of my current architecture is best shown by this image:\n.\nLet’s go over these components one - by - one.\nLambda Role This is the Role which allows the Lambda to access things in your account. It needs the following service access: s3, logs, lambda; and the following permissions: logs:Create*, logs:PutLogEvents, s3:GetObject, s3:ListBucket.\nCode Build Role This is the role which allows CodeBuild to have access to services it needs. These services are the following: s3, logs, ssm, codebuild. CodeBuild also needs the following actions allowed: logs:Create*, logs:PutLogEvents, s3:GetObject, s3:PutObject, ssm:GetParameters.\nBuild Bucket This is the bucket in which CodeBuild will push the generated build artifact.\nBlog Pusher Function This is the heart of this project. It contains the logic to download the zipped artifact, extract it, create a hollow repository from the extracted archive and push the changes to the repository. And just the changes.\nThis is achieve by a short Python 3.6 script which can be found in the linked repository.\nParameters The stack requires you to provide a couple of parameters which are described in the template. Like, bucket name, github repository, git token and such. Please refer to the template for a full description of each.\nCharges I recently push a couple of builds to test this configuration and I inferred 0.2 USD in charges. But that was like 10-15 builds a day.\nDeploying In order to deploy this you can use Furnace to easily manage the template and it’s parameters. Once you copy the template to the target directory, simply run furnace aws create and provide the necessary parameters.\nConclusion And that is all. A nice little stack which does the same as Wercker without costs but the leisure of simply pushing up some change to a repository of your choosing.\nI hope you enjoyed this little write up as much as I enjoyed creating it.\nAs always, Thanks for reading! Gergely.\n","title":"Commit-Build-Deploy With AWS CodeBuild and Lambda","uri":"/2017/12/04/commit-build-deploy/"},{"content":"Hi there folks.\nJust a quick post, of how I went on and created an IKEA manual about Furnace.\nPage 1: . Page 2: .\nI drew these using Krita. I mostly used a mouse but I also used a Wacom Bamboo drawing tabled, for sketches and such.\nThanks, Gergely.\n","title":"Furnace Ikea Manual","uri":"/2017/11/06/furnace-ikea-manual/"},{"content":"Hey folks.\nQuick note. Furnace now comes pre-compiled easy to access binaries which you can download and use out of the box.\nNo need to install anything, or compile the source. Just download, unzip and use.\nHere is the website: Furnace Website.\nEnjoy, Cheers, Gergely.\n","title":"Furnace Binaries","uri":"/2017/09/03/furnace-binaries/"},{"content":"","title":"Notetaking","uri":"/2017/05/31/notetaking/"},{"content":"Intro A while ago, I was added as a curator for a Gem called JsonPath. It’s a small but very useful and brilliant gem. It had a couple of problems which I fixed, but the hardest to eliminate proved to be a series of evals throughout the code.\nYou could opt in using eval with a constructor parameter, but generally, it was considered to be unsafe. Thus, normally when a project was using it, like Huginn they had to opt out by default, thus missing out on sweet parsing like this: $..book[?(@['price'] \u003e 20)].\nEval In order to remove eval, first I had to understand what it is actually doing. I had to take it apart.\nAfter much digging and understanding the code, I found, all it does is perform the given operations on the current node. And if the operation is true, it will select that node, otherwise, return false, and ignore that node.\nFor example $..book[?(@['price'] \u003e 20)] could be translated to:\nreturn @_current_node['price'] \u003e 20 Checking first if 'price' is even a key in @_current_node. Once I’ve understood this part, I set on trying to fix eval.\nSAFE = 4 In ruby, you could extract the part where you Eval and put it into its own proc and set SAFE = 4 which will disable some things like system calls.\nproc do SAFE = 4 eval(some_expression) end.call SAFE levels:\n$SAFE\tDescription 0\tNo checking of the use of externally supplied (tainted) data is performed. This is Ruby’s default mode.\n = 1\tRuby disallows the use of tainted data by potentially dangerous operations. = 2\tRuby prohibits the loading of program files from globally writable locations. = 3\tAll newly created objects are considered tainted. = 4\tRuby effectively partitions the running program in two. None - tainted objects may not be modified. Typically, this will be used to create a sandbox: the program sets up an environment using a lower $SAFE level, then resets $SAFE to 4 to prevent subsequent changes to that environment.\n This has the disadvantage that anything below 4 is just, meh. But nothing above 1 will actually work with JsonPath so… scratch that.\nSandboxing We could technically try and sandbox eval into it’s own process with a PID and whitelist methods which are allowed to be called.\nNot bad, and there are a few gems out there which are trying to do that like SafeRuby. But all of these project have been abandoned years ago for a good reason.\nObject.send Object.send is the best way to get some flexibility while still being safe. You basically just call methods on objects by describing said method on an object and giving parameters to it, like:\n1.send(:+, 2) =\u003e 3 This is a very powerful tool in our toolbox which we will exploit immensely.\nSo let’s get to it.\nWriting a parser Writing a parser in Ruby is a very fluid experience. It has nice tools which support that, and the one I used is StringScanner. It has the ability to track where you are currently at in a string and move a pointer along with regex matches. In fact, JsonPath already employs this method when parsing a json expression. So reusing that logic was in fact… elementary.\nThe expression How do we get from this:\n$..book[?(@['price'] \u003c 20)] To this:\n@_current_node['price'] \u003c 20 Well. By simple elimination. There are a couple of problems along the way of course. Because this wouldn’t be a parser if it couldn’t handle ALL the other cases…\nRemoving Clutter Some of this we don’t need. Like, $..book part.\nThe other things we don’t need are all the '[]?()\nOnce this is done, we can move to isolating the important bits.\nBreakDown Elements How does an expression actually look like?\nLet’s break it down.\nSo, this is a handful. Operations can be \u003c=,\u003e=,\u003c,\u003e,==,!= and operands can be either numbers, or words, and element accessor can be nested since something like this is perfectly valid: $..book[?(@.written.year == 1997)].\nTo avoid being overwhelmed, ruby has our back with a method called dig.\nThis, basically lets us pass in some parameters into a dig function on a hash or an array with variadic parameters, which will go on and access those elements in order how they were supplied. Until it either returns a nil or an end result.\nFor example:\n2.3.1 :001 \u003e a = {a: {b: 'c'}} =\u003e {:a=\u003e{:b=\u003e\"c\"}} 2.3.1 :002 \u003e a.dig(:a, :b) =\u003e \"c\" Easy. However… Dig was only added after ruby 2.3 thus, I had to write my own dig for now, until I stop supporting anything below 2.3.\nAt first, I wanted to add it to the hash class, but it proved to be a futile attempt if I wanted to do it nicely, thus the parser got it as a private method.\ndef dig(keys, hash) return hash unless hash.is_a? Hash return nil unless hash.key?(keys.first) return hash.fetch(keys.first) if keys.size == 1 prev = keys.shift dig(keys, hash.fetch(prev)) end And the corresponding regex behind getting a multitude of elements is as follows:\n... if t = scanner.scan(/\\['\\w+'\\]+/) ... Operator Selecting the operator is another interesting part as it can be a single one or multiple and all sorts. Until I realized that no… it can actually be only a couple.\nAlso, after a bit of fiddling and doing and doing a silly case statement first:\ncase op when '\u003e' dig(@_current_node, *elements) \u003e operand when '\u003c' dig(@_current_node, *elements) \u003e operand ... end …I promptly saw that this is not how it should be done.\nAnd here comes Object.send.\nThis gave me the opportunity to write this:\ndig(elements, @_current_node).send(operator, operand) Much better. Now I could send all the things in the way of a node.\nParsing an op be like:\nelsif t = scanner.scan(/\\s+[\u003c\u003e=][\u003c\u003e=]?\\s+?/) Operand Now comes the final piece. The value which we are comparing. This could either be a simple integer, a floating number, or a word. Hah. So coming up with a regex which fits this tightly took a little fiddling, but eventually I ended up with this:\nelsif t = scanner.scan(/(\\s+)?'?(\\w+)?[.,]?(\\w+)?'?(\\s+)?/) Without StackOverflow I would say this is fine ((although I need to remove all those space check, shees)). What are all the question marks? Basically, everything is optional. Because an this expression $..book[?(@.price)] is valid. Which is basically just asserting if a given node has a price element.\nLogical Operators The last thing that remains is logical operators, which if you are using eval, is pretty straight forward. It takes care of anything that you might add in like \u0026\u0026, ||, |, \u0026, ^ etc etc.\nNow, that’s something I did with a case though. Until I find a nicer solution. Since we can already parse a single expression it’s just a question of breaking down a multi structure expression as the following one: $..book[?(@['price'] \u003e 20 \u0026\u0026 @.written.year == 1998)].\nexps = exp.split(/(\u0026\u0026)|(\\|\\|)/) This splits up the string by either \u0026\u0026 or || and the usage of groups () also includes the operators. Than I evaluate the expressions and save the whole thing in an array like [true, '\u0026\u0026', false]. You know what could immediately resolve this? Yep…\n.\nI’d rather just parse it although technically an eval at this stage wouldn’t be that big of a problem…\ndef parse(exp) exps = exp.split(/(\u0026\u0026)|(\\|\\|)/) ret = parse_exp(exps.shift) exps.each_with_index do |item, index| case item when '\u0026\u0026' ret \u0026\u0026= parse_exp(exps[index + 1]) when '||' ret ||= parse_exp(exps[index + 1]) end end ret end Closing words That’s it folks. The parser is done. And there is no eval being used. There are some more things here that are interesting. Like, array indexing is allowed in jsonpath which is solved by sending .length to a current node. For example:\nif scanner.scan(/\\./) sym = scanner.scan(/\\w+/) op = scanner.scan(/./) num = scanner.scan(/\\d+/) return @_current_node.send(sym.to_sym).send(op.to_sym, num.to_i) end If an expression begins with a .. So you see that using send will help a lot, and understanding what eval is trying to evaluate and rather writing your own parser, isn’t that hard at all using ruby.\nI hope you enjoyed reading this little tid-bit as much as I enjoyed writing and drawing it. Leave a comment if your liked the drawings or if you did not and I should never do them again (( I don’t really care, this is my blog haha. )). Note to self: I shouldn’t draw on the other side of the drawing because of bleed-through.\nThank you! Gergely.\n","title":"Replacing Eval with Object.send and a self written Parser","uri":"/2017/05/28/replace-eval-with-object-send-and-a-parser/"},{"content":"Intro Hi folks.\nPreviously on this blog: Part 1. Part 2. Part 3.\nIn this part we are going to talk about Unit Testing Furnace and how to work some magic with AWS and Go.\nMock Stub Fake Dummy Canned  Unit testing in Go usually follows the Dependency Injection model of dealing with Mocks and Stubs.\n## DI\nDependency Inject in short is one object supplying the dependencies of another object. In a longer description, it’s ideal to be used for removing the lock on a third party library, like the AWS client. Imaging having code which solely depends on the AWS client. How would you unit test that code without having to ACTUALLY connect to AWS? You couldn’t. Every time you try to test the code it would run the live code and it would try and connect to AWS and perform the operations it’s design to do. The Ruby library with it’s metaprogramming allows you to set the client globally to stub responses, but, alas, this is not the world of Ruby.\nHere is where DI comes to the rescue. If you have control over the AWS client on a very high level, and would pass it around as a function parameter, or create that client in an init() function and have it globally defined; you would be able to implement your own client, and have your code use that with stubbed responses which your tests need. For example, you would like a CreateApplication call to fail, or you would like a DescribeStack which returns an aws.Error(“StackAlreadyExists”).\nFor this, however, you need the API of the AWS client. Which is provided by AWS.\nAWS Client API In order for DI to work, the injected object needs to be of a certain type for us to inject our own. Luckily, AWS provides an Interface for all of it’s clients. Meaning, we can implement our own version for all of the clients, like S3, CloudFormation, CodeDeploy etc.\nFor each client you want to mock out, an *iface package should be present like this:\n\"github.com/aws/aws-sdk-go/service/cloudformation/cloudformationiface\" In this package you find and use the interface like this:\ntype fakeCloudFormationClient struct { cloudformationiface.CloudFormationAPI err error } And with this, we have our own CloudFormation client. The real code uses the real clients as function parameters, like this:\n// Execute defines what this command does. func (c *Create) Execute(opts *commander.CommandHelper) { log.Println(\"Creating cloud formation session.\") sess := session.New(\u0026aws.Config{Region: aws.String(config.REGION)}) cfClient := cloudformation.New(sess, nil) client := CFClient{cfClient} createExecute(opts, \u0026client) } We can’t test Execute itself, as it’s using the real client here (or you could have a global from some library, thus allowing you to tests even Execute here) but there is very little logic in this function for this very reason. All the logic is in small functions for which the main starting point and our testing opportunity is, createExecute.\nStubbing Calls Now, that we have our own client, and with the power of Go’s interface embedding as seen above with CloudFormationAPI, we have to only stub the functions which we are actually using, instead of every function of the given interface. This looks like this:\ncfClient := new(CFClient) cfClient.Client = \u0026fakeCloudFormationClient{err: nil} Where cfClient is a struct like this:\n// CFClient abstraction for cloudFormation client. type CFClient struct { Client cloudformationiface.CloudFormationAPI } And a stubbed call can than be written as follows:\nfunc (fc *fakeCreateCFClient) WaitUntilStackCreateComplete(input *cloudformation.DescribeStacksInput) error { return nil } This can range from a very trivial example, like the one above, to intricate ones as well, like this gem:\nfunc (fc *fakePushCFClient) ListStackResources(input *cloudformation.ListStackResourcesInput) (*cloudformation.ListStackResourcesOutput, error) { if \"NoASG\" == *input.StackName { return \u0026cloudformation.ListStackResourcesOutput{ StackResourceSummaries: []*cloudformation.StackResourceSummary{ { ResourceType: aws.String(\"NoASG\"), PhysicalResourceId: aws.String(\"arn::whatever\"), }, }, }, fc.err } return \u0026cloudformation.ListStackResourcesOutput{ StackResourceSummaries: []*cloudformation.StackResourceSummary{ { ResourceType: aws.String(\"AWS::AutoScaling::AutoScalingGroup\"), PhysicalResourceId: aws.String(\"arn::whatever\"), }, }, }, fc.err } This ListStackResources stub lets us test two scenarios based on the stackname. If the test stackname is ‘NoASG’ it will return a result which equals to a result containing no AutoScaling Group. Otherwise, it will return the correct ResourceType for an ASG.\nIt is a common practice to line up several scenario based stubbed responses in order to test the robustness of your code.\nUnfortunately, this also means that your tests will be a bit cluttered with stubs and mock structs and whatnots. For that, I’m partially using a package available struct file in which I’m defining most of the mock structs at least. And from there on, the tests will only contain specific stubs for that particular file. This can be further fine grained by having defaults and than only override in case you need something else.\nTesting fatals Now, the other point which is not really AWS related, but still comes to mind when dealing with Furnace, is testing error scenarios.\nBecause Furnace is a CLI application it uses Fatals to signal if something is wrong and it doesn’t want to continue or recover because, frankly it can’t. If AWS throws an error, that’s it. You can retry, but in 90% of the cases, it’s usually something that you messed up.\nSo, how do we test for a fatal or an os.Exit? There are a number of points on that if you do a quick search. You may end up on this talk: GoTalk 2014 Testing Slide #23. Which does an interesting thing. It calls the test binary in a separate process and tests the exit code.\nOthers, and me as well, will say that you have to have your own logger implemented and use a different logger / os.Exit in your test environment.\nOthers others will tell you to not to have tests around os.Exit and fatal things, rather return an error and only the main should pop a world ending event. I leave it up to you which you want to use. Either is fine.\nIn Furnace, I’m using a global logger in my error handling util like this:\n// HandleFatal handler fatal errors in Furnace. func HandleFatal(s string, err error) { LogFatalf(s, err) } And LogFatalf is an exported variable var LogFatalf = log.Fatalf. Than in a test, I just override this variable with a local anonymous function:\nfunc TestCreateExecuteEmptyStack(t *testing.T) { failed := false utils.LogFatalf = func(s string, a ...interface{}) { failed = true } config.WAITFREQUENCY = 0 client := new(CFClient) stackname := \"EmptyStack\" client.Client = \u0026fakeCreateCFClient{err: nil, stackname: stackname} opts := \u0026commander.CommandHelper{} createExecute(opts, client) if !failed { t.Error(\"expected outcome to fail during create\") } } It can get even more granular by testing for the error message to make sure that it actually fails at the point we think we are testing:\nfunc TestCreateStackReturnsWithError(t *testing.T) { failed := false expectedMessage := \"failed to create stack\" var message string utils.LogFatalf = func(s string, a ...interface{}) { failed = true if err, ok := a[0].(error); ok { message = err.Error() } } config.WAITFREQUENCY = 0 client := new(CFClient) stackname := \"NotEmptyStack\" client.Client = \u0026fakeCreateCFClient{err: errors.New(expectedMessage), stackname: stackname} config := []byte(\"{}\") create(stackname, config, client) if !failed { t.Error(\"expected outcome to fail\") } if message != expectedMessage { t.Errorf(\"message did not equal expected message of '%s', was:%s\", expectedMessage, message) } } Conclusion This is it. That’s all it took to write Furnace. I hope you enjoyed reading it as much as I enjoyed writing all these thoughts down.\nI hope somebody might learn from my journey and also improve upon it.\nAny comments are much appreciated and welcomed. Also, PRs and Issues can be submitted on the GitHub page of Furnace.\nThank you for reading! Gergely.\n","title":"Furnace - The building of an AWS CLI Tool for CloudFormation and CodeDeploy - Part 4","uri":"/2017/04/16/building-furnace-part-4/"},{"content":"Intro Hi folks.\nPreviously on this blog: Part 1. Part 2. Part 4.\nIn this part, I’m going to talk about the experimental plugin system of Furnace.\nGo Experimental Plugins Since Go 1.8 was released, an exciting and new feature was introduced called a Plug-in system. This system works with dynamic libraries built with a special switch to go build. These libraries, .so or .dylib (later), are than loaded and once that succeeds, specific functions can be called from them (symbol resolution).\nWe will see how this works. For package information, visit the plugin packages Go doc page here.\nFurnace Plugins So, what does furnace use plugins for? Furnace uses plugins to execute arbitery code in, currently, four given locations / events.\nThese are: pre_create, post_create, pre_delete, post_delete. These events are called, as their name suggests, before and after the creation and deletion of the CloudFormation stack. It allows the user to execute some code without having to rebuild the whole project. It does that by defining a single entry point for the custom code called RunPlugin. Any number of functions can be implemented, but the plugin MUST provide this single, exported function. Otherwise it will fail and ignore that plugin.\nUsing Plugins It’s really easy to implement, and use these plugins. I’m not going into the detail of how to load them, because that is done by Furnace, but only how to write and use them.\nTo use a plugin, create a go file called: 0001_mailer.go. The 0001 before it will define WHEN it’s executed. Having multiple plugins is completely okay. Execution of order however, depends on the names of the files.\nNow, in 0001_mailer.post_create we would have something like this:\npackage main import \"log\" // RunPlugin runs the plugin. func RunPlugin() { log.Println(\"My Awesome Pre Create Plugin.\") } Next step is the build this file to be a plugin library. Note: Right now, this only works on Linux!\nTo build this file run the following:\ngo build -buildmode=plugin -o 0001_mailer.pre_create 0001_mailer.go The important part here is the extension of the file specified with -o. It’s important because that’s how Furnace identifies what plugins it has to run.\nFinally, copy this file to ~/.config/go-furnace/plugins and you are all set.\nSlack notification Plugin To demonstrate how a plugin could be used is if you need some kind of notification once a Stack is completed. For example, you might want to send a message to a Slack room. To do this, your plugin would look something like this:\npackage main import ( \"fmt\" \"os\" \"github.com/nlopes/slack\" ) func RunPlugin() { stackname := os.Getenv(\"FURNACE_STACKNAME\") api := slack.New(\"YOUR_TOKEN_HERE\") params := slack.PostMessageParameters{} channelID, timestamp, err := api.PostMessage(\"#general\", fmt.Sprintf(\"Stack with name '%s' is Done.\", stackname), params) if err != nil { fmt.Printf(\"%s\\n\", err) return } fmt.Printf(\"Message successfully sent to channel %s at %s\", channelID, timestamp) } Currently, Furnace has no ability to share information of the stack with an outside plugin. Thus ‘Done’ could be anything from Rollback to Failed to CreateComplete.\nClosing Words That’s it for plugins. Thanks very much for reading! Gergely.\n","title":"Furnace - The building of an AWS CLI Tool for CloudFormation and CodeDeploy - Part 3","uri":"/2017/03/22/building-furnace-part-3/"},{"content":"Intro Hi folks.\nPreviously on this blog: Part 1, Part 3, Part 4\nIn this part, I’m going to talk about the AWS Go SDK and begin do dissect the intricacies of Furnace.\nAWS SDK Fortunately, the Go SDK for AWS is quiet verbose and littered with examples of all sorts. But that doesn’t make it less complex and less cryptic at times. I’m here to lift some of the early confusions, in hopes that I can help someone to avoid wasting time.\nGetting Started and Developers Guide As always, and common from AWS, the documentation is top notch. There is a 141 pages long developer’s guide on the SDK containing a getting started section and an API reference. Go check it out. I’ll wait. AWS Go SDK DG PDF. I will only talk about some gotchas and things I encountered, not the basics of the SDK.\naws.String and other types Something which is immediately visible once we take a look at the API is that everything is a pointer. Now, there are a tremendous amount of discussions about this, but I’m with Amazon. There are various reasons for it, but to list the most prominent ones: - Type completion and compile time type safety. - Values for AWS API calls have valid zero values, in addition to being optional, i.e. not being provided at all. - Other option, like, empty interfaces with maps, or using zero values, or struct wrappers around every type, made life much harder rather than easier or not possible at all. - The AWS API is volatile. You never know when something gets to be optional, or required. Pointers made that decision easy.\nThere are good number of other discussions around this topic, for example: AWS Go GitHub #363.\nIn order to use primitives, AWS has helper functions like aws.String. Because \u0026“asdf” is not allowed, you would have to create a variable and use its address in situations where a string pointer is needed, for example, name of the stack. These primitive helpers will make in-lining possible. We’ll see later that they are used to a great extent. Pointers, however, make life a bit difficult when constructing Input structs and make for poor aesthetics.\nThis is something I’m returning in a test for stubbing a client call:\nreturn \u0026cloudformation.ListStackResourcesOutput{ StackResourceSummaries: []*cloudformation.StackResourceSummary{ { ResourceType: aws.String(\"NoASG\"), PhysicalResourceId: aws.String(\"arn::whatever\"), }, }, } This doesn’t look so appealing, but one gets used to it quickly.\nError handling Errors also have their own types. An AWS error looks like this:\nif err != nil { if awsErr, ok := err.(awserr.Error); ok { } } First, we check if error is nil, than we type check if the error is an AWS error or something different. In the wild, this will look something like this:\nif err != nil { if awsErr, ok := err.(awserr.Error); ok { if awsErr.Code() != codedeploy.ErrCodeDeploymentGroupAlreadyExistsException { log.Println(awsErr.Code()) return err } log.Println(\"DeploymentGroup already exists. Nothing to do.\") return nil } return err } If it’s an AWS error, we can check further for the error code that it returns in order to identify what to handle, or what to throw on to the caller to a potential fatal. Here, I’m ignoring the AlreadyExistsException because, if it does, we just go on to a next action.\nExamples Luckily the API doc is very mature. In most of the cases, they provide an example to an API call. These examples, however, from time to time provide more confusion than clarity. Take CloudFormation. For me, when I first glanced upon the description of the API it wasn’t immediately clear that the TemplateBody was supposed to be the whole template, and that the rest of the fields were almost all optional settings. Or provided overrides in special cases.\nAnd since the template is not an ordinary JAML or JSON file, I was looking for something that parses it into that the Struct I was going to use. After some time, and digging, I realized that I didn’t need that, and that I just need to read in the template, define some extra parameters, and give the TemplateBody the whole of the template. The parameters defined by the CloudFormation template where extracted for me by ValidateTemplate API call which returned all of them in a convenient []*cloudformation.Parameter slice. These things are not described in the document or visible from the examples. I mainly found them through playing with the API and focused experimentation.\nWaiters From other SDK implementations, we got used to Waiters. These handy methods wait for a service to become available or for certain situations to take in effect, like a Stage being CREATE_COMPLETE. The Go waiters, however, don’t allow for callback to be fired, or for running blocks, like the ruby SDK does. For this, I wrote a handy little waiter for myself, which outputs a spinner to see that we are currently waiting for something and not frozen in time. This waiter looks like this:\n// WaitForFunctionWithStatusOutput waits for a function to complete its action. func WaitForFunctionWithStatusOutput(state string, freq int, f func()) { var wg sync.WaitGroup wg.Add(1) done := make(chan bool) go func() { defer wg.Done() f() done \u003c- true }() go func() { counter := 0 for { counter = (counter + 1) % len(Spinners[config.SPINNER]) fmt.Printf(\"\\r[%s] Waiting for state: %s\", yellow(string(Spinners[config.SPINNER][counter])), red(state)) time.Sleep(time.Duration(freq) * time.Second) select { case \u003c-done: fmt.Println() break default: } } }() wg.Wait() } And I’m calling it with the following method:\nutils.WaitForFunctionWithStatusOutput(\"DELETE_COMPLETE\", config.WAITFREQUENCY, func() { cfClient.Client.WaitUntilStackDeleteComplete(describeStackInput) }) This would output these lines to the console:\n[\\] Waiting for state: DELETE_COMPLETE The spinner can be configured to be one of the following types:\nvar Spinners = []string{`←↖↑↗→↘↓↙`, `▁▃▄▅▆▇█▇▆▅▄▃`, `┤┘┴└├┌┬┐`, `◰◳◲◱`, `◴◷◶◵`, `◐◓◑◒`, `⣾⣽⣻⢿⡿⣟⣯⣷`, `|/-\\`} Handy.\nAnd with that, let’s dive into the basics of Furnace.\nFurnace Directory Structure and Packages Furnace is divided into three main packages.\ncommands Commands package is where the gist of Furnace lies. These commands represent the commands which are used through the CLI. Each file has the implementation for one command. The structure is devised by this library: Yitsushi’s Command Library. As of the writing of this post, the following commands are available:\n create - Creates a stack using the CloudFormation template file under ~/.config/go-furnace delete - Deletes the created Stack. Doesn’t do anything if the stack doesn’t exist push - Pushes an application to a stack status - Displays information about the stack delete-application - Deletes the CodeDeploy application and deployment group created by push  These commands represent the heart of furnace. I would like to keep these to a minimum, but I do plan on adding more, like update and rollout. Further details and help messages on these commands can be obtained by running: ./furnace help or ./furnace help create.\n❯ ./furnace help push Usage: furnace push appName [-s3] Push a version of the application to a stack Examples: furnace push furnace push appName furnace push appName -s3 furnace push -s3 config Contains the configuration loader and some project wide defaults which are as follows:\n Events for the plugin system - pre-create, post-create, pre-delete, post-delete. CodeDeploy role name - CodeDeployServiceRole. This is used if none is provided to locate the CodeDeploy IAM role. Wait frequency - Is the setting which controls how long the waiter should sleep in between status updates. Default is 1s. Spinner - Is just the number of the spinner to use. Plugin registry - Is a map of functions to run for the above events.  Further more, config loads the CloudFormation template and checks if some necessary settings are present in the environment, exp: the configuration folder under ~/.config/go-furnace.\nutils These are some helper functions which are used throughout the project. To list them:\n error_handler - Is a simple error handler. I’m thinking of refactoring this one to some saner version. spinner - Sets up which spinner to use in the waiter function. waiter - Contains the verbose waiter introduced above under Waiters.  Configuration and Environment variables Furnace is a Go application, thus it doesn’t have the luxury of Ruby or Python where the configuration files are usually bundled with the app. But, it does have a standard for it. Usually, configurations reside in either of these two locations. Environment Properties or|and configuration files under a fixed location ( i.e. HOME/.config/app-name ). Furnace employs both.\nSettings like, region, stack name, enable plugin system, are under environment properties ( though this can change ), while the CloudFormation template lives under ~/.config/go-furnace/. Lastly it assumes some things, like the Deployment IAM role just exists under the used AWS account. All these are loaded and handled by the config package described above.\nUsage A typical scenario for Furnace would be the following:\n Setup your CloudFormation template or use the one provided. The one provided sets up a highly available and self healing setting using Auto-Scaling and Load-Balancing with a single application instance. Edit this template to your liking than copy it to ~/.config/go-furnace. Create the configured stack with ./furnace create. Create will ask for the parameters defined in the template. If defaults are setup, simply hitting enter will use these defaults. Take note, that the provided template sets up SSH access via a provided key. If that key is not present in CF, you won’t be able to SSH into the created instance. Once the stack is completed, the application is ready to be pushed. To do this, run: ./furnace push. This will locate the appropriate version of the app from S3 or GitHub and push that version to the instances in the Auto-Scaling group. To all of them.  General Practices Applied to the Project Commands For each command the main entry point is the execute function. These functions are usually calling out the small chunks of distributed methods. Logic was kept to a bare minimum ( probably could be simplified even further ) in the execute functions mostly for testability and the likes. We will see that in a followup post.\nErrors Errors are handled immediately and usually through a fatal. If any error occurs than the application is halted. In followup versions this might become more granular. I.e. don’t immediately stop the world, maybe try to recover, or create a Poller or Re-Tryer, which tries a call again for a configured amount of times.\nOutput colors Not that important, but still… Aesthetics. Displaying data to the console in a nice way gives it some extra flare.\nMakefile This project works with a Makefile for various reasons. Later on, once the project might become more complex, a Makefile makes it really easy to handle different ways of packaging the application. Currently, for example, it provides a linux target which will make Go build the project for Linux architecture on any other Architecture i.e. cross-compiling.\nIt also provides an easy way to run unit tests with make test and installing with make \u0026\u0026 make install.\nClosing Words That is all for Part 2. Join me in Part 3 where I will talk about the experimental Plugin system that Furnace employs.\nThank you for reading! Gergely.\n","title":"Furnace - The building of an AWS CLI Tool for CloudFormation and CodeDeploy - Part 2","uri":"/2017/03/19/building-furnace-part-2/"},{"content":"Testing.\n","title":"Testing new Hugo if posts are generated properly","uri":"/2017/03/17/test-new-hugo/"},{"content":"Other posts: Part 2, Part 3, Part 4.\nBuilding Furnace: Part 1 Intro Hi folks.\nThis is the first part of a 4 part series which talks about the process of building a middlish sized project in Go, with AWS. Including Unit testing and a experimental plugin feature.\nThe first part will talk about the AWS services used in brief and will contain a basic description for those who are not familiar with them. The second part will talk about the Go SDK and the project structure itself, how it can be used, improved, and how it can help in everyday life. The third part will talk about the experimental plugin system, and finally, we will tackle unit testing AWS in Go.\nLet’s begin, shall we?\nAWS CloudFormation If you haven’t yet read about, or know off, AWS' CloudFormation service, you can either go ahead and read the Documentation or read on for a very quick summary. If you are familiar with CF, you should skip ahead to CodeDeploy section.\nCF is a service which bundles together other AWS services (for example: EC2, S3, ELB, ASG, RDS) into one, easily manageable stack. After a stack has been created, all the resources can be handled as one, located, tagged and used via CF specific console commands. It’s also possible to define any number of parameters, so a stack can actually be very versatile. A parameter can be anything, from SSH IP restriction to KeyPair names and list of tags to create or in what region the stack will be in.\nTo describe how these parts fit together, one must use a CloudFormation Template file which is either in JSON or in YAML format. A simple example looks like this:\nParameters: KeyName: Description: The EC2 Key Pair to allow SSH access to the instance Type: AWS::EC2::KeyPair::KeyName Resources: Ec2Instance: Type: AWS::EC2::Instance Properties: SecurityGroups: - Ref: InstanceSecurityGroup - MyExistingSecurityGroup KeyName: Ref: KeyName ImageId: ami-7a11e213 InstanceSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: Enable SSH access via port 22 SecurityGroupIngress: - IpProtocol: tcp FromPort: '22' ToPort: '22' CidrIp: 0.0.0.0/0 There are a myriad of these template samples here.\nI’m not going to explain this in too much detail. Parameters define the parameters, and resources define all the AWS services which we would like to configure. Here we can see, that we are creating an EC2 instance with a custom Security Group plus and already existing security group. ImageId is the AMI which will be used for the EC2 instance. The InstanceSecurityGroup is only defining some SSH access to the instance.\nThat is pretty much it. This can become bloated relatively quickly once, VPCs, ELBs, and ASGs come into play. And CloudFormation templates can also contain simple logical switches, like, conditions, ref for variables, maps and other shenanigans.\nFor example consider this part in the above example:\nKeyName: Ref: KeyName Here, we use the KeyName parameter as a Reference Value which will be interpolated to the real value, or the default one, as the template gets processed.\nCodeDeploy If you haven’t heard about CodeDeploy yet, please browse the relevant Documentation or follow along for a “quick” description.\nCodeDeploy just does what the name says. It deploys code. Any kind of code, as long as the deployment process is described in a file called appspec.yml. It can be easy as coping a file to a specific location or incredibly complex with builds of various kinds.\nFor a simple example look at this configuration:\nversion: 0.0 os: linux files: - source: /index.html destination: /var/www/html/ - source: /healthy.html destination: /var/www/html/ hooks: BeforeInstall: - location: scripts/install_dependencies timeout: 300 runas: root - location: scripts/clean_up timeout: 300 runas: root - location: scripts/start_server timeout: 300 runas: root ApplicationStop: - location: scripts/stop_server timeout: 300 runas: root CodeDeploy applications have hooks and life-cycle events which can be used to control the deployment process of an like, starting the WebServer; making sure files are in the right location; copying files, running configuration management software like puppet, ansible or chef; etc, etc.\nWhat can be done in an appspec.yml file is described here: Appspec Reference Documentation.\nDeployment happens in one of two ways:\nGitHub If the preferred way to deploy the application is from GitHub a commit hash must be used to identify which “version” of the application is to be deployed. For example:\nrev = \u0026codedeploy.RevisionLocation{ GitHubLocation: \u0026codedeploy.GitHubLocation{ CommitId: aws.String(\"kajdf94j0f9k309klksjdfkj\"), Repository: aws.String(\"Skarlso/furnace-codedeploy-app\"), }, RevisionType: aws.String(\"GitHub\"), } Commit Id is the hash of the latest release and repository is the full account/repository pointing to the application.\nS3 The second way is to use an S3 bucket. The bucket will contain an archived version of the application with a given extension. I’m saying given extension, because it has to be specified like this (and can be either ‘zip’, or ‘tar’ or ‘tgz’):\nrev = \u0026codedeploy.RevisionLocation{ S3Location: \u0026codedeploy.S3Location{ Bucket: aws.String(\"my_codedeploy_bucket\"), BundleType: aws.String(\"zip\"), Key: aws.String(\"my_awesome_app\"), Version: aws.String(\"VersionId\"), }, RevisionType: aws.String(\"S3\"), } Here, we specify the bucket name, the extension, the name of the file and an optional version id, which can be ignored.\nDeploying So how does code deploy get either of the applications to our EC2 instances? It uses an agent which is running on all of the instances that we create. In order to do this, the agent needs to be present on our instance. For linux this can be achieved with the following UserData (UserData in CF is the equivalent of a bootsrap script):\n\"UserData\" : { \"Fn::Base64\" : { \"Fn::Join\" : [ \"\\n\", [ \"#!/bin/bash -v\", \"sudo yum -y update\", \"sudo yum -y install ruby wget\", \"cd /home/ec2-user/\", \"wget https://aws-codedeploy-eu-central-1.s3.amazonaws.com/latest/install\", \"chmod +x ./install\", \"sudo ./install auto\", \"sudo service codedeploy-agent start\", ] ] } } A simple user data configuration in the CloudFormation template will make sure that every instance that we create will have the CodeDeploy agent running and waiting for instructions. This agent is self updating. Which can cause some trouble if AWS releases a broken agent. However unlikely, it can happen. Never the less, once installed, it’s no longer a concern to be bothered with.\nIt communications on HTTPS port 443.\nCodeDeploy identifies instances which need to be updated according to our preferences, by tagging the EC2 and Auto Scaling groups. Tagging happens in the CloudFormation template through the AutoScalingGroup settings like this:\n\"Tags\" : [ { \"Key\" : \"fu_stage\", \"Value\" : { \"Ref\": \"AWS::StackName\" }, \"PropagateAtLaunch\" : true } ] This will give the EC2 instance a tag called fu_stage with value equaling to the name of the stack. Once this is done, CodeDeploy looks like this:\nparams := \u0026codedeploy.CreateDeploymentInput{ ApplicationName: aws.String(appName), IgnoreApplicationStopFailures: aws.Bool(true), DeploymentGroupName: aws.String(appName + \"DeploymentGroup\"), Revision: revisionLocation(), TargetInstances: \u0026codedeploy.TargetInstances{ AutoScalingGroups: []*string{ aws.String(\"AutoScalingGroupPhysicalID\"), }, TagFilters: []*codedeploy.EC2TagFilter{ { Key: aws.String(\"fu_stage\"), Type: aws.String(\"KEY_AND_VALUE\"), Value: aws.String(config.STACKNAME), }, }, }, UpdateOutdatedInstancesOnly: aws.Bool(false), } CreateDeploymentInput is the entire parameter list that is needed in order to identify instances to deploy code to. We can see here that it looks for an AutoScalingGroup by Physical Id and the tag labeled fu_stage. Once found, it will use UpdateOutdatedInstancesOnly to determine if an instance needs to be updated or not. Set to false means, it always updates.\nFurnace Where does Furnace fit in, in all of this? Furnace provides a very easy mechanism to create, delete and push code to a CloudFormation stack using CodeDeploy, and a couple of environment properties. Furnace create will create a CloudFormation stack according to the provided template, all the while asking for the parameters defined in it for flexibility. delete will remove the stack and all affiliated resources except for the created CodeDeploy application. For that, there is delete-application. status will display information about the stack: Outputs, Parameters, Id, Name, and status. Something like this:\n2017/03/16 21:14:37 Stack state is: { Capabilities: [\"CAPABILITY_IAM\"], CreationTime: 2017-03-16 20:09:38.036 +0000 UTC, DisableRollback: false, Outputs: [{ Description: \"URL of the website\", OutputKey: \"URL\", OutputValue: \"http://FurnaceSt-ElasticL-ID.eu-central-1.elb.amazonaws.com\" }], Parameters: [ { ParameterKey: \"KeyName\", ParameterValue: \"UserKeyPair\" }, { ParameterKey: \"SSHLocation\", ParameterValue: \"0.0.0.0/0\" }, { ParameterKey: \"CodeDeployBucket\", ParameterValue: \"None\" }, { ParameterKey: \"InstanceType\", ParameterValue: \"t2.nano\" } ], StackId: \"arn:aws:cloudformation:eu-central-1:9999999999999:stack/FurnaceStack/asdfadsf-adsfa3-432d-a-fdasdf\", StackName: \"FurnaceStack\", StackStatus: \"CREATE_COMPLETE\" } ( This will later be improved to include created resources as well. )\nOnce the stack is CREATE_COMPLETE a simple push will deliver our application on each instance in the stack. We will get into more detail about how these commands are working in Part 2 of this series.\nFinal Words This is it for now.\nJoin me next time when I will talk about the AWS Go SDK and its intricacies and we will start to look at the basics of Furnace.\nAs always, Thanks for reading! Gergely.\n","title":"Furnace - The building of an AWS CLI Tool for CloudFormation and CodeDeploy - Part 1","uri":"/2017/03/16/building-furnace-part-1/"},{"content":"Hi folks.\nJust a quick headsup, that older posts and images, may have been lost unfortunately, because I made the terrible mistake, when I migrated over from my old blog, that I forgot to download all the images from the remote host.\nFor lack of options, I deleted the images. :/ Sorry for the inconvencience!\nGergely.\n","title":"Images on older posts","uri":"/2017/03/03/images-on-old-posts/"},{"content":"Intro Hi folks.\nToday, I would like to write about how to do HTTPS for a website, without the need to buy a certificate and set it up via your DNS provider. Let’s begin.\nAbstract What you will achieve by the end of this post:\n Every call to HTTP will be redirected to HTTPS via haproxy. HTTPS will be served with Haproxy and LetsEncrypt as the Certificate provider. Automatically update the certificate before its expiration. No need for IPTable rules to route 8080 to 80. Traffic to and from your page will be encrypted. This all will cost you nothing.  I will use a static website generator for this called Hugo which, if you know me, is my favorite generator tool. These instructions are for haproxy and hugo, if you wish to use apache and nginx for example, you’ll have to dig for the corresponding settings for letsencrypt and certbot.\nWhat You Will Need Hugo You will need hugo, which can be downloaded from here: Hugo. A simple website will be enough. For themes, you can take a look at the humongous list located here: HugoThemes.\nHaproxy Haproxy can be found here: Haproxy. There are a number of options to install haproxy. I chose a simple apt-get install haproxy.\nLet’s Encrypt Information about Let’s Encrypt can be found on their website here: Let’s Encrypt. Let’s Encrypt’s client is now called Certbot which is used to generate the certificates. To get the latest code you either clone the repository Certbot, or use an auto downloader:\nuser@webserver:~$ wget https://dl.eff.org/certbot-auto user@webserver:~$ chmod a+x ./certbot-auto user@webserver:~$ ./certbot-auto --help Either way, I’m using the current latest version: v0.11.1.\nSudo This goes without saying, but that these operations will require you to have sudo privileges. I suggest staying in sudo for ease of use. This means that the commands, I’ll write here, will assume you are in sudo su mode thus no sudo prefix will be used.\nPortforwarding In order for your website to work under https this guide assumes that you have port 80 and 443 open on your router / network security group.\nSetup Single Server Environment It is possible for haproxy, certbot and your website to run on designated servers. Haproxy’s abilities allows to define multiple server sources. In this guide, my haproxy, website and certbot will all run on the same server; thus redirecting to 127.0.0.1 and local ips. This is more convenient, because otherwise the haproxy IP would have to be a permanent local/remote ip. Or an automated script would have to be setup which is notified upon IP change and updates the ip records.\nCreating a Certificate Diving in, the first thing you will require is a certificate. A certificate will allow for encrypted traffic and an authenticated website. Let’s Encrypt which is basically functioning as an independent, free, automated CA (Certificate Authority). Usually, the process would be to pay a CA to give you a signed, generated certificate for your website, and you would have to set that up with your DNS provider. Let’s Encrypt has that all automated, and free of any charge. Neat.\nCertbot So let’s get started. Clone the repository into /opt/letsencrypt for further usage.\ngit clone https://github.com/certbot/certbot /opt/letsencrypt Generating the certificate Make sure that there is nothing listening on ports: 80, 443. To list usage:\nnetstat -nlt | grep ':80\\s' netstat -nlt | grep ':443\\s' Kill everything that might be on these ports, like apache2 and httpd. These will be used by haproxy and certbot for challenges and redirecting traffic.\nYou will be creating a standalone certificate. This is the reason we need port 80 and 443 open. Run certbot by defining the certonly and --standalone flags. For domain validation you are going to use port 443, tls-sni-01 challenge. The whole command looks like this:\ncd /opt/letsencrypt ./certbot-auto certonly --standalone -d example.com -d www.example.com If this displays something like, “couldn’t connect” you probably still have something running on a port it tries to use. The generated certificate will be located under /etc/letsencrypt/archive and /etc/letsencrypt/keys while /etc/letsencrypt/live is a symlink to the latest version of the cert. It’s wise to not copy these away from here, since the live link is always updated to the latest version. Our script will handle haproxy, which requires one cert file made from privkey + fullchain|.pem files.\nSetup Auto-Renewal Let’s Encrypt issues short lived certificates (90 days). In order to not have to do this procedure every 89 days, certbot provides a nifty command called renew. However, for the cert to be generated, the port 443 has to be open. This means, haproxy needs to be stopped before doing the renew. Now, you COULD write a script which stops it, and after the certificate has been renewed, starts it again, but certbot has you covered again in that department. It provides hooks called pre-hook and post-hook. Thus, all you have to write is the following:\n#!/bin/bash  cd /opt/letsencrypt ./certbot-auto renew --pre-hook \"service haproxy stop\" --post-hook \"service haproxy start\" DOMAIN='example.com' sudo -E bash -c 'cat /etc/letsencrypt/live/$DOMAIN/fullchain.pem /etc/letsencrypt/live/$DOMAIN/privkey.pem \u003e /etc/haproxy/certs/$DOMAIN.pem' If you would like to test it first, just include the switch --dry-run.\nIn case of success you should see something like this:\nroot@raspberrypi:/opt/letsencrypt# ./certbot-auto renew --pre-hook \"service haproxy stop\" --post-hook \"service haproxy start\" --dry-run Saving debug log to /var/log/letsencrypt/letsencrypt.log ------------------------------------------------------------------------------- Processing /etc/letsencrypt/renewal/example.com.conf ------------------------------------------------------------------------------- Cert not due for renewal, but simulating renewal for dry run Running pre-hook command: service haproxy stop Renewing an existing certificate Performing the following challenges: tls-sni-01 challenge for example.com Waiting for verification... Cleaning up challenges Generating key (2048 bits): /etc/letsencrypt/keys/0002_key-certbot.pem Creating CSR: /etc/letsencrypt/csr/0002_csr-certbot.pem ** DRY RUN: simulating 'certbot renew' close to cert expiry ** (The test certificates below have not been saved.) Congratulations, all renewals succeeded. The following certs have been renewed: /etc/letsencrypt/live/example.com/fullchain.pem (success) ** DRY RUN: simulating 'certbot renew' close to cert expiry ** (The test certificates above have not been saved.) Running post-hook command: service haproxy start Put this script into a crontab to run every 89 days like this:\ncrontab -e # Open crontab for edit and paste in this line * * */89 * * /root/renew-cert.sh And you should be all set. Now we move on the configure haproxy to redirect and to use our newly generated certificate.\nHaproxy Like I said, haproxy requires a single file certificate in order to encrypt traffic to and from the website. To do this, we need to combine privkey.pem and fullchain.pem. As of this writing, there are a couple of solutions to automate this via a post hook on renewal. And also, there is an open ticket with certbot to implement a simpler solution located here: https://github.com/certbot/certbot/issues/1201. I, for now, have chosen to simply concatenate the two files together with cat like this:\nDOMAIN='example.com' sudo -E bash -c 'cat /etc/letsencrypt/live/$DOMAIN/fullchain.pem /etc/letsencrypt/live/$DOMAIN/privkey.pem \u003e /etc/haproxy/certs/$DOMAIN.pem' It will create a combined cert under /etc/haproxy/certs/example.com.pem.\nHaproxy configuration If haproxy happens to be running, stop it with service haproxy stop.\nFirst, save the default configuration file: cp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.old. Now, overwrite the old one with this new one (comments about what each setting does, are in-lined; they are safe to copy):\nglobal daemon # Set this to your desired maximum connection count. maxconn 2048 # https://cbonte.github.io/haproxy-dconv/configuration-1.5.html#3.2-tune.ssl.default-dh-param # bit setting for Diffie - Hellman key size. tune.ssl.default-dh-param 2048 defaults option forwardfor option http-server-close log global mode http option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000 errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http # In case it's a simple http call, we redirect to the basic backend server # which in turn, if it isn't an SSL call, will redirect to HTTPS that is # handled by the frontend setting called 'www-https'. frontend www-http # Redirect HTTP to HTTPS bind *:80 # Adds http header to end of end of the HTTP request reqadd X-Forwarded-Proto:\\ http # Sets the default backend to use which is defined below with name 'www-backend' default_backend www-backend # If the call is HTTPS we set a challenge to letsencrypt backend which # verifies our certificate and than direct traffic to the backend server # which is the running hugo site that is served under https if the challenge succeeds. frontend www-https # Bind 443 with the generated letsencrypt cert. bind *:443 ssl crt /etc/haproxy/certs/skarlso.com.pem # set x-forward to https reqadd X-Forwarded-Proto:\\ https # set X-SSL in case of ssl_fc \u003c- explained below http-request set-header X-SSL %[ssl_fc] # Select a Challenge acl letsencrypt-acl path_beg /.well-known/acme-challenge/ # Use the challenge backend if the challenge is set use_backend letsencrypt-backend if letsencrypt-acl default_backend www-backend backend www-backend # Redirect with code 301 so the browser understands it is a redirect. If it's not SSL_FC. # ssl_fc: Returns true when the front connection was made via an SSL/TLS transport # layer and is locally deciphered. This means it has matched a socket declared # with a \"bind\" line having the \"ssl\" option. redirect scheme https code 301 if !{ ssl_fc } # Server for the running hugo site. server www-1 192.168.0.17:8080 check backend letsencrypt-backend # Lets encrypt backend server server letsencrypt 127.0.0.1:54321 Save this, and start haproxy with services haproxy start. If you did everything right, it should say nothing. If, however, there went something wrong with starting the proxy, it usually displays something like this:\nJob for haproxy.service failed. See 'systemctl status haproxy.service' and 'journalctl -xn' for details. You can also gather some more information on what went wrong from less /var/log/haproxy.log.\nStarting the Server Everything should be ready to go. Hugo has the concept of a baseUrl. Everything that it loads, and tries to access will be prefixed with it. You can either set it through it’s config.yaml file, or from the command line.\nTo start the server, call this from the site’s root folder:\nhugo server --bind=192.168.x.x --port=8080 --baseUrl=https://example.com --appendPort=false Interesting thing here to note is https and the port. The IP could be 127.0.0.1 as well. I experienced problems though with not binding to network IP when I was debugging the site from a different laptop on the same network.\nOnce the server is started, you should be able to open up your website from a different browser, not on your local network, and see that it has a valid certificate installed. In Chrome you should see a green icon telling you that the cert is valid.\nLast Words And that is all. The site should be up and running and the proxy should auto-renew your site’s certificate. If you happened to change DNS or change the server, you’ll have to reissue the certificate.\nThanks for reading! Any questions or trouble setting something up, please feel free to leave a comment.\nCheers, Gergely.\n","title":"How to HTTPS with Hugo LetsEncrypt and HAProxy","uri":"/2017/02/15/how-to-https-with-hugo-letsencrypt-haproxy/"},{"content":"Intro Hi Folks.\nThis is a follow up on my previous post about Google Sign-In. In this post we will discover what to do with the information retrieved in the first encounter, which you can find here: Google Sign-In Part 1.\nForewords The Project Everything I did in the first post, and that I’m going to do in this example, can be found in this project: Google-OAuth-Go-Sample.\nJust to recap, we left off previously on the point where we successfully obtained information about the user, with a secure token and a session initiated with them. Google nicely enough provided us with some details which we can use. This information was in JSON format and looked something like this:\n{ \"sub\": \"1111111111111111111111\", \"name\": \"Your Name\", \"given_name\": \"Your\", \"family_name\": \"Name\", \"profile\": \"https://plus.google.com/1111111111111111111111\", \"picture\": \"https://lh3.googleusercontent.com/asdfadsf/AAAAAAAAAAI/Aasdfads/Xasdfasdfs/photo.jpg\", \"email\": \"your@gmail.com\", \"email_verified\": true, \"gender\": \"male\" } In my example, to keep things simple, I will use the email address since that has to be unique in the land of Google. You could assign an ID to the user, and you could complicate things even further, but my goal is not to write an academic paper about cryptography here.\nImplementation Making something useful out of the data In order for the app to recognise a user it must save some data about the user. I’m doing that in MongoDB right now, but that could be any form of persistence layer, like, SQLite3, BoltDB, PostgresDB, etc.\nAfter successful user authorization Once the user used google to provide us with sufficient information about him/herself, we can retrieve data about that user from our records. The data could be anything that is linked to our unique identifier like: Character Profile, Player Information, Status, Last Logged-In, etcetc. For this, there are two things that need to happen after authorization: Save/Load user information and initiate a session.\nThe session can be in the form of a cookie, or a Redis storage, or URL re-writing. I’m choosing a cookie here.\nSave / Load user information All I’m doing is a simple, returning / new user handling. The concept is simple. If the email isn’t saved, we save it. If it’s saved, we set a logic to our page render to greet the returning user.\nIn the AuthHandler I’m doing the following:\n... seen := false db := database.MongoDBConnection{} if _, mongoErr := db.LoadUser(u.Email); mongoErr == nil { seen = true } else { err = db.SaveUser(\u0026u) if err != nil { log.Println(err) c.HTML(http.StatusBadRequest, \"error.tmpl\", gin.H{\"message\": \"Error while saving user. Please try again.\"}) return } } c.HTML(http.StatusOK, \"battle.tmpl\", gin.H{\"email\": u.Email, \"seen\": seen}) ... Let’s break this down a bit. There is a db connection here, which calls a function that either returns an error, or it doesn’t. If it doesn’t, that means we have our user. If it does, it means we have to save the user. This is a very simple case (disregard for now, that the error could be something else as well (If you can’t get passed that, you could type check the error or check if the returned record contains the requested user information instead of checking for an error.)).\nThe template is than rendered depending on the seen boolean like this:\n\u003c!DOCTYPE html\u003e \u003clink rel=\"icon\" type=\"image/png\" href=\"/img/favicon.ico\" /\u003e \u003chtml\u003e \u003chead\u003e \u003clink rel=\"stylesheet\" href=\"/css/main.css\"\u003e \u003c/head\u003e \u003cbody\u003e {{if .seen}} \u003ch1\u003eWelcome back to the battlefield '{{ .email }}'.\u003c/h1\u003e {{else}} \u003ch1\u003eWelcome to the battlefield '{{ .email }}'.\u003c/h1\u003e {{end}} \u003c/body\u003e \u003c/html\u003e You can see here, that if seen is true the header message will say: “Welcome back…”.\nInitiating a session When the user is successfully authenticated, we activate a session so that the user can access pages that require authorization. Here, I have to mention that I’m using Gin, so restricted end-points are made with groups which require a middleware.\nAs I mentioned earlier, I’m using cookies as session handlers. For this, a new session store has to be created with some secure token. This is achieved with the following code fragments ( note that I’m using a Gin session middleware which uses gorilla’s session handler located here: Gin-Gonic(Sessions)):\n// RandToken in handlers.go: // RandToken generates a random @l length token. func RandToken(l int) string { b := make([]byte, l) rand.Read(b) return base64.StdEncoding.EncodeToString(b) } // quest.go: // Create the cookie store in main.go. store := sessions.NewCookieStore([]byte(handlers.RandToken(64))) store.Options(sessions.Options{ Path: \"/\", MaxAge: 86400 * 7, }) // using the cookie store: router.Use(sessions.Sessions(\"goquestsession\", store)) After this gin.Context lets us access this session store by doing session := sessions.Default(c). Now, create a session variable called user-id like this:\nsession.Set(\"user-id\", u.Email) err = session.Save() if err != nil { log.Println(err) c.HTML(http.StatusBadRequest, \"error.tmpl\", gin.H{\"message\": \"Error while saving session. Please try again.\"}) return } Don’t forget to save the session. ;) That is it. If I restart the server, the cookie won’t be usable any longer, since it will generate a new token for the cookie store. The user will have to log in again. Note: It might be that you’ll see something like this, from session: [sessions] ERROR! securecookie: the value is not valid. You can ignore this error.\nRestricting access to certain end-points with the auth Middleware™ Now, that our session is alive, we can use it to restrict access to some part of the application. With Gin, it looks like this:\nauthorized := router.Group(\"/battle\") authorized.Use(middleware.AuthorizeRequest()) { authorized.GET(\"/field\", handlers.FieldHandler) } This creates a grouping of end-points under /battle. Which means, everything under /battle will only be accessible if the middleware passed to the Use function calls the next handler in the chain. If it aborts the call chain, the end-point will not be accessible. My middleware is pretty simple, but it gets the job done:\n// AuthorizeRequest is used to authorize a request for a certain end-point group. func AuthorizeRequest() gin.HandlerFunc { return func(c *gin.Context) { session := sessions.Default(c) v := session.Get(\"user-id\") if v == nil { c.HTML(http.StatusUnauthorized, \"error.tmpl\", gin.H{\"message\": \"Please log in.\"}) c.Abort() } c.Next() } } Note, that this only check if user-id is set or not. That’s certainly not enough for a secure application. Its only supposed to be a simple example of the mechanics of the auth middleware. Also, the session usually contains more than one parameter. It’s more likely that it contains several variables, which describe the user including a state for CORS protection. For CORS I’d recommend using rs/cors.\nIf you would try to access http://127.0.0.1:9090/battle/field without logging in, you’d be redirected to an error.tmpl with the message: Please log in..\nFinal Words That’s pretty much it. Important parts are:\n Saving the right information Secure cookie store CORS for sessions Checks of the users details in the cookie Authorised end-points Session handling  Any questions, remarks, ideas, are very welcomed in the comment section. There are plenty of very nice Go frameworks which do Google OAuth2 out of the box. I recommend using them, as they save you a lot of legwork.\nThank you for reading! Gergely.\n","title":"How to do Google Sign-In with Go - Part 2","uri":"/2016/11/02/google-signin-with-go-part2/"},{"content":"Intro Hey folks.\nSo, there is this project called Huginn which I absolutely love.\nBut the thing is, that for a couple of scrappers ( at least for me ), I don’t want to spin up a whole rails app.\nHence, I’ve come up with RScrap. Which is a bunch of Ruby scripts run as cron jobs on a raspberry pi. And because I dislike emails as well, and most of the time, I don’t read them, I opted for a nicer solution. Enter the world of Telegram. They provide you with the ability to create bots. You basically get an API key, and than using that key, you can send private messages, or even create an interactive bot which you can send messages too.\nIn my simple example, I’m using it to send private messages to myself, but I could just as well, make it interactive and than tell it to run one of the scripts.\nThe Code Let’s take a look at what we got.\nThe main scraper The main scraper, is simply bunch of convenience methods that wrap handling and working with the database and the telegram bot. That’s all. It’s very simple. Very short. The Telegram part is just this bit:\ndef send_message(text) Telegram::Bot::Client.run(@token) do |bot| bot.api.send_message(chat_id: @id, text: text) end end Straightforward. Creating an interactive bot, would look something like this:\n#!/usr/bin/env ruby require 'telegram/bot' token = 'YOUR_TELEGRAM_BOT_API_TOKEN' Telegram::Bot::Client.run(token) do |bot| bot.listen do |message| case message.text when '/start' bot.api.send_message(chat_id: message.chat.id, text: \"Hello, #{message.from.first_name}\") when '/stop' bot.api.send_message(chat_id: message.chat.id, text: \"Bye, #{message.from.first_name}\") end end end Basically, it will listen, and than you can send it messages and based on the parsed message.text you can define functions to call. For example, for rscrap I could define something like run_script(script). And the command would be: /run reddit. Which will execute my reddit script. The possibilities are endless.\nThe scripts The scripts use nokogiri to parse a web page, and than return a URL which will be sent by the TelegramBot. They are also saved in the database so that when a new comic strip comes out, I know that it’s new. For reddit, I’m saving a timestamp as well, and I collect everything after that timestamp through the reddit API as JSON, and send it as a bundled message with shortified links to the posts using bit.ly.\nThe scraping is most of the times the same for every comic. Thus, there is a helper method for it. The script itself, is very short. For example, lets look at gunnerkrigg court.\nrequire_relative '../rscrap' require 'nokogiri' require 'open-uri' url = 'http://www.gunnerkrigg.com' scrap = Rscrap.new page = Nokogiri::HTML(open(url)) comic_id = page.css('img.comic_image')[0].select { |e| e if e[0] == 'src' }[0][1] new_comic = \"#{url}#{comic_id}\" scrap.send_new_comic(url, new_comic) The interesting part of it is this bit: comic_id = page.css('img.comic_image')[0].select { |e| e if e[0] == 'src' }[0][1]. It extracts the URL for the comic image, and stores it as an “id” of the comic. This than, is sent as a message which Telegram will embed. There is no need to visit the web page, the image is in your feed and you can view it directly. Just like an RSS ready.\nCron These scripts are best used in a cron job. The comics are usually running with a daily frequency, where as the reddit gatherer is running with an hour frequency. Basically, I’m receiving updates on an hourly basis if there are new posts by then. Running ruby from cron was a bit tricky. I’m using bundler for the environment, and came up with this:\n0 6-23 * * * /bin/bash -l -c 'cd /home/\u003cyouruser\u003e/rubyproj/rscrap \u0026\u0026 bundle exec ruby scripts/reddit.rb' 0 8,22 * * * /bin/bash -l -c 'cd /home/\u003cyouruser\u003e/rubyproj/rscrap \u0026\u0026 bundle exec ruby scripts/gunnerkrigg.rb' 0 8,22 * * * /bin/bash -l -c 'cd /home/\u003cyouruser\u003e/rubyproj/rscrap \u0026\u0026 bundle exec ruby scripts/aws_blog.rb' 0 5,23 * * * /bin/bash -l -c 'cd /home/\u003cyouruser\u003e/rubyproj/rscrap \u0026\u0026 bundle exec ruby scripts/goblinscomic.rb' 0 6,20 * * * /bin/bash -l -c 'cd /home/\u003cyouruser\u003e/rubyproj/rscrap \u0026\u0026 bundle exec ruby scripts/xkcd.rb' 0 7,19 * * * /bin/bash -l -c 'cd /home/\u003cyouruser\u003e/rubyproj/rscrap \u0026\u0026 bundle exec ruby scripts/commitstrip.rb' 0 8 * * * /bin/bash -l -c 'cd /home/\u003cyouruser\u003e/rubyproj/rscrap \u0026\u0026 bundle exec ruby scripts/sequiential_art.rb' And a telegram message for all these things, looks like this: Reddit: Comics: Conclusion That’s it folks. Adding a new scraper is easy. I added the aws blog as a new entry as well by just copying the comics scripts. And I’m also getting Weather Reports delivered every morning to me.\nHave fun. Any questions, please feel free to leave a comment!\nThanks, Gergely.\n","title":"RScrap scraper","uri":"/2016/10/06/rscrap-ruby-scraping-with-cronjob-scripts/"},{"content":"Intro Hello folks.\nToday, I would like to tell you about my configuration for a low budget Home Theater setup.\nMy tools are as follows:\n FLIRC Raspberry Pi 2 500G SSD An a good ‘ol wifi  TL;DR Use Flirc for remote control, omxplayer for streaming the movie from an SSD on a headless PI controller via SSH and enjoy a nice, cold Lemon - Menta beer.\nFlirc First, the remote control. So, I like to sit in my couch and watch the movie from there. I hate getting up, or having a keyboard at arm length to control the pi. Flirc is a very easy way of doing just that with a simple remote control.\nIt costs ~$22 and is easy to setup. Works with any kind of remote control. Setting up key bindings for the control, is as simple as starting the Flirc software and pressing buttons on the remote to map to keyboard keys. Now, my pi is running headless, and the Flirc binary isn’t quite working with raspbian; so to do the binding, I just did that on my main machine. When I was done, I just plugged in the Flirc, and proceeded to setup the pi.\nRaspberry Pi 2 The pi 2 is a small powerhouse. However, the SD card on which it sits is simply not fast enough. From time to time, I experienced lateness in sound, or stutter in video. So, instead of having the movie on the pi, I’m streaming through a faster SSD with SSHFS. For playing, I’m using omxplayer. With omxplayer, I had a few problems, because sound was not coming through the HDMI cable. A little bit of research lead me to this change in the pi’s boot config. Uncomment this line:\n#hdmi_driver=2 After rebooting, I also, did this thing:\nsudo apt-get install alsa-utils sudo modprobe snd_bcm2835 sudo amixer -c 0 cset numid=3 2 This saved my bacon. The whole answer can be found here: Stackoverflow.\nOnce SSHFS was working, and HDMI received sound, I just executed this command: omxplayer -o hdmi /media/stream/my_movie.mkv. This told omxplayer to use the local HDMI connection for video output.\nAll this was from my computer through an SSH session so I never controlled the pi directly. Once done, I proceeded to sit down with a nice, cold Lemon - Menta beer and a remote control.\nOnce little gotcha – omxplayer is controlled through the buttons + (volume up), - (volume down), (stop, play), and q for quitting. Flirc is able to map any key combinations on a keyboard as well to any button on the remote. Combinations can be done by selecting a control key and pressing another key. So mapping + to the volume up button was by pressing shift and then ‘=’.\nWrapping Up I enjoyed the movie while being able to adjust the volume, or pause it, when my popcorn was ready, and close the player when the movie was done. There are a number of other ways to do this, like using kodi + yatse. Which lets you remote control a media software with your mobile phone. But I’m using the pi for a number of other things and the GUI is rather resource heavy.\nThere you have it folks. Might not be the easiest setup, but it’s pretty awesome anyways.\nCheers, Gergely.\n","title":"Budget Home Theather with a Headless Raspberry Pi and Flirc for Remote Controlling","uri":"/2016/09/17/simple-hometheater-with-remote-and-flirc/"},{"content":"Another quick reminder… Always go with []byte if possible. I said it before, and I’m going to say it over and over again. It’s crucial.\nHere is a little code from exercism.io. First, with strings:\npackage igpay import ( \"strings\" ) // PigLatin translates reguler old English into awesome pig-latin. func PigLatin(in string) (ret string) { for _, v := range strings.Fields(in) { ret += pigLatin(v) + \" \" } return strings.Trim(ret, \" \") } func pigLatin(in string) (ret string) { if strings.IndexAny(in, \"aeiou\") == 0 { ret += in + \"ay\" return } for i := 0; i \u003c len(in); i++ { vowelPos := strings.IndexAny(in, \"aeiou\") if (in[0] == 'y' || in[0] == 'x') \u0026\u0026 vowelPos \u003e 1 { vowelPos = 0 ret = in } if vowelPos != 0 { adjustPosition := vowelPos if in[adjustPosition] == 'u' \u0026\u0026 in[adjustPosition - 1] == 'q' { adjustPosition++ } ret = in[adjustPosition:] + in[:adjustPosition] } } ret += \"ay\" return } Than with []byte:\npackage igpay import ( // \"fmt\"  \"bytes\" ) // PigLatin translates reguler old English into awesome pig-latin. func PigLatin(in string) (ret string) { inBytes := []byte(in) var retBytes [][]byte for _, v := range bytes.Fields(inBytes) { v2 := make([]byte, len(v)) copy(v2, v) retBytes = append(retBytes, pigLatin(v2)) } ret = string(bytes.Join(retBytes, []byte(\" \"))) return } func pigLatin(in []byte) (ret []byte) { if bytes.IndexAny(in, \"aeiou\") == 0 { ret = append(in, []byte(\"ay\")...) return } for i := 0; i \u003c len(in); i++ { vowelPos := bytes.IndexAny(in, \"aeiou\") if (in[0] == 'y' || in[0] == 'x') \u0026\u0026 vowelPos \u003e 1 { vowelPos = 0 ret = in } if vowelPos != 0 { adjustPosition := vowelPos if in[adjustPosition] == 'u' \u0026\u0026 in[adjustPosition - 1] == 'q' { adjustPosition++ } in = append(in[adjustPosition:], in[:adjustPosition]...) ret = in // fmt.Printf(\"%s\\n\", ret)  } } ret = append(ret, []byte(\"ay\")...) return } And than,the benchmarks of course:\nBenchmarkPigLatin-8 200000\t10688 ns/op BenchmarkPigLatinStrings-8 100000\t15211 ns/op PASS The improvement is not massive in this case, but it’s more than enough to matter. And in a bigger, more complicated program, string concatenation will take a LOT of time away.\nIn Go, the bytes package has a 1-1 map compared to the strings packages, so chances are, if you are doing strings concatenations you will be able to port that piece of code easily to []byte.\nThat’s all folks.\nHappy coding, Gergely.\n","title":"Always Go with []byte","uri":"/2016/08/19/always-go-with-bytes/"},{"content":"Quick reminder. If you have a never changing regex in Go, do NOT put it into a frequently called function. ALWAYS put it into a global variable. I’ll show you why.\nBenchmark for code with a variable in a frequently called function:\nBenchmarkNumber-8 30000\t41633 ns/op BenchmarkAreaCode-8 50000\t27736 ns/op BenchmarkFormat-8 50000\t29263 ns/op PASS ok _/phone-number\t5.110s Benchmark for code with the same variable outside in a global scope:\nBenchmarkNumber-8 300000\t5618 ns/op BenchmarkAreaCode-8 500000\t3884 ns/op BenchmarkFormat-8 300000\t4696 ns/op PASS ok _/phone-number\t5.197s Notice the magnitude change in ns/op! That’s something to keep an eye out for.\nThanks for reading! Cheers, Gergely.\n","title":"Global variable for never changing regex","uri":"/2016/08/16/never-changing-regex/"},{"content":"Hi folks.\nQuick gotcha, when working with Drupal. If you just freshly installed it, and everything seems to work fine, and yet you are experiencing things like, the admin toolbar is randomly disappearing, or configuration is not saved; than you might not have modrewrite enabled on your apache server.\nBecause, by default, Drupal has clean url enabled, that needs URL rewriting on apache.\nSo, step one.\nHave this in your .htaccess file:\n\u003cIfModule mod_rewrite.c\u003e RewriteEngine on ... # and than a bunch of rewrite rules according to your leisure Than look up this line in your httpd.conf file and remove the prefix ‘#’.\n#LoadModule rewrite_module libexec/apache2/mod_rewrite.so That is all. From there on, everything should work. If, you don’t want the clean url setting, yet you can’t disable it, and don’t want to restart the server and edit the settings.php file; use drush like this:\ndrush vset clean_url 0 --yes This should disable it and bust the cache in the process so it’s immediately visible.\nThat is all folks.\nCheers, Gergely.\n","title":"Drupal missing ToolBar and settings not saving","uri":"/2016/08/13/drupal-missing-toolbar-and-settings-not-saving/"},{"content":"Hi folks.\nI wanted to take the time to share with you a talk that I recently did.\nThe slides and the source I used, can be found here: Github.\nAnd then, there is also a docker image which contains all the plugins, job configurations and all the practices which I did during the talk. Please feel free to have a go with it. DockerHub - Jenkins Best Practices.\nFor easy access and reading, here are the slides on Slideshare: Jenkins Best Practices Slides.\nI, gladly answer any questions which should arise.\nThanks! Gergely.\n","title":"Jenkins Best Practices Talk","uri":"/2016/07/28/jenkins-best-practices/"},{"content":"Though it could be done better, I’m sure, but I’m actually pretty satisfied with this one. It loops only twice as opposed to filtered ranges and whatnot other solutions to the sieve. I was thinking of rather creating a list and deleting elements from it, but that’s already three loops.\nMaybe I’ll do a benchmark later on more solutions.\n# Sieve contains a function to return a set of primes class Sieve def initialize(n) @n = n end # Returns a list of primes up to a certain limit # @param n limit # @return list of primes def primes marked = [] primes = [] (2..@n).each do |e| unless marked.include?(e) primes.push e (e..@n).step(e) { |s| marked.push s } end end primes end end Cheers, Gergely.\n","title":"Ruby Sieve","uri":"/2016/07/12/ruby-sieve/"},{"content":"Hi folks.\nThis is but a simple git hook to run a test in order to ensure you can push. It also ignores the vendor folder if you happen to have on in your directory.\nEdit the file under .git/hooks/pre-push.sample and add this at the end before the exit 0.\ngo test $(go list ./... |grep -v vendor) RESULT=$? if [ $RESULT -ne 0 ]; then echo \"Failed test run. Disallowing push.\" exit 1 fi After this, rename the file to pre-push removing the .sample from it.\nIf you now, mess something up, you should see something like this before your push:\n# github.com/Skarlso/goprogressquest ./create.go:40: undefined: sha1 in sha1.Sum ./create.go:41: undefined: fmt in fmt.Sprintf ./create.go:115: undefined: json in json.Unmarshal ./create.go:130: undefined: json in json.Unmarshal FAIL\tgithub.com/Skarlso/goprogressquest [build failed] Failed test run. Disallowing push. error: failed to push some refs to 'git@github.com:Skarlso/goprogressquest.git' That is all.\nCheers, Gergely.\n","title":"Simple hook to rid of trouble","uri":"/2016/07/12/simple-hook-to-rid-of-trouble/"},{"content":"Hi folks.\nToday, I would like to write up a step - by - step guide with a sample web app on how to do Google Sign-In and authorization.\nLet’s get started.\nEDIT: A sample project of this, and Part 2, can be found here or here.\nSetup Google OAuth token First what you need is, to register your application with Google, so you’ll get a Token that you can use to authorize later calls to Google services.\nYou can do that here: Google Developer Console. You’ll have to create a new project. Once it’s done, click on Credentials and create an OAuth token. You should see something like this: “To create an OAuth client ID, you must first set a product name on the consent screen.”. Go through the questions, like, what type your application is, and once you arrive at stage where it’s asking for your application’s name – there is a section asking for redirect URLs; there, write the url you wish to use when authorising your user. If you don’t know this yet, don’t fret, you can come back and change it later. Do NOT use localhost. If you are running on your own, use http://127.0.0.1:port/whatever.\nThis will get you a client ID and a client secret. I’m going to save these into a file which will sit next to my web app. It could be stored more securely, for example, in a database or a mounted secure, encrypted drive, and so and so forth.\nYour application can now be identified through Google services.\nThe Application Libraries Google has a nice library to use with OAuth 2.0. The library is available here: Google OAth 2.0. It’s a bit cryptic at first, but not to worry. After a bit of fiddling you’ll understand fast what it does. I’m also using Gin, and Gin’s session handling middleware Gin-Session.\nSetup - Credentials Let’s create a setup which configures your credentials from the file you saved earlier. This is pretty straightforward.\n// Credentials which stores google ids. type Credentials struct { Cid string `json:\"cid\"` Csecret string `json:\"csecret\"` } func init() { var c Credentials file, err := ioutil.ReadFile(\"./creds.json\") if err != nil { fmt.Printf(\"File error: %v\\n\", err) os.Exit(1) } json.Unmarshal(file, \u0026c) } Once you have the creds loaded, you can now go on to construct the OAuth client.\nSetup - OAuth client Construct the OAuth config like this:\nconf := \u0026oauth2.Config{ ClientID: c.Cid, ClientSecret: c.Csecret, RedirectURL: \"http://localhost:9090/auth\", Scopes: []string{ \"https://www.googleapis.com/auth/userinfo.email\", // You have to select your own scope from here -\u003e https://developers.google.com/identity/protocols/googlescopes#google_sign-in  }, Endpoint: google.Endpoint, } It will give you a struct which you can then use to Authorize the user in the google domain. Next, all you need to do is call AuthCodeURL on this config. It will give you a URL which redirects to a Google Sign-In form. Once the user fills that out and clicks ‘Allow’, you’ll get back a TOKEN in the code query parameter and a state which helps protect against CSRF attacks. Always check if the provided state is the same which you provided with AuthCodeURL. This will look something like this http://127.0.0.1:9090/auth?code=4FLKFskdjflf3343d4f\u0026state=lhfu3f983j;asdf. Small function for this:\nfunc getLoginURL(state string) string { // State can be some kind of random generated hash string.  // See relevant RFC: http://tools.ietf.org/html/rfc6749#section-10.12  return conf.AuthCodeURL(state) } Construct a button which the user can click and be redirected to the Google Sign-In form. When constructing the url, we must do one more thing. Create a secure state token and save it in the form of a cookie for the current user.\nRandom State and Button construction Small piece of code random token:\nfunc randToken() string { b := make([]byte, 32) rand.Read(b) return base64.StdEncoding.EncodeToString(b) } Storing it in a session and constructing the button:\nfunc loginHandler(c *gin.Context) { state = randToken() session := sessions.Default(c) session.Set(\"state\", state) session.Save() c.Writer.Write([]byte(\"\u003chtml\u003e\u003ctitle\u003eGolang Google\u003c/title\u003e \u003cbody\u003e \u003ca href='\" + getLoginURL() + \"'\u003e\u003cbutton\u003eLogin with Google!\u003c/button\u003e \u003c/a\u003e \u003c/body\u003e\u003c/html\u003e\")) } It’s not the nicest button I ever come up with, but it will have to do.\nUser Information After you got the token, you can construct an authorised Google HTTP Client, which let’s you call Google related services and retrieve information about the user.\nGetting the Client Before we construct a client, we must check if the retrieved state is still the same compared to the one we provided. I’m doing this before constructing the client. Together this looks like this:\nfunc authHandler(c *gin.Context) { // Check state validity.  session := sessions.Default(c) retrievedState := session.Get(\"state\") if retrievedState != c.Query(\"state\") { c.AbortWithError(http.StatusUnauthorized, fmt.Errorf(\"Invalid session state: %s\", retrievedState)) return } // Handle the exchange code to initiate a transport.  tok, err := conf.Exchange(oauth2.NoContext, c.Query(\"code\")) if err != nil { c.AbortWithError(http.StatusBadRequest, err) return } // Construct the client.  client := conf.Client(oauth2.NoContext, tok) ... Obtaining information Our next step is to retrieve information about the user. To achieve this, call Google’s API with the authorised client. The code for that is:\n... resp, err := client.Get(\"https://www.googleapis.com/oauth2/v3/userinfo\") if err != nil { c.AbortWithError(http.StatusBadRequest, err) return } defer resp.Body.Close() data, _ := ioutil.ReadAll(resp.Body) log.Println(\"Resp body: \", string(data)) ... And this will yield a body like this:\n{ \"sub\": \"1111111111111111111111\", \"name\": \"Your Name\", \"given_name\": \"Your\", \"family_name\": \"Name\", \"profile\": \"https://plus.google.com/1111111111111111111111\", \"picture\": \"https://lh3.googleusercontent.com/asdfadsf/AAAAAAAAAAI/Aasdfads/Xasdfasdfs/photo.jpg\", \"email\": \"your@gmail.com\", \"email_verified\": true, \"gender\": \"male\" } Parse it, and you’ve got an email which you can store somewhere for registration purposes. At this point, your user is not yet Authenticated. For that, I’m going to post a second post, which describes how to go on. Retrieving the stored email address, and user session handling with Gin and MongoDB.\nPutting it all together package main import ( \"crypto/rand\" \"encoding/base64\" \"encoding/json\" \"io/ioutil\" \"fmt\" \"log\" \"os\" \"net/http\" \"github.com/gin-gonic/contrib/sessions\" \"github.com/gin-gonic/gin\" \"golang.org/x/oauth2\" \"golang.org/x/oauth2/google\" ) // Credentials which stores google ids. type Credentials struct { Cid string `json:\"cid\"` Csecret string `json:\"csecret\"` } // User is a retrieved and authentiacted user. type User struct { Sub string `json:\"sub\"` Name string `json:\"name\"` GivenName string `json:\"given_name\"` FamilyName string `json:\"family_name\"` Profile string `json:\"profile\"` Picture string `json:\"picture\"` Email string `json:\"email\"` EmailVerified string `json:\"email_verified\"` Gender string `json:\"gender\"` } var cred Credentials var conf *oauth2.Config var state string var store = sessions.NewCookieStore([]byte(\"secret\")) func randToken() string { b := make([]byte, 32) rand.Read(b) return base64.StdEncoding.EncodeToString(b) } func init() { file, err := ioutil.ReadFile(\"./creds.json\") if err != nil { log.Printf(\"File error: %v\\n\", err) os.Exit(1) } json.Unmarshal(file, \u0026cred) conf = \u0026oauth2.Config{ ClientID: cred.Cid, ClientSecret: cred.Csecret, RedirectURL: \"http://127.0.0.1:9090/auth\", Scopes: []string{ \"https://www.googleapis.com/auth/userinfo.email\", // You have to select your own scope from here -\u003e https://developers.google.com/identity/protocols/googlescopes#google_sign-in  }, Endpoint: google.Endpoint, } } func indexHandler(c *gin.Context) { c.HTML(http.StatusOK, \"index.tmpl\", gin.H{}) } func getLoginURL(state string) string { return conf.AuthCodeURL(state) } func authHandler(c *gin.Context) { // Handle the exchange code to initiate a transport.  session := sessions.Default(c) retrievedState := session.Get(\"state\") if retrievedState != c.Query(\"state\") { c.AbortWithError(http.StatusUnauthorized, fmt.Errorf(\"Invalid session state: %s\", retrievedState)) return } tok, err := conf.Exchange(oauth2.NoContext, c.Query(\"code\")) if err != nil { c.AbortWithError(http.StatusBadRequest, err) return } client := conf.Client(oauth2.NoContext, tok) email, err := client.Get(\"https://www.googleapis.com/oauth2/v3/userinfo\") if err != nil { c.AbortWithError(http.StatusBadRequest, err) return } defer email.Body.Close() data, _ := ioutil.ReadAll(email.Body) log.Println(\"Email body: \", string(data)) c.Status(http.StatusOK) } func loginHandler(c *gin.Context) { state = randToken() session := sessions.Default(c) session.Set(\"state\", state) session.Save() c.Writer.Write([]byte(\"\u003chtml\u003e\u003ctitle\u003eGolang Google\u003c/title\u003e \u003cbody\u003e \u003ca href='\" + getLoginURL(state) + \"'\u003e\u003cbutton\u003eLogin with Google!\u003c/button\u003e \u003c/a\u003e \u003c/body\u003e\u003c/html\u003e\")) } func main() { router := gin.Default() router.Use(sessions.Sessions(\"goquestsession\", store)) router.Static(\"/css\", \"./static/css\") router.Static(\"/img\", \"./static/img\") router.LoadHTMLGlob(\"templates/*\") router.GET(\"/\", indexHandler) router.GET(\"/login\", loginHandler) router.GET(\"/auth\", authHandler) router.Run(\"127.0.0.1:9090\") } } This is it folks. I hope this helped. Any comments or advices are welcomed.\nGoogle API Documentation The documentation to this whole process, and MUCH more information can be found here: Google API Docs.\nThanks for reading, Gergely.\n","title":"How to do Google sign-in with Go","uri":"/2016/06/12/google-signin-with-go/"},{"content":"Hi folks.\nGot an update for the backup script. This time, you’ll have the ability to implement your own upload capabilities. I provide a mock implementation for the required functions.\nHere is the script again, now modified and a bit cleaned up. I hope it’s helpful.\n#!/bin/bash  if [[ -t 1 ]]; then colors=$(tput colors) if [[ $colors ]]; then RED='\\033[0;31m' LIGHT_GREEN='\\033[1;32m' NC='\\033[0m' fi fi if [[ -z ${MINECRAFT_BUCKET} ]]; then printf \"Please set the env variable %bMINECRAFT_BUCKET%b to the s3 archive bucket name.\\n\" \"${RED}\" \"${NC}\" exit 1 fi if [[ -z ${MINECRAFT_ARCHIVE_LIMIT} ]]; then printf \"Please set the env variable %bMINECRAFT_ARCHIVE_LIMIT%b to limit the number of archives to keep.\\n\" \"${RED}\" \"${NC}\" exit 1 fi if [[ -z ${MINECRAFT_WORLD} ]]; then printf \"Please set the env variable %bMINECRAFT_WORLD%b to specify what world to back-up.\\n\" \"${RED}\" \"${NC}\" exit 1 fi backup_world=${MINECRAFT_WORLD} backup_bucket=${MINECRAFT_BUCKET} backup_limit=${MINECRAFT_ARCHIVE_LIMIT} archive_name=\"${backup_world}-$(date +\"%H-%M-%S-%m-%d-%Y\").zip\" function create_archive { printf \"Creating archive of %b${backup_world}%b\\n\" \"${RED}\" \"${NC}\" zip -r $archive_name $backup_world } function amazon_bak { create_archive printf \"Checking if bucket has more than %b${backup_limit}%b files already.\\n\" \"${RED}\" \"${NC}\" content=( $(aws s3 ls s3://$backup_bucket | awk '{print $4}') ) if [[ ${#content[@]} -eq $backup_limit || ${#content[@]} -gt $backup_limit ]]; then echo \"There are too many archives. Deleting oldest one.\" # We can assume here that the list is in cronological order printf \"%bs3://${backup_bucket}/${content[0]}\\n%b\" \"${RED}\" \"${NC}\" aws s3 rm s3://$backup_bucket/${content[0]} fi printf \"Uploading %b${archive_name}%b to s3 archive bucket.\\n\" \"${RED}\" \"${NC}\" state=$(aws s3 cp $archive_name s3://$backup_bucket) if [[ \"$state\" =~ \"upload:\" ]]; then printf \"File upload %bsuccessful%b.\\n\" \"${LIGHT_GREEN}\" \"${NC}\" else printf \"%bError%b occured while uploading archive. Please investigate.\\n\" \"${RED}\" \"${NC}\" fi } function custom { if [[ -e custom.sh ]]; then source ./custom.sh else echo \"custom.sh script not found. Please implement the apropriate functions.\" exit 1 fi echo \"Checking for the number of files. Limit is: $backup_limit.\" files=( $(list) ) if [[ ${#files[@]} -eq $backup_limit || ${#files[@]} -gt $backup_limit ]]; then echo \"Deleting extra file.\" delete ${files[0]} if [[ $? != 0 ]]; then printf \"%bFailed%b to delete file. Please investigate failure.\" \"${RED}\" \"${NC}\" exit $? fi fi echo \"Zipping world.\" create_archive echo \"Uploading world.\" upload $archive_name if [[ $? != 0 ]]; then printf \"%bFailed%b to upload archive. Please investigate the error.\" \"${RED}\" \"${NC}\" exit $? fi printf \"Upload %bsuccessful%b\" \"${LIGHT_GREEN}\" \"${NC}\" } function help { echo \"Usage:\" echo \"./backup_world [METHOD]\" echo \"Exp.: ./backup_world aws|./backup_world custom|./backup_world dropbox\" echo \"Each method has it's own environment properties that it requires.\" echo \"Global: MINECRAFT_WORLD|MINECRAFT_BUCKET|MINECRAFT_ARCHIVE_LIMIT\" echo \"Custom: Have a file, called 'custom.sh' which is sourced.\" echo \"Implement these three functions: upload | list | delete.\" echo \"upload -\u003e should return exit code 0 on success, should return exit code 1 on failure.\" echo \"list -\u003e should return a list of cronologically ordered items.\" echo \"delete -\u003e should return exit code 0 on success, should return exit code 1 on failure.\" } case $1 in aws ) amazon_bak ;; custom ) custom ;; * ) help ;; esac And here is the sample implementation for the custom upload functionality.\n#!/bin/bash  function upload { echo \"uploading\" local result=0 return $result } function delete { echo \"deleting $1\" local result=0 return $result } function list { local arr=(\"file1\" \"file2\" \"file3\") echo \"${arr[@]}\" } Thanks for reading!\nGergely.\n","title":"Minecraft world automatic backup to AWS S3 bucket - Part 2 (Custom functions)","uri":"/2016/04/17/minecraft-server-aws-s3-backup-part2/"},{"content":"Hi Folks.\nPreviously we created a Minecraft server using Docker. After my server got popular in the family, and a lot of stuff started to pile up on it, as a good IT person, I’m backing up the world once in a while.\nFor that, I’m using AWS S3 with the CLI and a little bash script which runs once a week.\nThe script is really straightforward. I’m doing manual versioning, although S3 does provide one out of the box. However, amazon’s S3 versioning doesn’t allow limiting the number of versions being kept. And since I’m doing that anyways, might as well take care of the rest.\nWithout further ado, here is the script:\n#!/bin/bash  if [[ -t 1 ]]; then colors=$(tput colors) if [[ $colors ]]; then RED='\\033[0;31m' LIGHT_GREEN='\\033[1;32m' NC='\\033[0m' fi fi if [[ -z ${MINECRAFT_BUCKET} ]]; then printf \"Please set the env variable ${RED}MINECRAFT_BUCKET${NC}to the s3 archive bucket name.\\n\" exit 0 fi if [[ -z ${MINECRAFT_ARCHIVE_LIMIT} ]]; then printf \"Please set the env variable ${RED}MINECRAFT_ARCHIVE_LIMIT${NC}to limit the number of archives to keep.\\n\" exit 0 fi backup_bucket=${MINECRAFT_BUCKET} backup_limit=${MINECRAFT_ARCHIVE_LIMIT} world=$1 printf \"Creating archive of ${RED}${world}${NC}\\n\" archive_name=\"${world}-$(date +\"%H-%M-%S-%m-%d-%Y\").zip\" zip -r $archive_name $world printf \"Checking if bucket has more than ${RED}${backup_limit}${NC}files already.\\n\" content=( $(aws s3 ls s3://$backup_bucket | awk '{print $4}') ) if [[ ${#content[@]} -eq $backup_limit || ${#content[@]} -gt $backup_limit ]]; then echo \"There are too many archives. Deleting oldest one.\" # We can assume here that the list is in cronological order printf \"${RED}s3://${backup_bucket}/${content[0]}\\n\" aws s3 rm s3://$backup_bucket/${content[0]} fi printf \"Uploading ${RED}${archive_name}${NC}to s3 archive bucket.\\n\" state=$(aws s3 cp $archive_name s3://$backup_bucket) if [[ \"$state\" =~ \"upload:\" ]]; then printf \"File upload ${LIGHT_GREEN}successful${NC}.\\n\" else printf \"${RED}Error${NC}occured while uploading archive. Please investigate.\\n\" fi It uses environment properties to define where to upload the given world and how many versions to keep.\nI’m calling this from a cron job, and it’s sitting next to where the Minecraft world is.\nThat’s it folks.\nI’ll start expanding on this idea and implement various services, like your own server address, or dropbox, or what have you.\nHappy backing up.\nGergely.\n","title":"Minecraft world automatic backup to AWS S3 bucket","uri":"/2016/04/16/minecraft-server-aws-s3-backup/"},{"content":"Hi Folks.\nIntro Today, I wanted to write about how to create a secure server in a container. Ideally, you don’t want to run your server on your own machine. Running it in a container gives you a much needed control and an extra layer of security.\nDocker On OSX While on a mac, you have a couple of options to run docker.\nDocker-Machine Docker-Machine\nDocker machine is very simple. It just creates a Linux vm in the background on the given driver, for us it will be VirtualBox. Network, Memory and port-forwarding can all be managed through the VM directly. Then running and starting it is trivial through docker-machine start.\nBoot2Docker Boot2Docker\nRuns a tiny linux in which you can use Docker freely. This adds the benefit of not having to mess around with VirtualBox.\nDLite DLite\nDLite is the newest addition in the Game. Since docker uses /var/run/docker.sock file to communicate with the daemon, and this file is not there on OSX, DLite takes care of that. After DLite is running, you just simply use Docker. That’s it. No VM, no fuss, just use Docker.\nI recommend to use DLite, however, it’s not an official tool, so for the sake of this guide, I’ll be writing up a docker-machine oriented solution.\nDocker container - A Vanilla Server First, you’ll need a Dockerfile. Dockerfile. The steps on how to setup this file are written down in the README file, located here: Container Setup. The pwd command will use your current directory as a shared volume to copy stuff into the container from your host operating system.\nThis will download the Minecraft server version 1.9 (or whichever you define) and install java and vim. It uses /data as a shared folder. Data will also be the working directory which means we will run the server from there. This server will be a vanilla server, meaning, no modding. If you have a single player world which you would like to use, simply copy that to /data and rename it to world.\nThe tricky part is to make this available on the internet. Because the container is in a VM we need to do a chain forwarding. Forward from the container to the vm, and from the vm to your host machine. If you are using a dns service like dyna or no-ip, you probably have a software which refreshes an IP for you to link to. And you’ll also have port forwarding setup on your router. I won’t be going into detail on that part. That’s a whole different post.\nForwarding from virtualbox can be done through the UI or through command line. The UI is like this: Settings =\u003e Network =\u003e Adapter 1 =\u003e Advanced =\u003e PortForwarding. Here, setup something like this: Name: Minecraft; Protocol: TCP; Host IP: 192.168.0.X(x=your local machine); Host Port: 25565; Guest IP: Leave Blank; Guest Port: 25565.\nWhere the number is missing, you’ll have to define your own local IP which you are using for your domain address.\nThis should forward any ports coming from your VM to your local IP. In the container we have an expose and as the README states it’s started using -p 25565:25565 which will make sure that from the container, 25565 is exposed to the VM. And from the VM 25565 is exposed to your local. Chain forwarding.\nAfter that, once you start your server, it should all click together and you should have a running vanilla Minecraft server available under http://yourdomain:25565. You won’t be able to check this by default on your own network. Check if the port is open with a different service like: Check If Port is Open.\nDocker container - Modding If you are not looking for anything, just a simple server, the above will be enough. You can still do /tp 1 1 1 to teleport, or can still use bans and op commands, and distribute items. However, if you would like to use mods, and as far as kids are concerned, they will want it, you’ll have to be a bit more clever.\nI dug far and deep and found that you have two options. Either go with a Forge server, or a Bukkit server. What does that mean? The vanilla server of Minecraft does not support modding. Modding, is modifying the implementation of Minecraft. It injects code and runs a pre-server in front of the original Minecraft server in order to append functionality. But fret not, this is all taken care of for you by either solutions.\nIn order to jump into our container with the CMD omitted, we’ll have to run the following command instead of the one in the README.\ndocker run -it -v `pwd`:/data -p 25565:25565 --name mc_server minecraft:v1.9 bash This will give you an interactive prompt in which now we can operate.\nForge Download the latest forge version from here =\u003e Minecraft Forge. They are usually up-to-date. I’m using 1.9 so I downloaded the appropriate installer version. After I obtained it, it was a matter of running this piece of command line code from my container:\njava -jar forge-1.9-12.16.0.1813-1.9-installer.jar --extract --installServer This will unpack a bunch of things you don’t have to worry about in your current directory, which is /data. Now run the universal.\njava -jar forge-1.9-12.16.0.1813-1.9-universal.jar Everything under the mods folder will be loaded as a mod. Forge is very restrictive and can only use Forge based mods. It will usually warn you if you have a none forge mod in your mods folder. You can find these on Forge’s forum here: Forge Forum. Simply download a jar and put it into /data/mods.\nBukkit I found Bukkit to be the winner for me. Most of the mods the kid wanted worked with Bukkit and did not work with Forge. Others will swear on Forge, but it’s really up to you. Using Bukkit is similarly easy. Again, you’ll have to find and get the wrapper for your Minecraft version, which can be located here: GetSpigot. You can use Spigot as well, though I have no experience with that.\nOnce, you got the wrapper, which is called craftbukkit-1.9.jar for me, you run it the same way you would run Forge or Minecraft.\njava -jar craftbukkit-1.9.jar This will load mods from the plugins folder. Plugins can be found here: Latest Bukkit Plugins.\nIn the container, you can use wget or curl to get the mods, or simply use the shared volume which is setup for you.\nLast Words All in all this sounds complicated, but it’s actually not, once you’ll get the hang out of it. You never kill the container once it’s setup, you just do docker stop mc_server and then docker-machine stop if you want to stop the VM as well. To start it up simply do these steps:\ndocker_machine start eval $(docker-machine env) docker start mc_server We named the container mc_server on a previous step. If you named it something different, use that name.\nThat’s it. Hope this was clear. Any feedback is appreciated. If you think you have an easier way, or if I wrote something incorrectly, feel free to tell me in the comment sections below.\nThanks for reading.\nGergely.\n","title":"Minecraft Server with Docker on OSX + Mods","uri":"/2016/03/29/minecraft-server-with-docker-and-osx/"},{"content":"Hi Folks.\nSo Wercker was not working. After a minor modification it seems to be okay now. The config file needed for it to work looks like this:\nbox: golang build: steps: - arjen/hugo-build: theme: redlounge deploy: steps: - install-packages: packages: git - leipert/git-push: gh_oauth: $GIT_TOKEN repo: skarlso/skarlso.github.io branch: master basedir: public The modification is the box type to golang and removed ssh-client from packages.\nThanks, Gergely.\n","title":"Wercker Fixed","uri":"/2016/03/09/wercker-fixed/"},{"content":"Basics This is a wercker Test.\n","title":"Wercker Test","uri":"/2016/03/04/wercker-test-2/"},{"content":"There already is a nice tutorial on how to create github-pages with Hugo Here if you prefer deplying your pages to a different branch on the same repo. There is also a post about Wercker and Hugo Here deploying pages to said separate branch.\nHowever, I took an easier approach on the matter with a completely separate branch for my blog source and my compiled github pages.\nThis blog sits here: https://github.com/Skarlso/skarlso.github.io. In order to deploy to it, I just have to commit a new blog post to this repository: Blog Source. After that, Wercker takes care of the rest. It builds my blog, and pushes the generated pages to my blog’s repository to the master branch without creating the gh-pages branch.\nThe Wercker yml for that looks like this:\nbox: debian build: steps: - arjen/hugo-build: theme: redlounge deploy: steps: - install-packages: packages: git ssh-client - leipert/git-push: gh_oauth: $GIT_TOKEN repo: skarlso/skarlso.github.io branch: master basedir: public Pretty easy. The $GIT_TOKEN is a variable set-up on Wercker containing a restricted token which is only good for pushing. And note that you have to use an explicit package name with git-push or else Wercker will not find that step. Hugo-build will build my blog with a simple command using redlounge theme.\nAnd that’s it. No other setup is necessary and no new branch will be made. Any questions, please feel free to leave a comment.\nThanks for reading!\nGergely.\n","title":"Hugo Autodeploy with Wercker and Github - Pages","uri":"/2016/02/10/hugo-autodeploy-with-wercker/"},{"content":"Basics This is a wercker Test.\n","title":"Wercker Test","uri":"/2016/02/10/wercker-test/"},{"content":"Basics Hello folks.\nThis will be a quick post about how to do CORS with jQuery, Gin in Go with a very simple ajax GET and Json.\nI’m choosing JSON here because basically I don’t really like JSONP. And actually, it’s not very complicated to do CORS, it’s just hidden enough so that it doesn’t become transparent.\nFirst, what is CORS? It’s Cross-Platform Resource Sharing. It has been invented so that without your explicit authorization in the header of a request, Javascript can’t reach outside of your domain and be potentially harmful to your visitors.\nNow, suppose you have an architecture like this.\nYou have multiple agents sitting on multiple nodes. You have one central server, and you have multiple front-ends. Everybody can only talk to the Server but the server does talk to everyone. You would like to have a dynamic front-end and would like to display data with ajax calls. Since your front-end sits on a different server, you will have to do something about CORS. This is how I solved it…\nI’m using Gin for my REST service for Dockmaster. For this two work, you need to adjust two component.\nServer There is thing called a Preflight-Check. In essence, the preflight check is sent BEFORE the actual request to check if the next request is allowed to go out of the domain. The preflight check is sent to the same URI just with OPTIONS method. In order to tell the caller that the next one will be safe, you need three things.\nFirst, you need to set two Headers. #1 -\u003e Access-Control-Allow-Origin to “*”. #2 -\u003e Access-Control-Allow-Headers to “access-control-allow-origin, access-control-allow-headers”.\nThese are the minimum headers you can set. If you allow Access-Control-Allow-Origin you also have to allow it in the headers section because the next request will expect it to be there. Also, note here that setting Origin to * is only recommended in development environment. Otherwise it should be set to whatever your domain is.\nSecond, you need to respond to the OPTIONS method with a 200. In order to do that, I added a simple rule with the same end-point but with OPTIONS.\nfunc main() { router := gin.Default() v1 := router.Group(APIBASE) { v1.GET(\"/list\", listContainers) v1.POST(\"/add\", addContainers) v1.POST(\"/delete\", deleteContainers) v1.GET(\"/inspect/:agentID/:containerID\", inspectContainer) v1.OPTIONS(\"/inspect/:agentID/:containerID\", preflight) v1.POST(\"/stopAll\", stopAll) v1.OPTIONS(\"/stopAll\", preflight) } router.Run(\":8989\") } func preflight(c *gin.Context) { c.Header(\"Access-Control-Allow-Origin\", \"*\") c.Header(\"Access-Control-Allow-Headers\", \"access-control-allow-origin, access-control-allow-headers\") c.JSON(http.StatusOK, struct{}{}) } You can see that the preflight method is there for two end-points. I added it to those end-points which will reach over the domain. The others are all local, thus they don’t need that. This leads to a little duplication, but that is fine. I have a very fine control over what actually is allowed to go outside of the domain.\nSo, how do we call this?\nFrontend In the front-end’s web layout, I’m doing an Ajax GET, which looks like this:\n$.ajax({ url: 'http://localhost:8989/api/1/inspect/'+data.agentid+'/'+data.id, type: 'GET', dataType:\"json\", headers: {\"Access-Control-Allow-Origin\": \"*\", \"Access-Control-Allow-Headers\": \"access-control-allow-origin, access-control-allow-headers\"}, processData: false, success: function(data) { var json = JSON.stringify(data, null, 4) independentPopup.html(\"\u003cpre \u003e\"+json+\"\u003c/pre\u003e\"); $(link).after(independentPopup); } }); After the headers are set, the request will work nicely.\nY U No Middleware? And now you could say that, why not just have a middleware which will always accept OPTIONS for every end-point. Because I like it better this way. Some would argue that this is too granular, but fact is, that in my opinion, this is more readable and immediatly visible. However, if you DO want to do that, you have several options to your disposal.\nCors Basic Http Middleware and for Gin Gin CORS Middleware.\nSummary This is it. You can see the code in its entirety on Github. Have a better idea on how to do it? Please! Do not hesitate to share. I always like to learn.\nThank you for reading!\nAnd as always, Have a nice day!\nGergely.\n","title":"Doing CORS in Go with Gin and JSON","uri":"/2016/02/02/doing-cors-in-go-with-gin-and-json/"},{"content":"Hello folks.\nI wanted to share with you my tale of working through the problems with Advent Of Code.\nIt is a nice tale and there are a few things I learned from it, especially in Go, since I used that solve all of the problems. So, let’s get started.\nSolving the problems The most important lesson I learned while doing these exercises was, how to solve these problems. A couple of them were simple enough to not have to over think it, but most of them got very tricky. I could have gone with a brute force attempt, but as we see later, that wasn’t always a very good solution. And people who used that, actually just got lucky finding their solutions.\nThe eight stages of breaking down a problem according to this book Thinking Like a Programmer are the following:\n Have a plan Rephrase Divide Start with what you know Reduce Analogies Experiment Don’t get frustrated  Have a plan and understanding your goal This is simple. Always have a plan of what you would like to do, and how to start. This will help you massively along the way to not to loose sight of what your goal is actually. For example, look at Day 24. At first, it looks like a permutational puzzle, but if you understand the solution we are looking for, you realize that there is an easier way of finding it. Since you only want the packages which consists of the fewest item counts, you would only care about the largest numbers because those will be the fewest which still give you the desired package weight. Suddenly the problem gets easier because you don’t have to worry about the other groups any longer.\nRephrase Rephrasing the problem with your own words can help in understanding it better. Or even better, try explaining it to somebody else. If you cannot rephrase it, you didn’t understand it in the first place.\nDivide If the problem seems daunting because it’s massive, just divide it into smaller chunks. This is something that we usually do with large problems, but it’s more subtle than that. If you face a problem which seems complex, just factor out parts of it until you got a problem which you do understand. Even if you have to butcher the original puzzle problem. It doesn’t matter. Adding complexity later is easier than adding complexity in its infancy.\nStart with what you know \u0026\u0026 Finding analogies This one speaks for itself. If you know parts of the problem, because you know analogy for it, or you faced something similar before, or exactly that, start with that.\nReduce If the problem seems too complex, remove complexity. Start with a smaller set. Preferably something testable (I’ll come back to that later). Remove constraints, or add them as desired. A constraint makes it harder to solve the puzzle? Remove it, and try solving it without. After that, the solution will give you insight into the problem and you can add that constraint back in.\nConsider Day 11. I had fun with this one. In order to easy it up a little, I first, removed the constraint of doing the increment with letters. I did it with numbers. I also removed the constraint of doing it within the confines of a limited length array. After I got that I’ll use modulo to make the numbers wrap around, it was way more easy to apply it to characters. And after a little fidgeting this came to life:\npasswd[i] -= 'a' passwd[i] = (passwd[i] + 1) % (('z' - 'a') + 1) passwd[i] += 'a' The -,+ ‘a’ is needed so that it’s dealing with ascii code from 0 - ‘z’. This basically makes it so that when I reach the end of the alphabet it will wrap around and start from ‘a’ again.\nExperiment This led to more solutions than I care to admit. Basically just start experimenting with solutions which are in your head. There is a chance, that what you come up with, will be the solution. This goes very well with the principle of Make it work, Make it right, Make it fast. Just have something working first, and than you can make it work properly after. It’s always better to have something rather than nothing.\nAnd last but not least…\nDon’t get frustrated This is something I cannot say strongly enough. Seriously. DO NOT GET FRUSTRATED. Most of the problems were designed to be harder. Unless you work as a programmer professionally for several years now, or this is a field of interest for you, you will spend a day hacking around on a problem and trying to find a solution which is adequate. In these times, you will get frustrated and think you are too stupid for this, this couldn’t be more far from the truth! You might need some inspiration, you might need some time away from the screen, it helps if you draw out the problem in a piece of paper, or just think about it without seeing it for a while. Just take a break, eat something, watch a comedy and get back to it later with a fresh view.\nTechnical Gotchas So after the general problem solving side of things, I learned many things about Go, and about the tidbits of this language.\nByte Slices I already knew that []byte is more performant and that Go optimizes on them more, but not to this extent. As in my previous blog posts I discovered that using them can really make a huge difference. Go even has a library called bytes which has helper functions similar to that of strings to help you out in these situations. Go optimizes on map recalls as well when you cast to string from []byte and use that as a map key like this: myMap[string(data)].\nBrute Force or Looping Most of the times you could get away with looping or trying to brute force out a solution. But there were times, where you really had to huddle down and think the problem through. Because simply looping, either took too long, or didn’t come up with a good answer. That’s why I rather always start with: ‘How could I solve this without looping?’. This will get you into the right mindset. Or thinking: ‘How could I solve this without taking each and every combination into account?’. These questions will help you to think about the problem without loops. Or only if you REALLY must use one.\nDoing this will get you into the right way of thinking. I know that in advent of code there is a Leaderboard and you could get on it if you were fast. But most of the times having a fast solution is far from having the right solution.\nStructs are Awesome I like using structs. They are a very lightweight way of defining objects, structures which stick together. For example in the Day 6 Light puzzle, or even Day 3 Traveling santa example, a struct which stuck x,y locations together and made it a map key, it was trivial to make my gif out of it with SVG -\u003e\nGo is Simple to Read [opinion] I like Go because of its simplicity. You don’t see stuff in Go most of the times, where you need to look three times to understand what the heck is going on. I like filter, reduce, map and syntactic sugar, but they make for a very poor reading experience. Go, in that way, choose not to incorporate these paradigms and I find that refreshing. [/opinion]\nTesting TDD is something we all should know by now and care about. When I’m doing puzzles, or finger exercises, I tend to not write tests. But on a more complex puzzle, or a task, I always start with a test. Especially if you are given samples for a puzzle which work. That’s a gold mine. You can tweak your algorithm using those samples until they work and then simply apply a larger sample size.\nTests will also help you with breaking down a problem and identifying parts which you already know.\nFor example Day 13. Optimal Seating arrangements. Or the similar Day 9. Which was calculating shortest route distance. Or the password one, Day 11 which I showed before. In these cases, tests helped me make the core of the algorithm solid. Calculating connections, or the odd regex here and there, which was making sure that the password was validated properly.\nTests will also help you to be able to move on after you found your solution. When I was done with the first iteration of passwords which was still using strings, I went on to optimize it, to use []byte. The tests helped me to know that the code was still working as expected after the refactoring.\nClosing words All in all it was a massive amount of fun doing these exercises and I’m thankful to the creator for making it. And I did enjoy the story behind the exercises as well. I think this site stood out because it had a fun factor. For simple exercises there are a lot of other sites -like Project Euler, or Sphere Judge Online-, which just plainly present you a problem and that’s it. It’s still fun, but it can also became boring very fast. Don’t forget the fun factor which makes you plow on and go into a blind frenzy that you cannot quit until it’s done. That’s the fun part.\nThank you for reading! Have a nice day. Gergely.\n","title":"My Journey in advent of code","uri":"/2016/01/22/my-journey-in-advent-of-code/"},{"content":"Hello Folks.\nToday I would like to share with you my little tale of refactoring my solution to Advent Of Code Day 13.\nIt’s a lovely tale of action, adventure, drama, and comedy.\nLet’s being with my first iteration of the problem.\npackage main import ( \"bufio\" \"fmt\" \"math\" \"os\" \"strconv\" \"strings\" \"github.com/skarlso/goutils/arrayutils\" ) var seatingCombinations = make([][]string, 0) var table = make(map[string][]map[string]int) var keys = make([]string, 0) //Person a person type Person struct { // neighbour *Person \tname string like int } func main() { file, _ := os.Open(\"input.txt\") defer file.Close() scanner := bufio.NewScanner(file) for scanner.Scan() { line := scanner.Text() split := strings.Split(line, \" \") like, _ := strconv.Atoi(split[3]) //If lose -\u003e * -1 \tif split[2] == \"lose\" { like *= -1 } table[split[0]] = append(table[split[0]], map[string]int{strings.Trim(split[10], \".\"): like}) if !arrayutils.ContainsString(keys, split[0]) { keys = append(keys, split[0]) } } generatePermutation(keys, len(keys)) fmt.Println(\"Best seating efficiency:\", calculateSeatingEfficiancy()) } func generatePermutation(s []string, n int) { if n == 1 { news := make([]string, len(s)) copy(news, s) seatingCombinations = append(seatingCombinations, news) } for i := 0; i \u003c n; i++ { s[i], s[n-1] = s[n-1], s[i] generatePermutation(s, n-1) s[i], s[n-1] = s[n-1], s[i] } } func calculateSeatingEfficiancy() int { bestSeating := math.MinInt64 for _, v := range seatingCombinations { calculatedOrder := 0 for i := range v { left := (i - 1) % len(v) //This is to work around the fact that in Go \t//modulo of a negative number will not return a positive number. \t//So -1 % 4 will not return 3 but -1. In that case we add length. \tif left \u003c 0 { left += len(v) } right := (i + 1) % len(v) // fmt.Printf(\"Left: %d; Right: %d\\n\", left, right) \tleftLike := getLikeForTargetConnect(v[i], v[left]) rightLike := getLikeForTargetConnect(v[i], v[right]) // fmt.Printf(\"Name: %s; Left:%d; Right:%d\\n\", v[i], leftLike, rightLike) \tcalculatedOrder += leftLike + rightLike } // fmt.Printf(\"Order for: %v; Calc:%d\\n\", v, calculatedOrder) \tif calculatedOrder \u003e bestSeating { bestSeating = calculatedOrder } } return bestSeating } func getLikeForTargetConnect(name string, neighbour string) int { neighbours := table[name] for _, t := range neighbours { if v, ok := t[neighbour]; ok { return v } } return 0 } This is quiet large. And takes a bit of explaining. So what is happening here? We are putting the names which correspond with numbers and neighbours into a map which has a map as a value. The map contains seating information for a person. For example, next to Alice, a bunch of people can sit, and they have a certain relationship to Alice, represented by a number.\nWe could, at this point, represent it with a graph, but that would be overkill.\nPermutation is simple because I choose to represent a Table with a Circular Slice. This means that a slice like this =\u003e Alice, Bob, Tom; means that Alice is sitting next to Bob and Tom. So Alice’s neighbour of -1 (left) is in fact i-1 % 3. And Bob is i + 1. For Tom, Alice is i + 1 % 3. After we got this, we just permutate the possible combinations into slices of slices and iterate over them.\nThe benchmark for this is terrible.\n================With Strings================ 20\t589571259 ns/op ok github.com/skarlso/goprojects/advent/day13/part1\t11.873s So, my first thought was, convert everything I can to []byte. But because slices cannot be map keys, because map keys need to be comparable, we are still stuck with the same ns/ops.\n//var seatingCombinations = make([][]string, 0) //var keys = make([]string, 0) var seatingCombinations = make([][][]byte, 0) var keys = make([][]byte, 0) And I adjusted the code to work with []byte instead. What can we do to fix the map though? One obvious gain is, not to use string as a key. Because strings are immutable, working with them always means copy-ing and that’s why they get to be very slow. So removing them from Keys and using Numbers instead will mean a huge gain for us.\nTo do this, I created a map which maps names with numbers. I could hardcode them with iota, but that is a very bad thing to do. It would mean, that when I add a new name, I would have to go, and re-compile my code, because data changed. That’s not what we want.\nSo, I added this little tid-bit into the for cycle when I’m reading in the file lines =\u003e\n... if _, ok := nameMapping[split[0]]; !ok { nameMapping[split[0]] = id id++ } if _, ok := nameMapping[trimmedNeighbour]; !ok { nameMapping[trimmedNeighbour] = id id++ } ... Id starts as Zero. And nameMapping is a simple map[string]int. After this, we fix all the map calls, from table[split[0]] to table[nameMapping[split[0]]]. Table’s map will now work with int, but we can still work with strings otherwise.\ntable[nameMapping[split[0]]] = append(table[nameMapping[split[0]]], map[int]int{nameMapping[trimmedNeighbour]: like}) This has now a marginally better performance as before:\nBenchmarkCalculateSeating\t50\t32637879 ns/op ok github.com/skarlso/goprojects/advent/day13/part1\t1.698s But, we can still do a HUGE one better. Can you notice the other bottleneck? See, how keys are still []byte? That’s, now completely unnecessary. We can use int, since our keys are ints! Permutation changes, and the retrieve.\n... func generatePermutation(s []int, n int) { ... ... func getLikeForTargetConnect(name int, neighbour int) int { neighbours := table[name] for _, t := range neighbours { if v, ok := t[neighbour]; ok { return v } } return 0 } ... Permutation was the Other huge performance consumption. Now, our run time is…. drum rolls…\nBenchmarkCalculateSeating\t10000\t166431 ns/op ok github.com/skarlso/goprojects/advent/day13/part1\t1.695s Down to 166431 ns/op!!! From 32637879 ns/op!! And notice how suddenly, go’s benchmark jumped up in sample count. Our code is now blazing fast. It’s 0.05% of the previous run! It’s almost 200 times faster!\nWe could still improve it here and there. I’m sure I’m doing some extra stuff which is not needed or could be made easier somehow. But I’m actually quiet happy with this solution right now.\nThe full code:\npackage main import ( \"bufio\" \"math\" \"os\" \"strconv\" \"strings\" \"github.com/skarlso/goutils/arrayutils\" ) var seatingCombinations = make([][]int, 0) var table = make(map[int][]map[int]int) var keys = make([]int, 0) var nameMapping = make(map[string]int) //Person a person type Person struct { // neighbour *Person \tname string like int } func main() { CalculatePerfectSeating() } //CalculatePerfectSeating returns the perfect seating order based on Love/Hate relations func CalculatePerfectSeating() { file, _ := os.Open(\"input.txt\") defer file.Close() scanner := bufio.NewScanner(file) id := 0 for scanner.Scan() { line := scanner.Text() split := strings.Split(line, \" \") trimmedNeighbour := strings.Trim(split[10], \".\") like, _ := strconv.Atoi(split[3]) //If lose -\u003e * -1 \tif _, ok := nameMapping[split[0]]; !ok { nameMapping[split[0]] = id id++ } if _, ok := nameMapping[trimmedNeighbour]; !ok { nameMapping[trimmedNeighbour] = id id++ } if split[2] == \"lose\" { like *= -1 } table[nameMapping[split[0]]] = append(table[nameMapping[split[0]]], map[int]int{nameMapping[trimmedNeighbour]: like}) if !arrayutils.ContainsInt(keys, nameMapping[split[0]]) { keys = append(keys, nameMapping[split[0]]) } } generatePermutation(keys, len(keys)) // fmt.Println(\"Best seating efficiency:\", calculateSeatingEfficiancy()) } func generatePermutation(s []int, n int) { if n == 1 { news := make([]int, len(s)) copy(news, s) seatingCombinations = append(seatingCombinations, news) } for i := 0; i \u003c n; i++ { s[i], s[n-1] = s[n-1], s[i] generatePermutation(s, n-1) s[i], s[n-1] = s[n-1], s[i] } } func calculateSeatingEfficiancy() int { bestSeating := math.MinInt64 for _, v := range seatingCombinations { calculatedOrder := 0 for i := range v { left := (i - 1) % len(v) //This is to work around the fact that in Go \t//modulo of a negative number will not return a positive number. \t//So -1 % 4 will not return 3 but -1. In that case we add length. \tif left \u003c 0 { left += len(v) } right := (i + 1) % len(v) calculatedOrder += getLikeForTargetConnect(v[i], v[left]) + getLikeForTargetConnect(v[i], v[right]) } if calculatedOrder \u003e bestSeating { bestSeating = calculatedOrder } } return bestSeating } func getLikeForTargetConnect(name int, neighbour int) int { neighbours := table[name] for _, t := range neighbours { if v, ok := t[neighbour]; ok { return v } } return 0 } Also, on github =\u003e Advent Of Code Day 13.\nThank you very much for reading, this has been a massive fun to write and to refactor.\nHave something to say? Please don’t hesitate.\nAnd as always,\nHave a nice day!\nGergely.\n","title":"Improving performance with byte slice and int map","uri":"/2016/01/05/improving-performance-with-byte-slice-and-int-map/"},{"content":"Hi folks and a Happy new Year!\nToday, I would like to show you some interesting things you can do with channels. Consider the following simple example.\npackage main import \"fmt\" func main() { generatedPassword := make(chan int, 100) correctPassword := make(chan int) defer close(generatedPassword) defer close(correctPassword) go passwordIncrement(generatedPassword) go checkPassword(generatedPassword, correctPassword) pass := \u003c-correctPassword fmt.Println(pass) } func checkPassword(input \u003c-chan int, output chan\u003c- int) { for { p := \u003c-input //Introduce lengthy operation here \t// time.Sleep(time.Second) \tfmt.Println(\"Checking p:\", p) if p \u003e 100000 { output \u003c- p } } } func passwordIncrement(out chan\u003c- int) { p := 0 for { p++ out \u003c- p } } The premise is as follows. It launches two go routines. One, which generates passwords, and an other which checks for validity. The two routines talk to each other through the channel generatedPassword. That’s the providing connections between them. The channel correctPassword provides output for the checkPassword routine.\nIf there is data received from correctPassword channel, we found our first password and there is no need to look further so we, print the password and quit. The channels will close with defer. This works. But the password is usually either a []byte or a string. With string, it still works.\npackage main import \"fmt\" func main() { generatedPassword := make(chan string, 100) correctPassword := make(chan string) defer close(generatedPassword) defer close(correctPassword) go passwordIncrement(generatedPassword) go checkPassword(generatedPassword, correctPassword) pass := \u003c-correctPassword fmt.Println(pass) } func checkPassword(input \u003c-chan string, output chan\u003c- string) { for { p := \u003c-input //Introduce lengthy operation here \t// time.Sleep(time.Second) \tfmt.Println(\"Checking p:\", p) if performSomeCheckingOperation(p) { output \u003c- p } } } func generateNewPassword(out chan\u003c- string) { var p string for { p = generate(p) out \u003c- p } } The generating happens based on the previously generated password. For example, we increment, or permeate. aaaa, aaab, aaac…\nSo generatedPassword is a buffered channel, it gathers a 100 passwords from which checking retrieves passwords one by one and works on them in a slower process.\nNow, this is fine, but using []byte arrays will always be more powerful and faster. So we would like to use []byte. Like this:\npackage main import \"fmt\" func main() { generatedPassword := make(chan []byte, 100) correctPassword := make(chan []byte) defer close(generatedPassword) defer close(correctPassword) go passwordIncrement(generatedPassword) go checkPassword(generatedPassword, correctPassword) pass := \u003c-correctPassword fmt.Println(string(pass)) } func checkPassword(input \u003c-chan []byte, output chan\u003c- []byte) { for { p := \u003c-input //Introduce lengthy operation here \t// time.Sleep(time.Second) \tfmt.Println(\"Checking p:\", string(p)) if performSomeCheckingOperation(p) { output \u003c- p } } } func generateNewPassword(out chan\u003c- []byte) { var p []byte for { p = generate(p) out \u003c- p } } This will not work. Why? Because []byte is a slice and thus will be constantly overwritten. The checking go routine will always only check the last data and many generated passwords will be lost. This is also noted in go’s scanner here =\u003e Scanner.Bytes\nWe have a couple of options here.\nWe could use string channels and convert to []byte after. This is still okay, because the conversion isn’t very CPU intensive.\n... generatedPassword := make(chan string, 100) correctPassword := make(chan string) ... p := []byte(\u003c-input) //This will work very nicely. ... Options two would be If you have a fixed password to handle, fix data, for example MD5 hash, you can use a byte array. Like this:\npackage main import \"fmt\" const PASSWD=13 func main() { generatedPassword := make(chan [PASSWD]byte, 100) correctPassword := make(chan [PASSWD]byte) defer close(generatedPassword) defer close(correctPassword) go passwordIncrement(generatedPassword) go checkPassword(generatedPassword, correctPassword) pass := \u003c-correctPassword fmt.Println(string(pass)) } func checkPassword(input \u003c-chan [PASSWD]byte, output chan\u003c- [PASSWD]byte) { for { p := \u003c-input //Introduce lengthy operation here \t// time.Sleep(time.Second) \tfmt.Println(\"Checking p:\", string(p)) if performSomeCheckingOperation(p) { output \u003c- p } } } func generateNewPassword(out chan\u003c- [PASSWD]byte) { var p [PASSWD]byte for { p = generate(p) out \u003c- p } } This is also one solution. If you have to convert between the two, could go with p := byte[:].\nConclusion is, that use conversion rather than string types and be aware that using slices in channels is dangerous.\nThanks for reading! Gergely.\n","title":"Byte arrays and Channels","uri":"/2016/01/01/byte-arrays-and-channels/"},{"content":"Hello Folks.\nThis is just a quick post on the topic and a reminder for myself and everybody to ALWAYS USE []BYTE INSTEAD OF STRINGS.\n[]Byte is marginally faster than a simple Strings. In fact, I would say using []byte should be the standard instead of strings.\nSample code:\npackage solutions import \"fmt\" const ( //INPUT input  INPUT = \"1321131112\" //LIMIT limit  LIMIT = 50 ) //LookAndSay translates numbers according to Look and Say algo func LookAndSay(s string, c chan string) { charCount := 1 look := \"\" for i := range s { if i+1 \u003c len(s) { if s[i] == s[i+1] { charCount++ } else { look += fmt.Sprintf(\"%d%s\", charCount, string(s[i])) charCount = 1 } } else { look += fmt.Sprintf(\"%d%s\", charCount, string(s[i])) } } c \u003c- look } //GetLengthOfLookAndSay Retrieve the Length of a lookandsay done Limit times func GetLengthOfLookAndSay() { c := make(chan string, 0) go LookAndSay(INPUT, c) finalString := \u003c-c for i := 0; i \u003c= LIMIT-2; i++ { go LookAndSay(finalString, c) finalString = \u003c-c // fmt.Println(finalString)  } fmt.Println(\"Lenght of final String:\", len(finalString)) } This, with the limit raised to 50 run for ~1 hour. Even with the routines although they were just for show since they had to wait for each others input.\nNow change this to []byte and the run time was almost under 2 seconds on my machine.\npackage solutions import ( \"fmt\" \"strconv\" ) const ( //LIMIT limit  LIMIT = 50 ) //INPUT puzzle input //This used to be a string until I was reminded that BYTE ARRAY IS ALWAYS FASTER! var INPUT = []byte(\"1321131112\") //LookAndSay translates numbers according to Look and Say algo func LookAndSay(s []byte) (look []byte) { charCount := 1 for i := range s { if i+1 \u003c len(s) { if s[i] == s[i+1] { charCount++ } else { b := []byte(strconv.FormatInt(int64(charCount), 10)) look = append(look, b[0], s[i]) charCount = 1 } } else { b := []byte(strconv.FormatInt(int64(charCount), 10)) look = append(look, b[0], s[i]) } } return } //GetLengthOfLookAndSay Retrieve the Length of a lookandsay done Limit times func GetLengthOfLookAndSay() { finalString := INPUT for i := 0; i \u003c= LIMIT-1; i++ { finalString = LookAndSay(finalString) } fmt.Println(\"Lenght of final String:\", len(finalString)) } This is the solution for Day 10 on AdventOfCode by the way.\nThanks for readin'. Gergely.\n","title":"Use Byte Array Instead of Strings","uri":"/2015/12/29/use-byte-array-instead-of-strings/"},{"content":"Hello Folks.\nThis is just a quick post on the topic and a reminder for myself and everybody to ALWAYS USE []BYTE INSTEAD OF STRINGS.\n[]Byte is marginally faster than a simple Strings. In fact, I would say using []byte should be the standard instead of strings.\nSample code:\npackage solutions import \"fmt\" const ( //INPUT input  INPUT = \"1321131112\" //LIMIT limit  LIMIT = 50 ) //LookAndSay translates numbers according to Look and Say algo func LookAndSay(s string, c chan string) { charCount := 1 look := \"\" for i := range s { if i+1 \u003c len(s) { if s[i] == s[i+1] { charCount++ } else { look += fmt.Sprintf(\"%d%s\", charCount, string(s[i])) charCount = 1 } } else { look += fmt.Sprintf(\"%d%s\", charCount, string(s[i])) } } c \u003c- look } //GetLengthOfLookAndSay Retrieve the Length of a lookandsay done Limit times func GetLengthOfLookAndSay() { c := make(chan string, 0) go LookAndSay(INPUT, c) finalString := \u003c-c for i := 0; i \u003c= LIMIT-2; i++ { go LookAndSay(finalString, c) finalString = \u003c-c // fmt.Println(finalString)  } fmt.Println(\"Lenght of final String:\", len(finalString)) } This, with the limit raised to 50 run for ~1 hour. Even with the routines although they were just for show since they had to wait for each others input.\nNow change this to []byte and the run time was almost under 2 seconds on my machine.\npackage solutions import ( \"fmt\" \"strconv\" ) const ( //LIMIT limit  LIMIT = 50 ) //INPUT puzzle input //This used to be a string until I was reminded that BYTE ARRAY IS ALWAYS FASTER! var INPUT = []byte(\"1321131112\") //LookAndSay translates numbers according to Look and Say algo func LookAndSay(s []byte) (look []byte) { charCount := 1 for i := range s { if i+1 \u003c len(s) { if s[i] == s[i+1] { charCount++ } else { b := []byte(strconv.FormatInt(int64(charCount), 10)) look = append(look, b[0], s[i]) charCount = 1 } } else { b := []byte(strconv.FormatInt(int64(charCount), 10)) look = append(look, b[0], s[i]) } } return } //GetLengthOfLookAndSay Retrieve the Length of a lookandsay done Limit times func GetLengthOfLookAndSay() { finalString := INPUT for i := 0; i \u003c= LIMIT-1; i++ { finalString = LookAndSay(finalString) } fmt.Println(\"Lenght of final String:\", len(finalString)) } This is the solution for Day 10 on AdventOfCode by the way.\nThanks for readin'. Gergely.\n","title":"Use Byte Slice Instead of Strings","uri":"/2015/12/29/use-byte-slice-instead-of-strings/"},{"content":"Hello everybody!\nI wanted to do a sort post about word frequency count. I did it many times now and I was curious as how a recursive solution would perform as opposed to looping.\nSo I wrote it up quickly and added a few benchmarks with different sized data.\nFirst…. The code:\nvar freqMap = make(map[string]int, 0) func countLettersRecursive(s string) string { if len(s) == 0 { return s } freqMap[string(s[0])]++ return countLettersRecursive(s[1:]) } func countLettersLoop(s string) { for _, v := range s { freqMap[string(v)]++ } } Very simple. The first run with a small sample: “asdfasdfasdfasdfasdf”\nBenchmarkLoopFrequencyCount 5000000 377 ns/op BenchmarkRecursiveFrequencyCount 5000000 380 ns/op They almost equal but Recursive seems to be lagging behind. So I increased the sample size to a text which was 496 long.\nPASS BenchmarkLoopFrequencyCount 30000 53336 ns/op BenchmarkRecursiveFrequencyCount 20000 61780 ns/op And, as expected, recursing is less performant than looping. Also, I think my machine would die from a larger data size…\nBut the recursive looks so much cooler though.\nThanks for reading! Gergely.\n","title":"Recursive Letter Frequency Count","uri":"/2015/12/23/recursive-freq-count/"},{"content":"Hello folks.\nHere is a little something I’ve put together, since I’m doing it a lot.\nGo Development Environment\nIf I have a project I’d like to contribute, like GoHugo, I have to setup a development environment, because most of the times, I’m on a Mac. And on OSX things work differently. I like to work in a Linux environment since that’s what most of the projects are built on.\nSo here you go. Just download the files, and say vagrant up which will do the magic.\nThis sets up vim-go with code completion given by YouCompleteMe and some go features like, fmt on save and build error highlighting.\nAlso sets up ctags which will give you tags and the ability to do GoTo Declaration.\nInstalls a bunch of utilities, and configures Go. There is an option to install docker as well. But it’s ignored at the moment.\nJust uncomment this line:\n#config.vm.provision \"shell\", path: \"install_docker.sh\" Any questions or request, feel free to submit an Issue!\nThanks for reading! Gergely.\n","title":"Go Development Environment","uri":"/2015/12/08/go-development-environment/"},{"content":"Hello Folks Welcome to my new blog. I decided to move away for a number of reasons, but setting up a static page blog site is very cool if you don’t directly use a database. Since posts are just posts and I have a different way of hosting images, this really was just a matter of time.\nAnd Hugo / Github pages provided the tools which made this move possible.\nAlso, I love writing this post in Markdown. I always liked the formatting rules of it, so this is quiet the blast.\nCode will look a little more readble now as well:\nfunc main() { handlerChain := alice.New(Logging, PanicHandler) router := mux.NewRouter().StrictSlash(true) router.Handle(\"/create\", handlerChain.ThenFunc(createIssue)).Methods(\"POST\") router.Handle(\"/\", handlerChain.ThenFunc(renderMainPage)).Methods(\"GET\") router.PathPrefix(\"/css/\").Handler(http.StripPrefix(\"/css/\", http.FileServer(http.Dir(\"./css\")))) log.Printf(\"Starting server to listen on port: 8989...\") http.ListenAndServe(\":8989\", router) } Much easier on the eyes. And linking is a breeze as well.\nThings to notice There is now a content on the side which will list the sections in a post. And there is an estimated read timer in the post’s title. It takes average reading speed and wordcount into account.\nAnyhow, thanks for joining me in the new realm, and happy reading! Gergely.\n","title":"Welcome To My New Blog","uri":"/2015/12/07/welcome-to-my-new-blog/"},{"content":"Hi folks.\nSo, I was playing around and created a client for JIRA written in Go. It was nice to do some JSON transformation. And sending POSTS was really trivial.\nIt’s still in it’s infancy and I have a couple of more features I want to implement, but, here is the code…\nIt can also be found under my github page: GoJira Github.\nFeel free to open up issues if you would like to use it and need some features which you would find interesting. Currently the username and password for the API are stored in a local config file in your home folder. Later on, I’ll add the ability to have a token rather than a username:password combination.\nThanks for reading!\nGergely.\n","title":"Go JIRA API client","uri":"/2015/11/20/go-jira-api-client/"},{"content":"Hello folks.\nToday, I present to you the One Hundred Day Github Challenge.\nThe rules are simple:\n Minimum of One commit every day for a Hundred days. Commit has to be meaningful but can be as little as a fix in a Readme.md. Doesn’t matter if you are on vacation, there are no exceptions. There. Are. No. Exceptions. If you fail a day, you have to start over. No cheating. You only cheat yourself, so this is really up to you.  Let me be more clear here, because it seems I wasn’t clear enough. What you make out of this challenge, it’s up to you. If you just update a readme.md for hundred days, that’s fine. Just do it every day. It’s a commitment. At least you’ll have a nice Readme.\nAlso, let me be clear on another thing. THERE ARE NO EXCEPTIONS. Even on holidays. No. Exceptions.\nSo there you have it. It’s easy, but then again, it’s not.\nMine starts today! 100…\nThanks for reading.\nAnd happy coding.\nGergely.\n","title":"The One Hundred Day GitHub Challenge","uri":"/2015/11/15/the-one-hundred-day-github-challenge/"},{"content":"Hi Folks.\nI started to build a Progress Quest type of web app in Go.\nIf you’d like to join, or just tag along, please drop by here =\u003e Go Progress Questand feel free to submit an issue if you have an idea, or would like to contribute!\nI will try and document the Progress…\nThank you for reading!\nGergely.\n","title":"Go Progress Quest","uri":"/2015/11/09/go-progress-quest/"},{"content":"Hi folks.\nIf you have the tendency, like me, to forget that you are on the corporate VPN, or leave a certain software open when you bring your laptop to work, this might be helpful to you too.\nIt’s a small script which kills a program when you change your Wifi network.\nScript:\nNow, the trick is, on OSX to only trigger this when your network changes. For this, you can have a ‘launchd’ daemon, which is configured to watch three files which relate to a network being changed.\nThe script sits under your ~/Library/LaunchAgents folder. Create something like, com.username.checknetwork.plist.\nNow, when you change your network, to whatever your corporate network is, you’ll kill Sublime.\nHope this helps somebody.\nCheers,\nGergely.\n","title":"Kill a Program on Connecting to a specific WiFi – OSX","uri":"/2015/10/26/kill-a-program-on-connecting-to-a-specific-wifi-osx/"},{"content":"I’m proud of this one too. No peaking. I like how go let’s you do this kind of stuff in a very nice way.\npackage circular import \"fmt\" //TestVersion testVersion const TestVersion = 1 //Buffer buffer type type Buffer struct { buffer []byte full int size int s, e int } //NewBuffer creates a new Buffer func NewBuffer(size int) *Buffer { return \u0026Buffer{buffer: make([]byte, size), s: 0, e: 0, size: size, full: 0} } //ReadByte reads a byte from b Buffer func (b *Buffer) ReadByte() (byte, error) { if b.full == 0 { return 0, fmt.Errorf(\"Danger Will Robinson: %s\", b) } readByte := b.buffer[b.s] b.s = (b.s + 1) % b.size b.full-- return readByte, nil } //WriteByte writes c byte to the buffer func (b *Buffer) WriteByte(c byte) error { if b.full+1 \u003e b.size { return fmt.Errorf(\"Danger Will Robinson: %s\", b) } b.buffer[b.e] = c b.e = (b.e + 1) % b.size b.full++ return nil } //Overwrite overwrites the oldest byte in Buffer func (b *Buffer) Overwrite(c byte) { b.buffer[b.s] = c b.s = (b.s + 1) % b.size } //Reset resets the buffer func (b *Buffer) Reset() { *b = *NewBuffer(b.size) } func (b *Buffer) String() string { return fmt.Sprintf(\"Buffer: %d, %d, %d, %d\", b.buffer, b.s, b.e, b.size) } ","title":"Circular buffer in Go","uri":"/2015/10/15/circular-buffer-in-go/"},{"content":"Hi Folks.\nEver used Job DSL pluginfor Jenkins? What is that you say? Well, it’s TEH most awesome plug-in for Jenkins to have, because you can CODE your job configuration and put it under source control.\nToday, however, I’m not going to write about that because the tutorials on Jenkins JOB DSL are very extensive and very well done. Anyone can pick them up.\nToday, I would like to write about a part of it which is even more interesting. And that is, extracting re-occurring parts in your job configurations.\nIf you have jobs, which have a common part that is repeated everywhere, you usually have an urge to extracted that into one place, lest it changes and you have to go an apply the change everywhere. That’s not very efficient. But how do you do that in something which looks like a JSON descriptor?\nFret not, it is just Groovy. And being just groovy, you can use Groovy to implement parts of the job description and then apply that implementation to the job in the DSL.\nSuppose you have an email which you send after every job for which the DSL looks like this:\nNow, that big chunk of email setting is copied into a bunch of files, which is pretty ugly. And once you try to change it, you’ll have to change it everywhere. Also, the interesting bits here are those readFileFromWorkspace parts. Those allow us to export even larger chunks of the script into external files. Now, because the slave might be located somewhere else, you should not use new File(‘file’).text in your job DSL. readFileFromWorkspace in the background does that, but applies correct way to the PATH it looks on for the file specified.\nLet’s put this into a groovy script, shall we? Create a utilities folder where the DSL is and create a groovy file in it like this one:\nThe function addEmailTemplate gets two parameters. A job, which is an implementation of a Job, and a dslFactory which is a DslFactory. That factory is an interface which defines our readFileFromWorkspace. Where do we get the implementation from then? That will be from the Job. Let’s alter our job to apply this Groovy script.\nNotice three things here.\n#1 =\u003e import. We import the script from utilities folder which we created and placed the script into it.\n#2 =\u003e def myJob. We create a variable which will contain our job’s description.\n#3 =\u003e this. ‘this’ will be the DslFactory. That’s where we get our readFileFromWorkspace implementation.\nAnd that’s it. We have extracted a part of our job which is re-occurring and we found our implementation for our readFileFromWorkspace. DslFactory has most of the things which you need in a job description, would you want to expand on this and extract other bits and pieces.\nHave fun, and happy coding!\nAs always,\nThanks for reading!\nGergely.\n","title":"Jenkins Job DSL and Groovy goodness","uri":"/2015/10/15/jenkins-job-dsl-and-groovy-goodness/"},{"content":"Quickly wrote up the Data Munger code kata in Go.\nNext time, I want better abstractions. And a way to select columns based on their header data. For now, this is not bad.\n","title":"DataMunger Kata with Go","uri":"/2015/10/04/datamunger-kata-with-go/"},{"content":"Hello folks.\nToday, I would like to talk about something I came in contact with, and was hard to find a proper answer / solution for it.\nSo I’m writing this down to document my findings. Like the title says, this is about aggregating test result with Jenkins, using the plug-in provided. If you, like me, have a pipeline structure which do not work on the same artifact, but do have a upstream-downstream relationship, you will have a hard time configuring and making Aggregation work. So here is how, I fixed the issue.\nConnection In order for the aggregation to work, there needs to be an artifact connection between the upstream and downstream projects. And that is the key. But if you don’t have that, well, let’s create one. I have a parent job configured like this one. =\u003e\nAs you can see, it’s pretty basic. It isn’t much. It’s supposed to be a trigger job for downstream projects. You could have this one at anything. Maybe scheduled, or have some kind of gathering here of some results, and so on and so forth. The end part of the configuration is the interesting bit.\nAggregation is setup, but it won’t work, because despite there being an upstream/downstream relationship, there also needs to be an artifact connection which uses fingerprinting. Fingerprinting for Jenkins is needed in oder to make the physical connection between the jobs via hashes. This is what you will get if that is not setup:\nBut if there is no artifact between them, what do you do? You create one.\nThe Artifact which Binds Us Adding a simple timestamp file is enough to make a connection. So let’s do that. This is how it will look like =\u003e\nThe important bits about this picture are the small echo which simply creates a file which will contain some time stamp data, and after that the archive artifact, which also fingerprints that file, marking it with a hash which identifies this job as using that particular artifact.\nNow, the next step is to create the connection. For that, you need the artifact copy plugin =\u003e Copy Artifact Plugin.\nWith this, we create the childs configuration like this:\nAgain, the improtant bit is this:\nAfter the copy is setup, we launch our parent job and if everything is correct, you should see something like this:\nWrapping it Up For final words, important bit to take away from this is that you need an artifact connection between the jobs to make this work. Whatever your downstream / upstream connection is, it doesn’t matter. Also, there can be a problem that you have everything set up, and there are artifacts which bind the jobs together but you still can’t see the results, then your best option is to specify the jobs BY NAME in the aggregate test plug-in like this:\nI know this is a pain if there are multiple jobs, but at least, jenkins is providing you with Autoexpande once you start typing.\nOf course this also works with multiple downstream jobs if they copy the artifact to themselves.\nAny questions, please feel free to comment and I will answer to the best of my knowledge.\nCheers, Gergely.\n","title":"How to Aggregate Tests with Jenkins with Aggregate Plugin on non-relating jobs","uri":"/2015/10/02/how-to-aggregate-tests-with-jenkins-with-aggregate-plugin-on-non-relating-jobs/"},{"content":"I used to have great ideas on the toilet, but I no longer do. And I would like to reflect on that. So this is not going to be a technical post, rather some ramblings.\nI already had a post similar to this one, but I failed to follow up on it, and now I’m re-visiting the question. With technology on the rise, embedded systems, chips, augmented biology and information being available at our fingertips, I have but one concern. I don’t want to sound like an old guy reflecting on history, that now everything is changing and that we need to have a sight on the past and bla bla bla. I do have one valid concern though. We are in danger of loosing ourselves.\nWith mobile phones and the Internet always being around and available, we are in danger of loosing our thoughts and ideas, our individuality and our THINKING. We are reading news, posts, advancements, blogs, vlogs, and the dreaded 9gag. I am one of these people. I read 9gag. And I hate myself for it. It’s an immediate satisfaction and gain of euphoria and a way of shutting my brain down when it needs it. But I caught myself doing it one or more times when I should have read something more important or beneficial at least. Or catch up on a blog post or read a news, or Gods forbid just plain sit around and THINK for a little while.\nSo my previous post around this topic was to leave out technology from your life’s for a short period of time. This is the same. Have some alone time. Reflect. Write a diary. If you are a technical person, write down ideas you would want to create. If you don’t have any, write out bigger ones. For example, I want to write an RPG. Or That I want to learn how to do metaprogramming the proper way. Or that I want to read up on some Russian sciences fiction. There are SOOOO many things in the world. Don’t waste it on bullshit and immediate serotonin generating content, like frigging cats! When you do it, when your are at it, stop for a little bit, and think. THINK. What are you doing? Why are you reading up on that crap? What merit does it have?\nI understand that from time to time you need to shut off. You need a little bit of comfort. A little bit of serotonin in your system. There are better ways of achieving that. Go for a walk. Run. Bike. Eat a chocolate while staring out of a window. Read a comic book. Do random acts of kindness (not kidding). Drink a glass of water. Listen to some awesome music while drawing something ( anything, it doesn’t have to be a masterpiece! ). Sit back and listen to some music. Talk to a loved one. Talk to a friend. Talk to yourself (again, not kidding). If you have a pet, go play with it.\nSo I have a little challenge here as well -it would be a reflective post if I didn’t have any-, do not bring any electronic devices to the toilet. Or if you bring one, the rule is to turn on Airplane mode. I used to have great ideas on the toilet because I didn’t used to watch stuff on my phone. I used to be by myself with my thoughts. I have a family so there is very little time or space to be alone and with my thoughts. And then when I had the chance, I was browsing on my phone, which again, effectively, led to not being alone with my thoughts.\nThere you have it. This is my little rant about technology and thinking.\nThanks for reading,\nAnd as always,\nHave a nice day.\nGergely.\n","title":"I used to have great ideas on the toilet, but I no longer do.","uri":"/2015/09/07/i-used-to-have-great-ideas-on-the-toilet-but-i-no-longer-do/"},{"content":"I’m pretty proud of this one as well.\n","title":"Sieve of Eratosthenes in Go","uri":"/2015/07/30/sieve-of-eratosthenes-in-go/"},{"content":"If you are installing something with Packer and you have Headless enabled(and you are lazy and don’t want to switch it off), it gets difficult, to see output.\nEspecially on a windows install the Answer File / Unattended install can be like =\u003e Waiting for SSH… for about an hour or two! If you are doing this locally fret not. Just start VirtualBox, and watch the Preview section which will display the current state even if it’s a headless install!\nIt’s a small windows, but your can click on Show which will open the VM in a proper view.\nEnjoy, Gergely.\n","title":"Quick Tip for Debugging Headless Locally","uri":"/2015/07/22/quick-tip-for-debugging-headless-locally/"},{"content":"I quiet like this one. My first go program snippet without any peaking or googling. I’m proud, though it could be improved with a bit of struct magic and such and such. And it only counts ’till 1000…\n","title":"Converting numbers into string representations","uri":"/2015/07/19/converting-numbers-into-string-representations/"},{"content":"So, recently, the tester team talked to me, that their build takes too long, and why is that? A quick look at their configuration and build scripts showed me, that they are actually using a vagrant box, which never gets destroyed or re-started at least. To remedy this problem, I came up with the following solution…\nSame old… Same as in my previous post, we are going to build a Windows Machine for this purpose. The only addition to my previous settings, will be some Java install, downloading selenium and installing Chrome, and Firefox.\nInstallation Answer File Here is the configuration and setup of Windows before the provision phase.\nThis is the part were I’m installing Java. The script for the jdk_inst.ps1 is in my previous post, but I’ll paste it here for ease of read.\nThis installs both x86 and 64 bit version of Java.\nProvision I decided to put these into the provision phase to get log messages written out properly. Because in the unattended file, you can’t see any progress.\nChrome And Firefox Installing these two proved a little bit more difficult. Chrome didn’t really like me to download their installer without accepting something first, like Java. Luckily, after a LOT of digging, I found a chrome installer which lets you install silently. Here is the script to install the two.\nThey both install silently. Pretty neat.\nSelenium This only has to be downloaded, so this is pretty simple. Vagrant will handle the startup of course when it does a vagrant up.\nStraightforward.\nThe Packer Json File So putting this all together, here is the Packer JSON file for this:\nAdditional Software This is not done here. Obviously, in order to test your stuff, you first need to install your software on this box. Ideally, everything you need should be in the code you clone to this box, and should be contained mostly. And your application deployment should take core of that. But, if you require something like a DB, postgres, oracle, whatnot, than this is the place where you would install all that.\nVagrant and Using the Packer Box Now, this has been interesting so far, but how do you actually go about using this image? That’s the real question now, isn’t it? Having a box, just sitting on a shared folder, doesn’t do you too much good. So let’s create a Jenkins job, which utilizes this box in a job which runs a bunch of tests for some application.\nVagrantfile Your vagrant file, could either be generated automatically, under source control ( which is preferred ) or sitting somewhere entirely elsewhere. In any case, it would look something like this.\nEasy, no? Here is the script to start selenium.\nStraight forward. We also are forwarding the port on which Selenium is running in order for the test to see it.\nThe Jenkins Job The job can be anything. This is actually too large to cover here. It could be a gradle job, a maven job, an ant, a nant – or whatever is running the test -, job; it’s up to you.\nJust make sure that before the test runs, do a vagrant up and after the test finishes, in an ALWAYS TO BE EXECUTED HOOK -like gradle’s finalizedBy , call a vagrant destroy. This way, your test will always run on a clean instance that has the necessary stuff on it.\nClosing words So, there you have it. It’s relatively simple. Tying this all into your infrastructure might prove difficult though depending on how rigid your deployment is. But it will always help you make your tests a bit more robust.\nAlso, you could run the whole deployment and test phase on a vagrant box, from the start, which is tied to jenkins as a slave and gets started when the job starts and destroyed when the job ends. That way you wouldn’t have to create a, box in a box running on a box, kind of effect.\nThanks for reading,\nGergely.\n","title":"Selenium Testing with Packer and Vagrant","uri":"/2015/07/16/selenium-testing-with-packer-and-vagrant/"},{"content":"The first, and only time so far, that I got to use the bitwise \u0026 operator. I enjoyed doing so!!\nAnd of course from now on, I’ll be looking for more opportunities to (ab)use it.\n","title":"Bitwise \u0026 Operator","uri":"/2015/07/15/bitwise-operator/"},{"content":"Previously I wrote that the scripts I’m writing, are failing because Packer hangs.\nApparently, this was a known issue. And apparently, I was using an older version, 0.7.5. After I updated everything is working wonderfully!!!\nAnd for my thanks, here is an updated PowerShell script for provisioning my dotnet stuff.\nThanks for reading!\nGergely.\n","title":"Packer 0.8.1.","uri":"/2015/07/01/packer-0-8-1/"},{"content":"Hello folks.\nToday, I would like to show you a small script. It installs Java JDK, both version, x86 and 64 bit, silently, and wait for that process to finish.\nThe wait is necessary because /s on a java install has the nasty habit of running in the background. If you are using a .bat file, you shouldn’t, than you would use something like: start /w jdk-setup.exe /s. This gets it done, but is ugly. Also, if you are using Packer and PowerShell provisioning, you might want to set up some environment variables as well for the next script. And you want that property to be available and you don’t want to mess it up with setting a path into a file and then re-setting your path on the begin of your other script. Or pass it around with Packer. No. Use a proper PowerShell script. Learn it. It’s not that hard. Be a professional. Don’t hack something together for the next person to suffer at.\nHere is how I did it. Hope it helps somebody out.\nNow, there is room for improvement here. Like checking exit code, doing something extra after a failed exit. Throwing an exception, and so on and so forth. But this is a much needed improvement from calling a BAT file.\nAnd you would use this in a Packer JSON file like this..\nEasy. And at the end, the System.Environment actually writes out into the registry permanently so no need to pass it around in a file or something ugly like that.\nHope this helps somebody.\nThanks for reading.\nGergely.\n","title":"Powershell can also be nice -Or Installing Java silently and waiting","uri":"/2015/06/30/powershell-can-also-be-nice-or-installing-java-silently-and-waiting/"},{"content":"Hello folks.\nToday, I would like to write about something close to my heart recently. I’ve been fiddling with Packer, Windows and Vagrant these days. Trying to get a Windows box up in running is a pain in the arse though, so I thought I share my pain with you nice folks out there. Let’s begin.\nSetup First things first. You need Packer, and Vagrantobviously. I’ll leave the install up to you. Next, you should clone this git repo =\u003e Packer Windows Plugin. This plugin contains all the files necessary to get, install, and provision Windows boxes. Luckily, some very nice and clever folks, figured out a lot of things about how to install stuff on Windows. And given that people at Microsoft realised that sys admins would like to install stuff remotely, there are a bunch of forums and places where you can search for how to install software without user interaction. And this is the keyword you should look for =\u003e unattended Windows install.\nThis will lead you further into the bowls of Windows technology and silent / quiet installs all over the place.\nPacker and Answer Files When it comes to installing software on Windows, you have quite a few obstacles to overtake. One of the biggest obstacle you are facing, are restarts. Windows has a special place in hell for that. Every time you install something important which requires system libraries or other kind of configuration which “will only take effect after you restart Windows” you have to do a restart. Now, a little background on how Packer interacts with Windows. At the moment, it uses OpenSSH to talk to the box which has to be the last which comes up. If it looses connection to openssh because, I don’t know, it restarted itself, you loose communication to the box, and the setup process stops in mid tracks.\nIf you read about that in an earlier attempt to overtake this, you saw that you could use time-outs. You could kill ssh process which presumably makes packer do an attempt to start a new connection. If you are like me, you experienced that Packer does indeed NOT re-try. Because the previous task couldn’t finish, the restart killed the ssh service which could tell Packer that the previous task, an install for example, has finished. Hence, Packer will stay there and wait for that task to complete; which will never happen at this point.\nWhat can we do? Enter the world of Answer Files. Basically, it’s an xml file which sets up Windows. When Packer is running this file, the last service which should be installed, must be openSSH. And after that, in the provisioning phase, you should only install software which does not require restarts.\nLet’s look at an example.\nExample #1: Windows Updates This is another layer of purgatory for Windows. It’s updates. The updates take massive amount of times, if you are doing them from scratch, and also require several restart before it’s actually done. You **could **speed up the process a little bit, if you have a private network share where all of the Windows updates are sitting. At least that way you don’t have to download them every time you are creating a box. But you can’t avert the install process itself.\nLet’s look at a setup for packer. Packer works with JSON files for it’s configuration. An example for a Windows 7 box would look something like this:\nIf it feels daunting, don’t worry. You’ll get used to it fairly quickly. Let’s go over section by section on what this does.\nBuilders Packer uses builders for, well, building stuff. These two builders are virtualbox and vmware. I’m only interested in virtualbox. This builder downloads win7 and sets up some virtual box details like, disk size, vagrant user, memory, and so and so forth. The interesting part is the floppy part. Here, we can add in some files for setup. We will use this part later on.\nProvisioners Now here is an interesting tid-bit. There are a bunch of provisioners available as plugin for packer. Installing them is fairly easy. Packer needs binary plugins. Just copy them into ~/.packer.d/plugins or directly into the packer home directly. I’d advice against that. Have them in your own packer.d, that’s much cleaner. For binary plugin releases in the Windows side, look here =\u003e https://github.com/packer-community/packer-windows-plugins/releases. If you would like to build them yourself from source, download the source and use go gcc to build it. You will have to go get a few packages though. Also you will have to have $GOPATH (pointing to your own workspace) and $GOROOT (pointing to your working go) setup. But this is not a Go guide. After that just do **go build main.go **and you have your plugin.\nProvisioners are like vagrant provision they will execute post setup stuff on your box. Like installing utils, 7zip, choco, nuget, and so and so forth. There are a few interesting Windows provisioners, like restart-windows, powershell, and Windows shell. Which is like shell, but without the need of pre-setup if you are trying to use it on Windows. The basic shell on Windows is a little clanky and can hang from time-to-time so I recommend using PowerShell or WindowsShell provisioner if you are dealing with Windows post-setup Setup.\nPost-Processor This will create the Vagrant box after everything is done.\nRunning the Update For use, two things are interesting from here at this moment. These guys =\u003e\nThese two contain most of the logic which is part of the update process. You should see it in your checked out source. There is some very interesting logic in there which describes how the update happens. Basically it’s a loop which re-checks if there are updates available or if a re-start is needed. Packer handles re-starts well at this point in the install because it simply waits for SSH to come only. The rest is handled by Windows.\nThese scripts are called in the Answer File which the Windows Setup uses for configuration purposes. Take a look at this section:\nThis is were the floppy part comes on handy. This part uses the scripts bound by floppy and which will be available from a:.\nThis will install all the updates available. It will take a while. A very very long while… But let’s go a step further.\nExample #2: Installing DotNet 4.5 Let’s assume you want to create a box with visual studio 2013, office, and have choco on it, and a couple of more things for which you need lots of restarts. You could try installing with /norestart switch, which also works; however if you definitely need it to restart I suggest installing stuff with the Answer File. For this, let’s create a PowerShell script which downloads and installs dotnet 451 which is needed for visual studio ultimate 2013.\nSo this downloads it right from the source. As mentioned earlier, you could have this on a nice shared drive so downloading from the internet is not necessary. The installer is in fact a bit friendly. It has a switch called /q /norestart. The /q is called silent install and the /norestart speaks for itself. If you leave it out, you can use /forcerestart or you could have the following two lines after this finishes: LogWrite “Resarting Computer.” Restart-Computer -Force. This will force a restart. You need the -Force because otherwise it won’t let it restart while there are active sessions logged on the computer.\nNow, let’s add this to the answer file:\nSee, how easy this is? And now we make use of the floppy part of the windows-7.json by adding this line: “./scripts/install-dotnet-451.ps1”. Don’t forget to append the “,” at the end of the previous line. This is an array.\nWe are ready to go. Just run packer build -only=virtualbox-iso windows-7.json and you should be done!\nExample #3: Installing Visual Studio Ultimate Installing visual studio is almost trivial as well. With the addition that visual studio requires an admin.xml for silent install which has a bunch of settings. When you have the admin.xml just bind it into the floppy drive as well and call the visual studio install powershell script like this:\nAgain, this will take a while………….\nPost Setup Provisioning When all this is done, you can still add some provisioning steps to add some utils with PowerShell or WindowsShell provisioner. I would advice against using simple shell. Bare in mind one other thing. If you have a batch file, and you are calling another batch file in that batch file, like choco install 7zip, it will happen that the install process will hang on installing 7zip. Because in Windows land the called script will not return the exec handler to the caller unless specifically asking for it with call. Which means your bat file will look something like this:\nor\nAnd so on, and so forth.\nWrap-Up So, what have we learned? We have learned that installing software which requires re-start is better left to Windows itself with an answer file. Batch files will not return the handler. SSH MUST be the last thing you start up in the answer file. Use PowerShell provisioner or WindowsShell provisioner on Windows.\nHope this helped.\nHappy installing, and as always,\nThanks for reading.\nGergely.\n","title":"The Packer, The Windows, and the Vagrant box","uri":"/2015/06/27/the-packer-the-windows-and-the-vagrant-box/"},{"content":"Hello folks.\nToday, I would like to write about something interesting and close to me at the moment. I’m going to setup Go.cd with Docker, and I’m going to get a Ruby Lotus app running. Let’s get started.\nFluff Now, obviously, you don’t really need Go.Cd or Docker to setup a Java Gradle application, since it’s dead easy. But I’m going to do it just for the heck of it.\nSetup Okay, lets start with Vagrant. Docker’s strength is coming from Linux’s process isolation capabilities it’s not yet properly working on OSX or Windows. You have a couple of options if you’d like to try never the less, like boot2docker, or a Tiny Linux kernel, but at that point, I think it’s easier to use a VM.\nVagrant So, let’s start with my small Vagrantfile.\nVery simple. I’m setting up a trusty64(because docker requires 3.10 \u003c= x) box and then doing a simple shell provision. Also, I gave it a bit juice, since go-server requires a raw power. Here is the shell script:\nThe debconf at the end accepts java8’s terms and conditions. And the last line installs docker in my box. This runs for a little while…\nThe routing on the end routes every traffic from 172.17.*.* to my vagrant box, which in turn I’ll be able to use from my mac local, like 127.0.0.1:8153/go/home.\nAfter a vagrant up, my box is ready to be used.\nDocker When that’s finished, we can move on to the next part, which is writing a little Dockerfile for our image. Go.cd will require java and a couple of other things, so let’s automate the installation of that so we don’t have to do it by hand.\nHere is a Dockerfile I came up with:\nSo, our docker images have to be setup with Java as well for go.cd which I’m taking care of here, and a little bit extra, to add vim, and unzip, which is required for dpkg later.\nAt this point run: docker build -t ubuntu:go . -\u003e This will use the dockerfile and create the ubuntu:go image. Note the **. **at the end.\nGo.cd Now, I’m creating two containers. One, go-server, will be the go server, and the other, go-agent, will be the go agent.\nFirst, go-server:\nPretty straight forward, no? We forward 8153 to vagrant (which forwards it to my mac), so after we start go-server service we should be able to visit: http://127.0.0.1:8153/go/home.\nLo’, and behold, go server. Let’s add an agent too.\nNo need to forward anything here. And as you can see, my agent was added successfully.\nAll nice, and dandy. The agent is there, and I enabled it, so it’s ready to work. Let’s give it something to do, shall we?\nThe App I’m going to use my gradle project which is on github. This one =\u003e https://github.com/Skarlso/DataMung.git.\nVery basic setup. Just check it out and then build \u0026 run tests. Easy, right?\nFirst step in this process, define the pipeline. I’m going to keep it simple. Name the pipeline DataMunger. Group is Linux. Now, in go.cd you have to define something called, an environment. Environment can be anything you want, I’m going to go with Linux. You have to assign agents to this environment who fulfil it and the pipeline which will use that environment. More on that you can read in the go.cd documentation. This is how you would handle a pipeline which uses linux, and a windows environment at the same time.\nIn step one you have to define something called the Material. That will be the source on which the agent will work. This can be multiple, in different folders within the confines of the pipeline, or singular.\nI defined my git project and tested the connection OK. Next up is the first **Stage **and the initial **Job **to perform. This, for me, will be a compile or an assemble, and later on a test run.\nNow, Go is awesome in parallelising jobs. If my project would be large enough, I could have multiple jobs here. But for now, I’ll use stages because they run subsequently. So, first stage, compile. Next stage, testing and archiving the results.\nI added the next stage and defined the artefact. Go supports test-reports. If you define the path to a test artefact than go will parse it and create a nice report out of it.\nNow, let’s run it. It will probably fail on something. 😉\nWell, I’ll be… It worked on the first run.\nAnd here are the test results.\nWrap-up Well, that’s it folks. Gradle project, with vagrant, docker, and go.cd. I hope you all enjoyed reading about it as much as I did doing it.\nAny questions, please feel free to ask it in the comment section below.\nCheers, Have a nice weekend, Gergely.\n","title":"Docker + Java + Vagrant+ GO.CD","uri":"/2015/06/06/docker-ruby-lotus-go-cd/"},{"content":"Hello folks.\nSo, some of you know puppet, some of you don’t. Puppet is a configuration management system. It’s quite awesome. I like working with it. One of the benefits of puppet is, that I never, ever, EVER have to setup a new laptop from scratch, EVER again.\nI’m writing a puppet manifest file which sets up my new laptop to my liking. I will improve it as I go along. Here is version 1.0.\nI’ll improve upon it as I go, and you can check it out later from my git repo. I removed the parts which required extra libraries for now, as I want it to run without the need of getting extra stuff installed. I might automate that part as well later on.\nEDIT: https://github.com/Skarlso/puppet/blob/master/manifests/base_setup.ppHave fun.\nGergely.\n","title":"Setting up a new Laptop with Puppet","uri":"/2015/05/21/setting-up-a-new-laptop-with-puppet/"},{"content":"Fact is, I’ve been busy.\nI’ve got a new job as a build engineer. As sort of a devops kind of guy. It’s extremely interesting considering that I made a career as a tester. Granted, I always was technical, and never really knew my path; but it seems my path is finding me after all.\nIn the past years, I got better at Docker, Puppet, Chef, AWS, Packer, Vagrant, Gradle, and a hell of a lot more. Also honed my linux skills from the ability of doing an ls -l to do an find . -type f -atime +5 | xargs rm -fr (find all the files which are 5 days older and pipe them to a delete command). I already read many books about devops but this time, it’s different. This time, I can actually do these things as well in a live environment.\nAs once a friend of mine told me: “You ain’t gonna learn anything unless you are getting payed for it.” Wise words. True words.\nSo stayed tuned for some devops and engineering type posts. I would like to continue working on the Django parts as well, however given my priorities and lack of time ( family and stuff, must choose what I learned in the hour I get each day), it might fall behind. I might stick it into some kind of provision practice or even give it a Travis.ci and put it into a gradle project. Now THAT’S interesting.\nCheers folks.\nAnd as always,\nThanks for reading.\nGergely.\n","title":"Busy building the future","uri":"/2015/05/19/busy-building-the-future/"},{"content":"Hello folks.\nA small update to this. I created the model now, which is the database design for this app. It’s very simple, nothing fancy. Also, I’m writing the app with Python 3 from now on.\nHere is the model now:\nWorth noting a few things here. The __str__ is only with Python 3. In Python 2 it would be unicode. And the OneToOne and the foreign key are automatically using Primary keys defined in the references model. The __str__ is there to return some view when you are debugging in the console instead of [\u003cItem: Item object\u003e].\nIn order to apply this change you just have to run this commend (given you set up your app in the settings.py as an INSTALLED_APP):\nThis creates the migration script. And this applies it:\nI love the fact that django creates incremental migration scripts out of the box. So if there was any problem at all, you can always roll back. Which comes very handy in certain situations.\nThat’s it.\nThanks for reading!\nGergely.\n","title":"Django – RPG – Part 3","uri":"/2015/04/21/django-rpg-part-3/"},{"content":"Hello.\nContinuing where we left off with the Django RPG project. Next up is implementing a rudimentary registration and adding the ability to create a character. Maybe even, design the database through django’s modelling.\nSince we are using Django’s very own authentication model, I think we are covered in terms of users. Let’s add two things for now. An Index page, where there is a link to login and a link to registration.\nAdding the index first. Later I would like to switch to a base template model, but for now, I created a simple index.html page. That only contains the two links to the two views. The views are a simple function call in the views.py too which the URLConfig will later point to.\nFor now, the index function looks like this:\nNote, that the title here is utterly unimportant but because I want to switch to a base.html template I’ll leave it here for later usage.\nThat concludes the index. Now, let’s create the registration. That is a little more complex, but still rather easy. We are just checking of the user already exists or not, if so, display and error, if not, create the user.\nHere, I’m checking to see of the username already exists with the filter. This is by using Django’s model which models the database like hibernate. It’s a simple query. And I’m doing this, because this is faster than raising an exception. Later on, I’ll be switching to a validation framework and django’s own auth view. Because, why not.\nThe URL conf looks like this:\nAnd this now, resides in a file under the RPG app and not the main one. The main one includes this one, like this:\nThat’s it for now. As always, you can check out the code under github.\nTune in next time, when I’ll attempt to create a view to create a Character for a logged in user and link it to the user. I’ll do this with django’s model framework.\nThanks for reading,\nGergely.\n","title":"Django – RPG – Part 2","uri":"/2015/04/12/django-rpg-part-2/"},{"content":"Hi folks.\nSo last time, we tried to implement a little RPG of mine using Meteor, which failed miserably. This time we are going to try and use Django. Let’s have at it, shall we?\nIf you don’t know what django is (than you are probably not reading this blog entry anyways… ), have a look =\u003e Link. It’s a Python Web Framework.\nIn the beginning So here we are again. I’m starting off by creating the skeleton for my RPG. First, the welcome page with a tiny login capability. Which means session tracking. For now, on the same page.\nTo create the skeleton I just run this little command:\nEasy enough, right? Basic directory structure is created. You can read that in django’s own documentation: https://docs.djangoproject.com/en/1.8/intro/tutorial01/I’m using Sublime Text 3 to build this app. I was thinking of pycharm, but that would take away too much fun.\nBy default Django uses SQLlite and I’m okay with that.\nThe basics are there. Let’s move to higher ground. The next step can be to design the login page. I’m not much of a designer so I’ll leave that part to people who care more / understand it better. I’m only looking for a simple Login with a username and a password. Nothing fancy. And I’m going to use Django’s auth system for users: django.contrib.auth.It’s pretty good, has lots of features and can auth a user pretty quickly given a username and password, which is all I want for now.\nI running a migrate command to create all the necessary tables and data:\nA quick check with sqlite3 and running **.schema **showed me that the tables are indeed created and filled with default data for all the **INSTALLED_APPS **django has at the moment.\nI did a quick check with python manage.py runserver and everything seems to work fine.\nThe RPG App  So, that concludes the setup. Following the tutorial, django has this notion of apps and projects. In short, a project is a set of configurations and applications which forms a website. Hence, I’m going to create an RPG APP for now. The tutorial continues with a Poll type of App, but I’m sure I can apply the same things to a login page.\nFirst, let’s create the App using the command:\nThat creates the skeleton for the app. The tutorial then suggests to define the database first. Which I think is a bad idea; usually when doing design, you don’t want to start with the database model. But for now, I shall comply. We want a login page and we are using the auth module, which means, for now, I don’t care about the database. Skip.\nI’m going to go off corse for now, since I only want to create a basic login. For that, I have to read up on views and models a little bit in order to create a simple login page. Be right back…\nI’m fighting the basic auth system at the moment. Everything seems to be working fine. In the Admin screen I was able to add a basic user to do some debugging, but for now, the login seems to not work with this error:\nCSRF verification failed. Request aborted.\nThis is the CSRF protection which came out with Django. I did a bit of more document reading and setup the CSRF as was described here:\nhttps://docs.djangoproject.com/en/1.8/ref/csrf/However, I’m still facing the same problem. Let’s research further… Ahh yes. I wasn’t returning the request context I’ve set up. Before:\nAfter:\nNotice that now I’m returning the request context at the end when I’m rendering the response. And lo’ and behold, my login is now working.\nProgress so far  So, I’ve set up a user using the Admin portal accessible from the app / admin. Created my default rpg app, and added my default auth view into a template folder. Configured the URL setting in urls.py to look like this:\nVery basic for now, but when I return to it, we are going to clean it up and then continue with implementing the main screen which will greet the user and display some stats about his/her character.\nThe whole code is under github here =\u003e https://github.com/Skarlso/myrpgBye for now,\nAnd thanks for reading.\nGergely.\n","title":"Django – RPG – Part 1","uri":"/2015/04/10/django-rpg-2/"},{"content":"Hi folks.\nJust a small script which calculates your distance from a lever focal point if you know your weight, the object’s weight and the object’s and the distance the object has from the focal point of the lever.\nLike this:\nThis script will give you D1. And this is how it will look like in doing so:\nSo, in order for me (77kg) to lift an object of 80kg which is on a, by default, 1 meter long lever, I have to stand back ~1.03meters. Which is totally cool, right?\nHere is the code:\nPlease enjoy, and feel free to alter in any way. I’m using Tkinter and a grid layout which I find very easy to work with.\nThanks for reading, Gergely.\n","title":"Small Python GUI to Calculate Lever Distance","uri":"/2015/04/10/small-python-gui-to-calculate-lever-distance/"},{"content":"Let’s talk about plans. It’s good to have one. For example, I have a plan for this year.\nI kind of like math. So, I have this book:\nIt’s 1400 pages long and basically, has everything in it. It’s a rather exhaustive book. Hence, my plan is to finish the book by the end of 2015 and write a couple of python scripts that calculate something interesting.\nFor example, Newton’s law of cooling how I learned it is:\nWhere k =\u003e a material’s surface based constant. Tzero =\u003e initial temperature. T =\u003e target temperature. K =\u003e Environment’s temperature.\nA simple python script for this:\nEnjoy.\nAnd as always, Thanks for reading!\n","title":"Python and my Math commitment","uri":"/2015/03/15/python-and-my-math-commitment/"},{"content":"Here we are again. I will attempt to further this little journey of mine into the land of Android and Python.\nThis is the second part of the advanture you can read the first one a little bit back.\n We left off at a point where I successfully configured my environment and compiled my first hello world APK. At that point it took a little bit fiddling to get it to work on my phone.\nNow, I have progressed a little bit into spoj’s page parsing. The code so far is as follows:\nThis is pretty straight forward so far. It gets the problems page, loads in all of the links and prints it out.\nMy goal is an application which looks something like this:\n_________________\n| ______________ |\n| | | |\n| | | |\n| | Display Problem Description | |\n| | | |\n| | | |\n| | | |\n| |_________________ | |\n| |\n| |\n| Button:Finish Problem |\n| |\n| Button:Next Problem |\n|____________________ |\nIt’s very basic. When it loads up, it will gather and display a new problem. You have two options, either get a new one, or save / finish this item, saying you never want to see it again.\nLet’s put the first part into an android app. Just gather data, and get it disaplyed.\n*Queue a days worth of hacking and frustrated cussing.*\nSo, turns out it’s not as easy as I would have liked it to be. I ran into some pretty nasty problems. Some of them I’ll write down below for the record, and an attempted solution as well.\n#1: Problem: Libraries. I’m using lxml and requests. Requests is a pure python library, but lxml is partially C. Which apparently is not very well supported yet.\nSolution (Partial): I could optain request by two ways, but the most simple one, was basically just building my distribution with the optional requests module like this:\nAttempting to do the same with LXML resulted in a compile issue which I tracked down to something like: “sorry, but we don’t support OSX”. But it’s okay. There are other ways to parse an html page, I just really like the xpath filter. So I soldiered on with trying to get something to work at least.\n#3: Problem: Bogus compile time exception. There were some exceptions on the way when I was trying to compile with buildozer. Solution: It’s interesting because previously my solution to another compile time issue was to use a specific version of Cython. But this time the solution was to actually remove that version and install the latest one. Which is 0.22 as of the time of this writing. So:\n#2: Problem: Connection. So now, I’m down to the bare bone. At this point, I just want to see a page source in a label. My code looks like this:\nHowever, running this results in a connection error in adb logcat:\nSolution: I tried simply putting out a random number at some point, which actullay worked, so I know it’s the connection. I’m guessing I need permission to access the network. Which would be this:\nAnd yes! Building and installing it with this additional permission got me so far as I can display the web page’s content in a label.\nThere is a saying that you should end on a high note, so that is what I’m going to do here right now. Join me next time, when I’ll try to replace lxml with something else…\nThanks for reading!\nGergely.\n","title":"Sphere Judge Online – Python Kivy Android app – Part 2","uri":"/2015/03/02/sphere-judge-online-python-kivy-android-app-part-2/"},{"content":"Hello folks.\nToday I would like to take you on a journey I fought myself through in order to write a python android app, which gets you a random problem from Sphere Judge Online. Then you can mark it as solved and it will be stored as such, and you can move on to the next problem. With the words of Neil deGrasse Tyson, Come with Me!\nWhen I first embarked on this endeavour I ran into numerous errors, many amongst them being compilation issues when I was trying to install libraries.\nI started to write down all of these, and then started fresh on a new machine. I realised that ALL of my problems where only because of **ONE **thing. One thing, which I wanted to do, but it ended up being the death of me. And that is…. *Drummrolls* **Python 3. **I tried doing all the things that I started to do, with Python 3. Turns out, that neither libraries are supporting it very well yet. And that’s including Cython as well, which I thought would be up to speed by now. But sadly, it’s not.\nIn order to go any further we need a few things first. For this to work, you’ll have to perform these things in order as I found out later. And certain versions of certain libraries are required instead of the latest ones.\nDepending on the environment you are using, you need to install python-dev and some other graphic libraries. I followed this and that was fine. Latest packages are working alright.\nOnly install these if you are absolutely certain you need them.\nClone python-android from git into a nice and cosy directory.\nWhile this is underway, for python-android you also need android-sdkand android-ndk. Select the ones which are for your environment. The NDK is needed in order to build the APK out of our python code later on.\nAfter you are done, run ./android and install tools, APIs and other things you want. Make sure you have these set up:\nThe API version needs to be the one which you installed on your machine.\nNow, we have to get a specific version of Cython. In order to do that, execute the following command:\nSource your new .bash_profile file if you haven’t done so already.\nAt this point we are ready to install Kivy. Please follow the instructions for your environment on the respective page from Kivy’s documentation:\nhttp://kivy.org/docs/installation/installation.htmlNote: For Mac users. In addition, before doing the kivy stuff, and if you would like to execute kivy applications on your mac, you need to install pygame.\nIt’s a bit of a hassle but you only need to perform these commands:\nInstall Quartz =\u003e http://xquartz.macosforge.org/landing/Install Homebrew =\u003e ruby -e “$(curl -fsSL https://raw.github.com/Homebrew/homebrew/go/install)\u0026#8221;Install some other packages =\u003e brew install hg sdl sdl_image sdl_mixer sdl_ttf portmidiInstall pygame =\u003e pip install hg+http://bitbucket.org/pygame/pygameOnce this finishes, you should be good to go for the final command in the prerequisites. Go to your cloned python-android folder and run this (make sure you have ANT installed):\nNow we are ready for some coding.\nSo, finally after our environment is all setup, we can move on to write some python code. Let’s start with a simple hello world application:\nThis is a simple Hello World python-android app. Save this into a file called main.py. Main.py is used to execute the app on your phone. It’s your entry point. Whatever app you are writing, this has to be where it will begin.\nIn order to get this installed on our device, we will use python-android’s distribution.sh. The command to run after you changed directory into python-android is this (make sure that you have a compatible android device plugged in and in developer mode):\nUpon success, you should see it on your device. This is how the hello world app looks like:\nThis has been quite the ride so far. We will continue our journey when I’ll start writing my own app for SPOJ.\nThanks for reading! Gergely.\n","title":"Sphere Judge Online – Python Kivy Android app","uri":"/2015/02/26/sphere-judge-online-python-kivy-android-app/"},{"content":" Not a great many people know that I’m actually into Lock Picking as a hobby. This will not be a tutorial on how to do it, or I won’t really talk about how I do it; I would like to write about something completely different. So if you came here for that, here are a few very good resources:\nhttp://www.lockpicking101.com/ – Tutorials\nhttp://www.ukbumpkeys.com/collections/lock-picking– Tools ( UK )\nhttp://www.reddit.com/r/lockpicking/– Reddit\nFor my post, click on…\nSo, why is lock picking like testing? I saw a great many posts which where trying to compare software testing to some activity. I even knew somebody who compared it to dancing. Because…? I don’t know, maybe he was trying out a talk material maybe? And he said – “I was in this club and watched a couple dance, and I was thinking, wow, this is like software testing.” – I couldn’t really react to that. But hey, kudos for out of the box thinking I guess.\nBut let’s see some of the similarities in picking and testing:\nThinking It requires logical, critical thinking. It’s a puzzle that you have to solve. A puzzle which you basically solve by testing out solutions and see if they work. You can achieve this by trial and error, something testers face on a daily basis. After awhile you acquire something called finesse. Which will help you identify breaking points more easily. Since you have experience with a certain lock type, you already have a wast knowledge to rely on when you are trying to solve a new lock with the same build. You did your research you know its weaknesses hence you immediately have an attack vector on your hand.\nTools Lock picking has a wast number of tools. Each tool is design for a specific purpose. You can reuse tools but they are rarely a fit for other locks. Unless it’s a similar one. There are delicate tools and then there are brute force tools, which are very hard to use at first but after you get used to it and know how to handle it, it becomes massively helpful. You can build your own tools or use tools that are built for you by tool experts.\nSolving Process When can you test an application to its fullest extend and capabilities? How can you achieve the most and best testing ever possible? You have to know how the application works. You have to know its components, its abilities, its capabilities and **limits. **For Lock Picking to be the best at what you do and to be able to move to a completely new lock and try to pick it, you have to know its flaws. You have to know when, and how it was built. You have to know what makes it tick, how it works, what it uses, how the inside mechanics look like in order to try to exploit them.\nAfter you acquire this knowledge you will be able to build, or purchase a tool which will help you in solving the puzzle. But you were only able to do that because after you researched its manufacturing procedure you know that the last pin is hard to get to and that it has an anti drilling shield, so drilling is out of the question, and that it has a safety spring which locks the lock permanently upon tampering. If you would not be in the possession of this information you could have made a fatal error and could have made the client lost money ( since at that point they pretty much need to break down the door, unless the key is found again ( assuming the picking took place because the key was lost ) ).\nLast Words There you have it folks. That’s why Lock Picking is like software testing. Knowing structure, inside workings and the proper tools will help to achieve your goal. Just like in testing when you know your field, you know what to use, when, and how. Knowledge makes you the Best.\nGood luck,\nHappy Picking,\nGergely.\n","title":"Why Lock Picking is like Testing","uri":"/2015/02/08/why-lock-picking-is-like-testing/"},{"content":"In my previous post, I was getting ready to enjoy some time with the JavaScript web framework Meteor.\nThis time I would like to bring it to a bit of overdrive. See, how re-factoring works on a larger scale model with multiple pages. And how it can organize assets, such as, images, multiple CSS, some plugins, you know, ordinary web stuff.\nLet’s dive in.\nI’m planning this to be a series of posts as I’m going along building up my RPG app. Let’s define the rules.\nIn the beginning  Rules  Inventory  Our main character will have a basic inventory. He will have space to carry stuff around and a body to put stuff on. One ring on each hand, one weapon in each hand, helmet, armour, legs, and a necklace. That’s it. For simplicities sake. The game mechanics will be like those old books which you could play, Fighting RPG Books, like the one Ian Livingstone was writing. This is one of my favourites; Robot commando:\nStats  A very basic stat system.\n Strength Agility Constitution Intelligence Magic   Fighting  A very basic fighting system with the possibility of casting magic which, for simplicity, will count as attacks and can be dodged based on agility.\nLet’s say we have dice throwing with a couple of 6 sided ones. So X * 6 sided dice. Dodging will require agility, HP is defined by constitution, Intelligence will help in puzzles which require a throw against intelligence, Magic will define Mana Points.\nSimple, right?\nDesign  I’m not much of a front-end developer, so I don’t really care about how it will look like. I’ll try to squeeze in some very basic stuff, like ordering, but that’s it.\nGame Play  Basically there will be a story which can be loaded threw a JSON structured file. The file will hold information about what a current page has. The probable things a page can contain at any given time:\n Current location description Selectable proceed location ( page number ) Enemy -\u003e Fight ( Might contain an option to not to attack the beast ) Riddle -\u003e Solving it is determined by a throw against intelligence Trap -\u003e Springing it is determined by a throw against agility Lootable items Death  All of the above define an action that a player can, or HAS to take. If there is no ability to choose the player has to proceed as the page requests it. That might be easier to do if I just say if there is only one possible choose it’s choosen automatically for you.\nImplementation  I’ll be using Meteor which is based on Node and MongoDB. Hence, my stuff will be in mongoDB. I have a fair knowledge of how mongodb works, I’ll write down my progress as I go along.\nEverything I’ll do is of course under version control and can be followed here:\nhttps://github.com/Skarlso/coolrpgappCharacter  I need to be able to create a character with a name. Meaning, I need to figure out how meteor handles input. I already know that it uses templates and Spacebars Compiler. So what I want at this point is to enter a username and then click a button which will direct me to the story page. Simple, right…?\nFor data handling we will use Meteor’s Collections.\nUsing a form to submit the username looks like this:\nOf course there is no way to know if that actually succeeded so far unless I get a look at the DB. Navigate to the folder of your app and type in:\nThis will open a console to your database where you can query it like you would normally do with a mongodb console. Hence for me it’s:\nAs you can see, I already have two characters in the system. This is so far very easy but it does not redirect me to a new page displaying the beginning of my journey. Let’s try a redirect.\nComplications Turns out it’s not that easy to get a redirect going. If I would be a beginner at this, I would give up right now and move on. The guide, or the tutorial does not contain any HINTS at least that I have to use a different method if I want a multi-layered multi-paged app. Of course Meteor provides a built in, easy to use, easy to add, answer-to-everything-you-ever-would-want-to-do, Login feature. But guys, it’s not useful. I would go as far as say it’s completely useless. Do you actually know someone who uses it? I would never use a built in something which is completely hidden from me and have no idea what it does. The ability to control what’s happening is THE most important thing in every developers life.\nSo after I did a bit of digging and StackOverflowing ( which replaces the tutorial AND the user guide (and is a trademarked expression)), I found out that you can add Iron-Routerwhich was built specifically for this purpose.\nSo all of a sudden my Page is completely screwed up with Iron Router information. Again, there is no information on this on Meteors page or in the guide nor in the COMPLETE guide so, I’m left Googling.\nA very helpful StackOverflow ( again, and I’m wondering why people don’t bother with the guide in the first place just go to stackoverflow straight ) answer explains to me the following:\n“You have to define a subscription handle (an object returned by Meteor.subscribe) in order to use it’s reactive ready method : we’ll reference it in the myDataIsReady helper to track data availability, and the helper will automatically rerun when the state of ready changes.”\nOkay, so subscriptions are mentioned in the SECURITY section of the guide regarding detecting specific users and private data and so on and so forth. All right so that’s used by iron routing as well which means I have to build that in, and not to mention first of all understanding how Iron Router works.\nI’m going to stop here now. After spending a couple of hours I can determine that this stuff is not intuitive and “easy”. I don’t know enough about JavaScript and redirecting and Iron Router to be able to use Meteor out of the box. Which means I have to educate myself a bit before returning to this stuff.\nStay tuned for more.\nAnd as always, Thanks for reading!\n","title":"Building an RPG App with Meteor – Part One – The struggle","uri":"/2015/02/01/building-an-rpg-app-with-meteor-part-one-the-struggle/"},{"content":"Hi,\nThis time I would like to write about something that interests me. I wanted to try out a pure JavaScript web framework.\nMy choice is: Meteor. Looks interesting enough and it was recommended by a friend of mine. So, let’s dive in.\nInstallation As always, one starts with installation. The page tells us to follow this simple step:\nEasy enough, when you are on Linux. Turns out, that there is no official release yet for Windows. I’m in luck then. After running the command though, I saw this popping up into my face:\nThere is always something… in that case a more accurate command to use would be the following:\nThis will force an insecure download. You might not face this issue, but just in case you do, use this command instead.\nBranching off here. For those of you whom the curl didn’t work because you are sitting behind a proxy you can specify a –proxy protocol//username:password@proxy:port after your curl. Of course if that doesn’t work then the script won’t work either.\nSo open the script in one of your favourite editors, for me it’s Sublime text, and find this line: “Downloading Meteor distribution“. Lo, and behold; it uses curl. This is the only one in the script, so just edit it by adding in your –proxy setting as before and you should be right on track.\nIf that still gives you problems, try this:\nAssuming that your browser is set up correctly with the proxy and just command line commands aren’t working, you can go to this URL defined by the variable TARBALL_URL:\nNote that there are two variables in there. For me these are:\nRELEASE: 1.0.3.1\nPLATFORM: os.linux.x86_64\nThe full URL is:\nDownload the latest tarball and delete the CURL AND TAR command on the following line. After that, you just have to extract the tarball and move the directory to ~/.meteor.\nNow you can run your sh again and you should be on the road, for sure this time.\nJust to make sure, these are the line which you need to comment out:\n Getting started After a nice installation process we can continue to the getting started phase.\nSo, the documentation tells us that we have to simply execute a command.\nAt this point we should get a directory structure which is written in the manual. And, behold, that’s exactly what happened. As usually, creating a skeleton is easy. Lets run the app. For that, the command is:\nI can do that, I think.\nAnd sure enough, I’ve got this little message, which I actually expected to see:\nIn this world, where there are tons of applications running on your dev environment at any given time, it’s possible to have something already running on the port 3000. Luckily this is something that’s anticipated by now, and we are presented with an option to add in a proxy setting of our choice with –port .\nAfter I did that, I’ve got a nice confirm message that meteor is up and running. A quick check on the presented URL provided me with the confidence that my app is indeed reachable.\nAfter Getting Started… Now that we know that it’s up and running we can continue with the tutorial. Up comes next a simple Todo list application with Templates. It’s telling us to replace the code in the default starter app. At this point I’m wondering if it can hotswap. It should, since javascript and HTML is dynamic so there should be no problems there, right?\nAnd sure enough, the moment I replaced the code and checked on my server status, I could see this:\nWith a brief flash of “Rebuilding…”. So it does sort of work. It did, however, restart the server it just did it without your manual intervention. Which is nice, but on a larger scale application it might prove to be a tad bit annoying. For example, I add another item to the list, and suddenly, the server is restarted.\nSince, I am a tester, let’s see how it handles some problems.\nI modified the JavaScript so that it has a syntax error.\nNote the missing “,”. And, nicely enough I’m getting an error message telling me that I messed something up:\nIt even tells you where the error is and it’s waiting for you to fix it. After I’ve corrected my error it compiled fine and the application is up and running. Deleting the files did little difference as did corrupting the HTML pages or the CSS file. Nothing to see here, moving on…\nAndroid Device I’m sure everybody can read a manual and continue with collections, forms, events and such. What I’m more interested in is that Meteor promises it can run on Android devices. Now that perked my curiosity. With the rise of mobile devices, the desktop platform is slowly pushed back into a dark corner where even a Tineye would have problems seeing it.\nHence, I want to see how easy it really is.\nMeteor gives you a set of commands to install the android sdk and droid support for your application, which is nice. You just need to run this:\nNow, if you are like me, someone who has experience with the android SDK and its emulator, you’ll know that running that thing requires more time and processing power than simulating the chances of Leonardo DiCaprio winning an Oscar. I’ll use a real device instead. For that, it appears I only have to run a simple command again.\nAnd sure enough the app appeared on my device.\nThis is actually quite awesome. I only plugged in my device, enabled developer options and USB debugging and that’s it. I’m quite impressed so far with Meteor and the Power of JavaScript. The app is on my phone and the static JavaScript parts are still working even though I shut the server down.\nSo my next burning question is… Will it Blend? I mean, Perform?\nBenchmarking So, now that I know that using, installing and getting started is pretty simple, what I also would like to know is how well it performs.\nI have a quad core i7 16GB RAM Samsung SSD running Linux. Let’s see 100 threads 10 second interval 10 times loop for a start. Look at how gorgeous this is.\n40ms on average. Now let’s crank it up and I’m performing the test on a separate machine but still on the same network. 1000 threads.\nThis time I’ve got a bit more churn and my pc started to fan like there is no tomorrow. But the server stayed stable. Latency did not waver for a bit. Next, 10.000 for as long as my machine can handle it…. Better save my work. Hah, my JMeter died. But it clocked at an average of 1000ms response time and the server stayed absolutely stable with no package lost, or errors.\nConclusion I can say with a full heart that I’m impressed by Meteor and I very much like it. It’s easy to use, even more easy to install and definitely can handle itself given that it’s rather lightweight. The hot swapping / server re-starting can’t be avoided, but that’s only a minor inconvenience and we got used to that already.\nI recommend Meteor and I’ll be playing around with it a bit more for sure.\nThanks for reading! Gergely.\n","title":"JavaScript Web Framework – Meteor","uri":"/2015/01/29/javascript-web-framework-meteor/"},{"content":"Hi,\nLet’s face the horrible truth:\nIt’s rare / never happens that a manager / scrum master / product owner actually reads your cucumber test cases.\nBack in the old days, this was one of the selling points of human readable tests and DSLs. It sounds nice and I’m sure in a utopia it also works.\nBDD is a very nice approach to write tests if used in a correct way. And I can relate that at some point, a manager or the product owner, actually writes up a draft of the tests. But that enthusiasm very rarely stays for the rest of the project.\nEspecially when you get to the point where your Cucumber test cases start to look something like this:\nIf a product owner reads this, his reaction will be like: “What the hell is this? What’s users.json? Why is it there? Why should I even care? What’s a JSON response? Why should it match with the request? And what, if I keep the id at USER_ID? Huh?”\nIt’s easy to get overwhelmed by things like this scenario when you start introducing actors into your tests and payloads to your public API. And suddenly you’ll end up with cucumber features which no other will be able to understand but the person who wrote it.\nI’m a little bit skeptic that it ever worked as intended. Sure, for a little while. But the dynamic nature of tests will surface soon enough. You can’t hide it forever.\nThe above example, if the payload and user would be hidden in a reusable code fragment behind the implementation, would look a bit more readable:\nSee? Easier to understand. I don’t care about the payload. I don’t care about the user ID, in fact, I would rather see this test as a unit test somewhere deep down in the bowls of the system. Although I can understand that you want a set of automated UATs.\nI’m sure Cucumber has a couple of success stories behind his back, I just didn’t happen to come across them as of late. But please, if you have one, share it with me so I can rest easily.\nGergely.\n","title":"When cucumber goes wrong","uri":"/2015/01/28/when-cucumber-goes-wrong/"},{"content":"I thought I throw my grudge out of the window against Scala and try something with it.\nI also got my hands on a fairly new book, called: “Learning Scala: Practical Functional Programming for the JVM“. Turns out to be a rather fun book to read. And Jason Swartz has a nice way of writing. So I wanted to play around with Play 2 Framework. It now comes packaged in Activator.\nSo, I started the long path from almost zero to handle all that. I’m running the latest Ubuntu ( 14 ) and latest Java ( 8 ). The list: Scala, SBT, IntelliJ, Play ( through activator ).\nI was pleased that, considering a network which allowed me a download speed of ~1.5MB/s ( that’s byte, not bit ), I was up and running in about 4 minutes. That’s Play running, with a test application created through activator and then imported into an IntelliJ Scala project.\nI’m impressed.\nI added SBT through the package manager like this:\necho \"deb http://dl.bintray.com/sbt/debian /\" | sudo tee -a /etc/apt/sources.list.d/sbt.list sudo apt-get update sudo apt-get install sbt  It’s really simple.\nAfter that, I did an apt-get on the latest Scala.\nI already had IntelliJ.\nActivator download took me ~1 minute; then I executed the command to create a test app:\nSimple. Activator downloaded everything my system was still missing.\nThen run the start command:\nAnd you are ready to rock \u0026 roll. Importing it in IntelliJ was a blink of an eye.\nI’m really impressed with how easy getting started became with these projects and frameworks. I remember a time where I had to configure everything, get tomcat and the whole JVM or Jetty or whatnot, and try to get up and running took half a day at least. Would my internet be faster, I think this would have been even less.\nI’ll post more as I go forward.\nAs always,\nThanks for reading.\n","title":"From Zero to Hundred in Four seconds","uri":"/2015/01/27/from-zero-to-hundred-in-four-seconds/"},{"content":"Hello everybody.\nToday, I would like to write about little observation I made along the way of being a tester.\nThe title says it all. I think Testers, are like scientists. I would go even further, and say: Testers, are scientists.\nLet us see how the two compare.\nAsking Questions A scientist is all about asking the right question to the right person, or the right subject matter. They are using the same 5 Ws as we are. Who, Why, What, When, Where. It’s simple, really. Asking the right question is sometimes the most difficult task of them all. Ideas come from these questions. They can also help in flushing out early design problems with the experiment / product.\nIdentify the subject matter This one is all about getting to know your environment and taking notes while you are doing so. Become familiar with what you are working with, and what tools you are using in your research. Which standards are you following, and why? It’s the same, when a Tester finds herself/himself in a new environment and tries to get her(is) bearing what the task is and how to perform it.\nFirst impression, outline of an idea The first impression will result in a vague idea of what the problem is about. This is the stage where the first thoughts emerge and the first draft of a possible solution is crafted into existence. This is the part where a person will draw information, power, and knowledge from her(is) past resulting in a solution that can get close to a real one; but rarely is this solution the one which will be chosen in the end.\nTesting Indeed in the life of a scientist it is imperative that ideas, solutions and vague drafts of brainstormed possibilities are **tested all the way through. **For a tester it’s just as important that all the cases are verified and ‘approved’ by either common sense, the requirements, or any other kind of verification that they can come up in certain environments. Fact is, that if a solution / test case is a valid one, depends on several different options based on any given circumstance. That is why it’s difficult to decide if something is acceptable or not. What is a good solution in one case, might be entirely wrong in another.\nResults If the tests / variables / cases / solutions are satisfying, then we can come to think about producing an actual result. The result could be anything of value to the observer. A failing test could be a good result, if we actually were waiting for that test to fail. In that case the result is a success. So bare in mind that a failing test, a failed solution to an experiment, is **always **also information which **tells you something important. **Never dismiss a failing test, a failed experiment, a failed solution. It gives you a **hint **for a passing one.\nCommunity and Sharing Scientists have a large community where they are sharing experiments and discoveries. I think I don’t have to draw a parallel here. We all know several testing pages, forums, events, gatherings where there are people talking about the subject matter. I’m proud that it such a long way.\nPractice, practice, practice I think this one is self explanatory. In order to become good at something, we have to continuously do it. There is no magic formula, no pill you can take, no matrix to download Kung Fu, you have to do it to learn it. You have to practice it, gather experience and share / write them down in order to strengthen your knowledge and understanding about it. Writing it out, and sharing information, **explaining it to somebody else, **will make it stick better and increase your knowledge in return. So start a blog now, even if you will have only a few readers it will be worth it. Just by writing a couple of lines about something will make you remember it better, and your understanding of it deeper.\nSo, there you have it. I think these are pretty good points that let’s us identify ourselves. It’s called Computer science for a reason.\nAs always,\nThanks for reading,\nGergely.\n","title":"Why Testers are, like scientists","uri":"/2015/01/26/why-testers-are-like-scientists/"},{"content":"Hello Everybody.\nThis time I’d like to write about the expressiveness of a Test. I think that it’s very important to write understandable and expressive tests. In older times I was studying novel writing. There is a rule which a novel needs to follow. It goes something like this: “A novel needs to lead its reader and make him understand in the simplest way what’s going on, with whom and why?”. In other words, it’s not a puzzle. It should be obvious what the test is trying to do and it should not require the reader to try and solve it in order to understand it.\nI’m planning this as a series since there are multiple problems with a test I can talk about here.\nGeb Tests\nExample:\nNow, read this test. It doesn’t really make any sense at the first read, right? You need to actually think what is going on there. Of course if you read it slow enough you’ll get what it’s trying to do. But you don’t know what fillform does. Apparently it also submits the form because after fillform you are suddenly at MyAccountPage.\nThere are several things wrong with this one, let’s start with the pageobject.\nPageObjects\nAt and toAt return page objects. We can use that to actually make the calling explicit and make it more readable and identify where a function comes from.\nThis reads much better now. You know where the function is coming from and your IDE will not go nuts from things it can’t find. And you have autocompletion so there is no fear that you simply mistype something.\nSide effects\nNext step, let’s remove some of the side effects.\nNow this is again much better. There are no steps left out. And you can test now the FillForm and the submit independently. Like, submitting the form without filling it out! Or filling it out and not submiting it. Reads better, is explicit, more easy to understand.\nAnd the last one for today:\nGrammar\nI wonder if you noticed it… The grammar is a little bit off in the tests. A small mistake here and there. You might think that, who cares? That’s a very bad thought. I think the correct grammar reflects caring. It reflects that we thought about this test and that we thought about the quality of it. Because it means that after you wrote it, you actually re-read the test to make sure it’s understandable and readable.\nSo let us correct that:\nI also took the liberty of re-phrasing some of the text so that it shows what the test is about and what the user really would like to achieve here. Now try reading that last one. Does it make more sense? Did you understand it at first go? Did it read like a good story?\nThere is a coding practice which goes something like this: “Good code is code which doesn’t surprise you as you read it.” Which means the exact thing happens which you thought of would happen. I think that applies to tests as well. The steps of the test shouldn’t come to you as a surprise. Especially if you know what the application is supposed to do.\nSo that’s all for today folks. Thank you for reading! If you have a nasty test which you would like me to dissect and make it better and human readable, please share it in the comment section and I will do my best to come up with a good solution for it.\nAnd as always,\nHave a nice day!\nGergely.\n","title":"Why the expressiveness of your Tests is important – Part One","uri":"/2014/11/15/why-the-expressiveness-of-your-tests-is-important-part-one/"},{"content":"Hello everybody.\nI would like to share with you a small script I wrote to update all, or a single, Jenkins job from a Python script remotely.\nThis will enable you to update a Jenkins job from anywhere using an admin credential based on a config.xml template that you have. With this, if you want to apply a config change to all or just a single job in Jenkins, you don’t have to go and do it for all the rest. You just call this script and it will cycle through all the jobs you have and update them if the begin with “yourpipelinedelimiter” or if they aren’t in a restricted list of jobs. The delimiter helps to identify pipelines which are dev pipelines. If you have multiple pipelines which are helpers or builders and you don’t usually apply the same config to them, than the delimiter can help identify the dev pipelines you actually want to update.\nEnjoy, hope it helps someone.\nAnd now, without any further ado:\nThanks for reading.\nGergely.\n","title":"Updating All Jenkins Jobs Via Jenkins API – Python","uri":"/2014/11/07/updating-all-jenkins-jobs-via-jenkins-api-python/"},{"content":"Hi Folks!\nI just wanted to share with you my mind map on the seven principles of Context Driven Testing.\nI used the notion of dragons since it’s seven. And a dragon is usually seven headed in my mind…\nSo without further ado…\nClick for larger image.\nI have a hand drawn as well but I’m guessing that one wouldn’t be of much use since it follows my own imagination.\nHere are the seven principles in points which correspond to the leafs.\n Value -\u003e Practice -\u003e Depends -\u003e Context =\u003e The value of any practice depends on its context. People -\u003e Working -\u003e Together -\u003e Important =\u003e People, working together, are the most important part of any project’s context. ( The mob, trying to kill the dragon by working together. ) Problem -\u003e Project -\u003e Solution =\u003e The product is a solution. If the problem isn’t solved, the product doesn’t work. ( The sword. It’s a sword but if it doesn’t kill the Dragon it’s useless. ) Good Testing -\u003e Intellectual Challenge =\u003e Good software testing is a challenging intellectual process. ( Well it’s a dragon with glasses. That’s something you rarely see. ) Good Practice | Bad Practice -\u003e In Context -\u003e No Best Practices =\u003e There are good practices in context, but there are no best practices. ( This one was a little bit tricky. But basically you want to solve the dungeon and not plow through it ) Uncertainty =\u003e Projects unfold over time in ways that are often not predictable. ( For me this one word was enough. ) Exercised By All In Time -\u003e Judgement | Skill | Cooperatively =\u003e Only through judgment and skill, exercised cooperatively throughout the entire project, are we able to do the right things at the right times to effectively test our products. ( It’s a flexing dragon… Easy, right? )  So this map helped me to remember everything. The descriptions, applications that are further described I remembered by using this mind map as a loci.\nI hope somebody found this useful.\nThanks for reading!\nGergely.\n","title":"The seven Principles of Context Driven Testing – Mind Map","uri":"/2014/10/23/the-seven-guidelines-of-context-driven-testing/"},{"content":"Hello Everybody.\nToday I’d like to write a little bit about a python course I did.\nIt’s an 8 week course on algorithmic programming with fun little projects. I’d like to write down some of my solutions with pseudo code for my own better understanding and for the sake of sharing knowledge. I won’t, however, share full projects since that would be against the honour code.\nLet’s begin…\nWeek Zero\nThis week was all about getting the hang out of reading the posts and the documentation and the questions and getting used to the wordings. The tutors really out did themselves. They tried to make a course that can be both funny and teach something at the same time which is anything but easy.\nEven though the tasks that were given were interesting al-bight at times a little bit far stretched and could have been easier done if used something else to complete them. But if we would have done that, what’s the point of it all then?\nSo I’ll try to recall everything solely based on my notes taken in those 8 weeks. Let’s see how much I truly learned.\nAssignment of week one\nWe were to build the game 2048 which if you played you know what it is all about.\nThe goal is to add up numbers so that higher and higher numbers are created. There is no real “end” of the game. You can continue as long as you have space left although the intended goal is to create 2048. There are a couple of clones of this game and we were supposed to write one this time of our own.\nWe approached this game with the intention of refreshing our memory about Python. Handling matrices python syntax, counting indices and a bit of assessment about the general understanding of Mathematics and programming from the populace.\nI must say it was hard. It was hard to get back into the habit of properly thinking about something at first. It was hard to get used to Math again which I missed for a very long time. I forgot many things and as English is not my first language many things written about Math in English were extremely hard to understand in the beginning.\nBut thankfully for my trusty mathematics Bible in Hungarian I was saved.\nThis book is 1448 pages long but contained all the information necessary to get my mind back into the game. And oh boy was it worth the initial trouble. I had to first realize that I forgot so much it was very painful and immensely disappointing, frustrating and shameful. But you should never give up and so I fought my way through it.\n   And it was extremely helpful to do Tests First. As the grading was based on how many of a given set of Unit Tests were passing it was very helpful to start tests first which were leading the design of the program. Also it was crucial to work in as little chunks as possible since one could easily lost himself trying to grasp a problem proving to be too large to look at from afar.\nThe hard part about this project was the Merging of the numbers and creating the proper grid which results from the Merge. Here are some examples:\n [2, 0, 2, 4]should return [4, 4, 0, 0] [0, 0, 2, 2]should return [4, 0, 0, 0] [2, 2, 0, 0]should return [4, 0, 0, 0] [2, 2, 2, 2]should return [4, 4, 0, 0] [8, 16, 16, 8]should return [8, 32, 8, 0]  In order to achieve this you must trim the zeros like this [2,2,4]and than produce the result which is [4,4]and put a couple of zeros at the end [4,4,0,0]. My first though was to use the Deque class in Python in order to achieve this but that was an outside module which was not allowed.\nIt was an interesting way to begin the course. Many people left at this point and were leaving afterwards too. Most of them in frustration that they were missing the python knowledge the rest out of frustration of not knowing the necessary math. At the end though it was getting easier to follow the problems after we got used to the conventions and sentence structures. The professors were also helpful and sometimes re-worded some of the descriptions to better describe what they wanted.\nWeek One\nSo after a hard start we moved on to a very interesting week one. This one got me into a certain game I don’t want to see ever again… It’s name: Cookie Clicker.\nWe sort of had to re-create the cookie clicker but without the clicking. We only were supposed to re-create the buying of upgrades and simulate a sequence of clicks via the means of a cycle.\nIn this week the description of the tasks was, at the least, confusing. They started to use the term _time _which lead many to believe that we were somehow supposed to use python’s date / time methods and libraries. But after a couple of re-reads it was apparent that by _time _they actually were referring to cycle count.** **\nKnowing this made the task at hand a lot easier. This time around our main focus were the following:\n Mathematical Sums Finding the Max Higher – Order functions Plotting with Python  The course just went into overdrive. We were looking a lot at thing like these:\n∑n i=0 α^i = α^0 + α^1 + α^2 + ... + α^n = α^(n+1) − 1 / α − 1It was rather awesome though frustrating at first like I wrote earlier. These were the easier one.\nFinding the maximum is trivial. Especially if you are using a built in **max **provided by Python. But if you mix it with Higher-Order functions it gets interesting. If you want to do anything else as well and not just a max, for example getting the index of the maximum item as well, you usually end up writing a custom Max any ways.\nPlotting in Python was exquisite interesting. The garphs which were produced showed as an insight into how powerful a solution really is or how effective. Here came in the Big-O notations almost. To see if a function was exponential, logarithmic or plain polynomial. O(n), O(n^2), O(logn) etc, etc.\nThis resulted in very interesting graphs like this:\nWeek Two\nSo last week we had plotting and counting this week was even more interesting. Week Two’s main focus was Probability. Specifically the Monte Carlomethods. Tl;dr; it describes that if you try something enough times you can derive a result that will be, with very high probability, the one you are looking for (expected value).\nWe tried out this algorithm by playing a nice game of tic-tac-toe.\nThe point of this exercise was to create a machine player which, after running a 1000 or so random scenarios, was choosing the best option given a certain game state. It was difficult to get the results to always return a correct answer. At this point last weeks plotting became important since if your Monet Carlo algorithm was not fast enough the program was running increasingly slower and slower.\nI was not very satisfied with my solution. It worked, but it was very slow and it wasn’t always returning the best option…\nWeek Three\nSo what comes after probability? Correct. **Combinatorics. **\nThe next hill to climb was combinatorics. Fortunately for me I love probability and combinatorics so this was a little bit easier for me. I was getting the hang out of function calculation as well so I wrote better homework and better projects at this point. Which is the aim of the course, right?\nThe game we used for this approach was Yahtzee.\nNow let me say this without too much remorse. I truly, fully and with all my heart, hate Yahtzee. I think it’s stupid. I’m sorry. I truly do. The only thing I can think of when I hear yahtzee is the South Park version of it.\n South Park Version of Yahtzee + Tron:\nI share the enthusiastic look of Stan here.\nAnyhow, moving on… Thank to the coursera Gods we weren’t suppose to write a whole game of Yathzee just a very simplified version of it. We were supposed to count the upper combinations on the second throw. So you already have one throw and you must choose how many die you want to hold on to to maximise the possibility of the best outcome possible. Huh… come again?\nSo you already had a hand. And now the program was to determine which die you were supposed to hold on too in order to maximise the score you can achieve with the remaining two throws.\nThere were a few interesting things that came in with this task. For example this was the first time I could use a Dict init from a list with a zip.\nAnd this was the point in course were I found perhaps the most interesting thing. I found an actual use for reduce which was working. I was beginning to get into the habit of using map, filter, reduce.\nThis was the beauty:\nThis piece of code produced all the combinations of a given hand which was a list of Tuples. It merged them into a list of lists which I created a list of Tuples out from. After this my life was never ever the same again.\nWeek Four\nFor me this was the most interesting part of the course. I LOVED this task. Focus was:\n Python Generators Stacks / Queues Inheritance in Python Girds Grid Search / Breadth First Search  And the task with which we were supposed to achieve this was…. Zombie Apocalypse. It was sort of like Conway’s Game of Life. Given a grid in which there were Zombies** **and Humans **Obstacles.**\nThe Zombies could only move up, down, left, right. The humans could flee diagonally and none of them could penetrate an obstacle. It was very much fun to write this. The most challenge was to learn the proper implementation of the breadth first search algorithm as the Zombies had to detect the nearest Humans to move towards to and the Humans needed to see the nearest Zombies to flee from.\nWeek Five\nHalfway through it was become difficult to maintain the time needed for this course. I was finding myself applying a few late days here and there. This was a two months course after all. I did not have all the time in the world at my disposal. But I managed to submit everything without penalties.\nSo this weeks task was rather mundane. It was a world wrangler. Which means given a word generate valid words from the letters in the provided word.\nThe algorithm we were supposed to use though was for me a bit of a challenge. I’ll be honest with you, for me, it was a little bit hard to wrap my head around this one. But eventually I succeeded.\nIt was Merge-Sort and Recursion. Let me tell you this now, I hate merge-sort. Recursion I do love though. What I never learned though and was very interesting for me to learn now was recurrences in mathematics. Well never learned is a bit harsh since I knew Fibonacci already and Pascal’s Triangle but the mathematical definition was a refreshing new view. I’m talking about Recurrence Relations.\nThis is the Recurrence relation of the famous Fibonacci. Easy, right? Well the hard part is when you are trying to get the closed-form-expressionof a Recurrence.\nWeek Six\nFinally it was coming to an end. The last week was easy so this week had to have a punch. And oh boy it did. This weeks focus:\n Trees Lambdas Illustration of Trees Minimax Depth First Search  If I hated merge-sort I hated minimax more. I don’t know why but it was again very hard for me to properly grasp this concept. I mean I understood what needed to be done of course but writing it done with code proved to be more difficult then I thought it would be.\nAfter hours of research and reading finally I could come up with a solution which was working. I wouldn’t say it was good… But it was working.\nThe game with which we were supposed to demonstrate this algorithm was… Tic-Tac-Toe. Turns out that it’s rather common place to show off minimax with tic-tac-toe as it was fewer possibilities. The point of the exercise was the following…:\n To create trees out of the possible moves given a certain game state. This time we wanted to make absolutely sure that if we can’t win the game at least we won’t loose it. And that’s the point of minimax. It will minimize your losses.\nNow there are several things about this algorithm that are hard.\nPerformance of Minimax\nIt has and always will be a very interesting task for programmers to try to achieve a better performance for these calculations. Since it’s trying to build up a tree with all the possible combinations a game can have it will end up with a huge tree which will take ages to traverse.\nA few of the solutions could be to exit the search once you have a definitive answer. If you find a winning move there is no point of looking any further. You just stop.\nYou can create the tree dynamically. You can make it somehow intelligent enough to predict a possible best first move and then use minimax on the rest. Or use Alpha-beta pruning.\nWeek Seven\nAnd so we are coming to an end. Last weeks assignment was basically to put all the previous weeks knowledge together to create the application called 15 puzzle.\nNow, there was however a little addition to the previous knowledge. It was invariants. Now, I love Logic. And I’ve been actually using invariants in computer science programming and testing for a long time so this part was not really a problem.\n  An example of an invariant in python would be something like this:\nThis is a loop invariant example. In you find a very useful invariant in your program you can write an assert for it which will help you debug your code and work in small chunks. Invariants will make refactoring your code a hell of a lot easier. As if your invariant is suddenly false you need to check what went wrong.\nThis assert will make sure that if your invariant for whatever reason isn’t true your code fails immediately.\nEnd Credits\nSo this was the end of the course. I learned a a lot from this course and I’m very proud of myself for completing it. I took away a lot from this course. I took away confidence and logical thinking. I took away greater trust in my Python knowledge and that it’s very important to keep my skills from deteriorating.\nAnd I think math is important for proper, deep understanding of programming as a science. I think refreshing my math skills gave me at least a deeper trust in my ability to write a piece of code however complicated it might appear. After writing a minimax algorithm I think some Hibernate with DIP and SRP will prove to be less of a problem. Or at least a different category of a problem…. Hehe.\nThanks for reading! Gergely.\n","title":"Python Course Review","uri":"/2014/08/25/python-course-review/"},{"content":"Hello Folks.\nSo last I was writing about why a tester should learn Javascript. Today I would like to write about why a tester should learn SQL.\nThere, I said it. I know many people, especially testers, don’t like SQL. They view it as a monster best be avoided. Something only Database people will know. Something which is so scary and ugly, nobody really wants it.\nBut I will give you a couple of good reasons why you shouldn’t be afraid of SQL. And why you should welcome it as your best friend and partner in crime.\nLet’s go.\nReason Number One: Data gathering\nThis one is obvious and clear as the sun. You can use simple queries to mine for data. To look up changes and compare time intervals between insertion and update events. You can monitor certain tables so that when they are updated, you’ll get a red flag.\nReason Number Two: Test Run Statistics\nThis one is from my friend Adrian. Basically if you would like to know more about what’s happening with your tests when they run, for example:\n Run time Frequency of Test outcome Failure rate Environment details -\u003e Execution slave  One interesting way to achieve this is, to have the test report running and outcomes into a little mysql database and than create queries of certain types like, show me the last run of every test called xyz and show me the environmental details and the run time. With this closely monitored you could find out that Slave #345 is sluggish because each time the test ran on it, it took more then 10 minutes where as the others only took 5-6.\nReason Number Three: Data Manipulation\nSo after monitoring comes naturally the edit. Understanding how databases work and knowing a few queries here and there can help you manipulating your data in a way that it will be easier to test data dependant scenarios.\nInstead of making a new entry you could edit what you already have.\nFor example:\nYou have a customer and you want to test the system’s ability to handle people on your site who are suspended from access. But the ability to suspend is not yet working. Will that stop you from testing this feature? Will you put this feature into blocked because: “Ohh, we can’t yet suspend a player so we need to wait until that’s done.”\nNo. You don’t wait. You dig into your database, find the necessary record, change it’s state to the desired state; even if it has a foreign key which needs to be updated or that status doesn’t exist yet, in which case you ADD it to the list of states yourself. You don’t let yourself be stopped just because something is not done yet. You move on by being clever.\nReason Number Four: Understanding Data migration issues\nA big issue these days if you go into a project where you have a previous version of what you are currently building will be migrating over old data into the new database scheme. Testing such a thing can be a pain in the butt. But it will be a LOT easier if you understand the changes. If you know what changed, how and why, you can manipulate your data in order to fit the new scheme. Or if you need to test it you won’t be afraid of running some stored procedures in a dummy database with old data and than run a few queries to see what broke and what didn’t.\nDo you have a foreign key violation somewhere when you migrated over and have no idea what do to? Time to learn some SQL so that you don’t have to run to somebody every time you encounter it. Even if you can’t fix it, the database engineers or the devs who will fix the bug will be very happy that you provided as much information as possible in your report.\nReason Number Five: Security\nSQL Injection is still at large. Even with these days frameworks doing full escapes it can’t hurt to test a couple just to be on the safe side. And writing a clever script that mines for accessible tables here and there is an essential skill in a security tester’s repertoire.\nReason Bonus: Performance, HibernateQL, Information\nLastly a bit of a bonus are these three.\nPerformance\nSuppose you are running a web application. You access a large list and you notice that it’s sort of sluggish. You first blame it on the network so you test it locally. It seems to be better now so you move on but it leaves a little voice in your head so you can’t abandon it. You go back and try locally again with a bit more data in your database.\nYou notice it’s a little bit better but it’s still somehow sluggish. Suddenly you get a hunch and turn on SQL logging in your tomcat instance. You click the link again and wait with eyes wide open on what happens next. It’s your worst nightmare.\nThe SQL queries the whole database with a simple select and than filters the data on the front-end side. Which is a really really dumb thing to do. So you file a bug for which the title is something similar to this: “Customer Transaction History query doesn’t apply WHERE | INNER SELECT | PAGING on SQL rather it filters on the application layer.”\nHibernateQL\nThis is the SQL language of hibernate which is the underlying technology in many java web frameworks these days. It uses it’s own thing called HQL. Main difference, as this page already says, is that it’s a full blown Object Oriented query language which understand inheritance and polymorphism which is very exciting.\nInformation\nLast but not least I mentioned this one earlier. You can provide more information in your bug reports as in what data you used, where was it happening, what was the last update date, which environment and what query was executed ( if you have query debugging turned on ). Whoever reads that bug report will find it very helpful that you provided enough information to reproduce it anywhere.\nBecause many times the culprit for a bug is the underlying data.\nReading and Practising resources\nHere is a very awesome picture of how to understand JOINS which is everybody’s fear.\nAnd a post on Coding Horrorwhich is essentially the same but I like how Jeff writes.\nAlso if you would like to practice writing SQL scripts and no longer be afraid of them all the rest of your life go to this site =\u003e SQLZoo. It’s an interactive way of trying out your SQL skills and testing them on very clever database structures.\nBut if you, like me, love to learn PLAYING than THIS is the place for you =\u003e The Schemaverse. It’s a SQL based space shooting strategy game awesomeness! Have FUN learning SQL.\nThanks for reading!\nHave a nice day|night!\nGergely.\n","title":"Five reasons why a tester should learn SQL","uri":"/2014/05/31/five-reasons-why-a-tester-should-learn-sql/"},{"content":"Is TDD dead?\nNot really. So let’s talk about this topic for a little bit.\nI’m sure you already read a gazillion posts about this debate but frankly I’m writing this for myself, to rant a little bit, you know… Because somebody is wrong on the internet and I must intervene.\nSo first of all, the hashtag #tddisdead (and I will use it shamelessly as well). This is clearly an attempt to get as many peoples attention as you can. TDD is NOT DEAD. Obviously since it has soooo many supporters how could it be dead? It’s like asking, is Design Patterns dead? Or is Functional Automation dead? Or is Oreo cookies dead?\nNo, it’s not dead. And it won’t ever be dead. It will maybe change into something new, something better even, but it will never be dead. So let’s skip that part.\nNow, about the debate.\nI haven’t hear so much bull spoken for this long since I watched the political debate of two fractions in my home country. The right wing extremists against the left wing…. I don’t know whats. And it was just that. A political debate. It had no merit and no value whatsoever. At all. Nothing.\nAnd right in the middle DHH says this:\n“…you’re not done until you also have tests for a piece of functionality — I’m completely on board with that.”.\nThat made the whole conversation completely irrelevant.\nEvery counter against TDD I heard was bull. Not in that debate, in general. People are either too lazy to write them, just don’t want to get out of their comfort zone, don’t really care about tests, or don’t really care about quality or under time pressure ( I get to this later. ).\nWhich brings me to my next point.\nQuality\nPeople seem to not care about quality that much. Would they, they would understand that having a bulletproof west will save your life when you get shot in the chest with a 357 magnum. You can flush out early design flaws you can detect early bugs and do a better system design.\nSure if you are the most intelligent man on the planet maybe you can come up with a perfect system on the first draft and then implement it flawlessly so that it doesn’t fall apart in two months time. But most people can’t. Most people make errors on the way.\nAnd yes, writing tests can be hard. But guess what? If writing a test is hard because that part of the system is complicated, than it will be that part of the system which will react the worst to change. And only change is constant. Which brings me to the next item…\nTime constraints\nSo your manager is sitting right next to you and saying come on we are paying you to write code and not tests so do it! And you have to have a feature done today but if you write a suite of tests you’ll only finish tomorrow. Sure, your estimate at that point will become a very quick one because you make a sacrifice of trust.\nAnd then the next story comes along and you say… “Sure I can do that as well. No problem I know how my system works, right? Hmm… why the hell did that break all of a sudden? I didn’t change anything in that module… Ahh damn it I said I’ll be done today, so let’s just fix this quickly and then move on to the next card.”\nAnd the next story comes along… “Sure I can do that… wait a minute… Didn’t that part brake already twice? Damn, better refactor. Ohh shit, why is that now breaking???? Damn it I said I’ll be done tomorrow, better patch it, and then move on. Hmm let’s write a test here to make sure this does not break. Ohhh damn I need PowerMock for that stuff since it’s in another module. Why the hell is that there? Should it be here in the first place since it’s somehow used by that other class there? Interesting. Let’s refactor and put it in here so I can mock it. Ahhhh f*ck now all the rest of the system is not working. Damn, I’ll just use PowerMock. Shit. Checkstyle error. PowerMock is not allowed?? What?? Who the f*ck says that?”\nYou get my drift. And suddenly you end up with estimates of WEEKS!!!! instead of days / hours for a simple story.\nFinishing it up\nThis a rant only. It’s my personal opinion, experience and observation of a 10 year time period in Software Testing. Starting with at least a Weak Skeleton and a few upfront tests will help you in the long run. Writing at least ONE – TWO acceptance tests WILL help you understand business logic better. Writing ONE or TWO unit tests will help you understand your logic better. I’m not saying write a whole damn suite of tests I can understand you don’t want to do that, but for quality’s sake write at least a couple.\nYou will love it, I promise you that.\nThanks for reading.\nGergely.\n","title":"TDD is Dead – Not really","uri":"/2014/05/26/tdd-is-dead-not-really/"},{"content":"Hello everybody.\nToday I would like to write about a very interesting topic, I hope. So let’s get started.\nAs the title already suggests, I’m writing about why a front-end tester should learn at least a little bit about JavaScripting and the DOM.\nOhhh and contrary to the belief CSP ( Content Security Policy ) will not be the death of such scripts. There are white-lists and workarounds and exclusions which can be implemented in order to allow local JavaScripting to continue. So don’t fret… Read on.\nReason Number 1: Injection\nEvery front-ender tester has a waste amount of tools at their disposal. Various things, like Firebug and web developer Toolbar and… Bookmarklets and Greasemonkeyand Tampermonkey. These are the real powerful tools though. Your main objective at this point would be to inject something into your current site.\nSuppose a scenario where you are testing something and you have an API at your disposal for a quick edit on a customer, like closing his account or giving him money, or doing something with his appliances. Suppose you don’t want to always switch away to another tab or call a service with certain parameters or go to a rest client and initiate that from there.\nSuppose you could have a small injected DIV right on the page you are which gathers information for you, like the customer’s username, and wallet and whatnot, and with a simple press of a button, you just closed their account. Neat, isn’t it? Simple, fast, very comfortable with just one press of a button.\nSuppose you have a DIV at the corner of a page, or even a drag and drop enabled one which you can drag around, with an arsenal of these buttons RIGHT THERE on your page. You don’t have to switch context at all.\nThese days it’s especially easy with tools like jQuery at your disposal. You just inject jQuery first, if the site is not already using it, and you are good to go and do whatever you like to do….\nReason Number 2: Data gathering\nWhile we are testing these application we always create some kind of a report. That report contains many things about the customer, or it’s appliances or the things he does, does not. All these could be constantly gathered by your script as it runs in the background. It gathers statistics and information which otherwise you would have to gather from some transaction history, or some kind of an action history. But no… Not the JavaScript Wizard.\nYou, would just press a button and the script would generate a report for you. It would even print it out. Create a persistent layer in which you are gathering information continuously. Create a small mySql database on your local machine and have the JavaScript enter data into that. Tadaaam…. Usage statistics at the touch of a button. All there, only waiting to be extracted.\nReason Number 3: Tempering\nIn understanding the ways of the DOM and the JavaScript you can create some very interesting test cases not to mention XSS attacks which is essentially JavaScript Injection. That’s always fun and produces many very good bugs.\nCookie manipulation. You want to simulate something? Like a time-out or a session loss or anything like that with a push of a button? Easy…\nReason Number 4: Shortcuts\nYou have a massive field like registration that you need to fill out? The shortest way is to have an API which you can call via a curl script. But if that’s not available and you would like to exercise the front-end any ways, then you will end up wasting hours and hours on always filling out all of those pesky fields.\nAnd suddenly I’m hearing: “But I’m using Selenium plugin for that.” – you might say. Sure, use that. But I’m using Chrome. “But there is iMacros for that.” – you might say again. Sure, I know… But! Let’s see which takes longer…\nOpen selenium, load the script, run it, see it fail, run it again, ahh success, good. Same with iMacros. As opposed to, having a Bookmarklet right in front of you, on your bookmark, and with a click of a button, or with entering something into the browsers search bar, you suddenly fill out the form and press submit.\nYou see the difference is that JavaScript runs faster and more accurate by it self in such short things then with a wrapper around it. And it’s faster accessible as well.\nReason Number 5: Security\nThere are all sorts of things that a security tester can do with a small script which gathers session information and availability.\nReason Number 6: Accessibility\nThis is of course the easiest one. There are ample of scripts and browser plugins to test accessibility which is an all time favourite for everybody in the front-end land. Make your life a little bit easier. How bout just running a bookmark like HTML_CodesSniffer an see a very gorgeous result like this:\nAin’t it beautiful? What stops YOU from writing your own?\nSo get out there and learn JavaScript. It’s easy. I’m not telling you to become a front-end developer, just know thy tools and you shellet receive the blessings of the IT Gods.\nAs always,\nThanks for reading.\nGergely.\n","title":"Five reasons why a front-end tester should learn Javascript","uri":"/2014/05/23/five-reasons-why-a-front-end-tester-should-learn-javascript/"},{"content":"Hello Everybody.\nToday I would like to tell you about a little conversation I had with my friend Adrian.\nThe topic was about how, when and what to log during a testing session regarding a story.\nSo let’s see what came out of that talk…\nLogging your work\nThe question is really not whether you should log your work or not, but rather to what extend. Because we all know that you should log your work. It helps you recall and helps you organize and think out testing strategies. But the extend and methods are a big question. So here is my routine.\nVisually inclined\nI am a very visual person. I log my work with doodles and such like Mr. Huib Schoots who uses Sketchnote taking. You have seen some of my sketches if you follow my twitter feed, like this one =\u003e . You know I like to draw so for me colour and organizing and doodling is key to make a note more personalized and live.\nThat comes with a drawback. The draw back are many…\nFirst of, I need to do that in a notebook. I could bring my wacoom tablet to work and draw in paint, but that wouldn’t be the same. And I don’t particularly like drawing on a tablet actually.\nSecond one, it’s completely unsearchable. As Adrian pointed out, you can’t search in a notebook, even if you are using side pointers like a sticky note or something. You won’t be able to instantly find and track down something you did. And after a while it gets cluttered with doodles and the notes are lost in-between. It will be very colourful and utterly useless.\nThird one is, it’s slow. Typing is much faster.\nOn the bright side… I love doodling. And if I can draw something up with a picture I will understand it much faster rather then reading a few lines of words chunked together.\nIf I draw an awesome diagram I will get what’s going on much much faster.\nAnd it will be more fun to actually right it down with a nice fountain pen with a beautiful tip that can create such marvels that you don’t want to put it down.\nIt’s portable. I can carry my notebook everywhere. I should be able to do the same with a laptop, but since it’s tied into a dock station and I don’t want to carry it around with myself everywhere I won’t do that. I could create notes though in my notebook and then write them down into the laptop after I’m done? Sure…\nSo now let’s go and see the other one.\nElectronic note-taking\nI think I already pointed out a few positives about that. It’s searchable. Definitely. It’s fast. Yes. It’s more convenient, more organized can be backed up and you won’t run out of ink or space or somebody won’t take it away to a meeting because “Ohh I saw it on your desk and I needed a paper hope that’s okay.”.\nI understand the benefits of it. But it’s very very rigid. It has no colour, no personalization and no feeling whatsoever. It’s frigid, standardized and without life. So I need something to make it a bit better. I have a couple of tools for that.\nFirst off I need a good graphics maker to create diagrams. There is an awesome free tool pointed out by Adrian to do that called yEd. Download the zip and run the jar. I can live with that. Creates nice diagrams and flowcharts.\nIt’s quite powerful. Go look it up. There are a gazillion picture on Ze internet.\nBut the notes still lack personalization. There are a couple of tools which you can use if you, like me, shy away from everything that’s Microsoft, you have the opportunity to use Emacs with the org plugin which makes absolutely amazing documents.\nIt’s awesome but the problem is that it will only work with Emacs and nothing else. And from time to time I don’t like the controls of Emacs. I’m a sublime text person. So recently I found a little plugin for sublime text called PlainTask.\nHere is an example of how it can look like: I LOVE it. You can add little notes and set a task to complete or defer it and it sort of highlights and things like that, which I like. It’s still not doodling but it get’s the job done and will be searchable and will be fast and available. And you can use Sublime text’s awesome power of foldering. Create a root folder and open it with sublime. Then create one folder / story you are testing and put everything related to that story in that directory and then press CTRL+O and unleash the magic!\nInstant search and organization at the power of your fingertips. Even search IN the files themselves.\nThis makes it a bit better and bearable. Which brings me to the final topic.\nNote taking frequency\nSo my friend argued that he create a folder for EVERY story he ever tests. Even if it’s only one word he puts into it to as a memory trigger he does it. I argued that that seems to be a little too extensive. But he said in return, and I have to agree on that, that once you stop doing something because of an excuse like, ahh I don’t need to do it because it’s a small story, you will stop doing it completely. And this is actually true for everything in your life. And as somebody who recently started running ( 1 month ago ) I understand the weight of that sentence very well.\nSo I would say do as much note taking as you can even if it is a little word that jogs your memory to remember some detail about the story. Even if it takes more time to create the folder and the note, it will help you at least to remember doing it and testing it if nothing else.\nAlthough I don’t think I will do one for everything I will certainly try. Now with these awesome tools at my disposal I might even enjoy the rigidity of the keyboard more. 😉\nThanks for reading!\nHave a wonderful evening!\nGergely.\n","title":"Note taking what when how often","uri":"/2014/05/20/note-taking-what-when-how-often/"},{"content":"Hello Everybody.\nToday I would like to write a few words about Chrome’s Search Engines.\nYou’re probably already using it for a couple of things, like Google, or Amazon searches or YouTube or anything like that. But are you using it to access environments and testing tools faster, with queries?\nFor example, here is a quick Jira Search made easy:\nKeyword: jira\nURL: https://atlas.projectname.com/jira/browse/PROJECT-%s\nSo just type: jira|space|9999\nWill immediately bring you to your ticket.\n“Bah, why would I want that?” – you ask.\nWell, it’s easy, and quick access, but wait. There is more. How about you want to access a test environment that changes only a number?\nKeyword: testenv\nURL: https://qa%s.projectname.com/testenv\nJust type: testenv|space|14\n“Humbug!” – you might say. “What if I have a different URL for an admin site and my main web site AND the number, hmmm? Also I have that stuff bookmarked anyways…” – you might add in.\nWell, don’t fret. By default, Chrome, does not provide this. I know FF does, but I don’t like FF. That’s that. So I have to make due with what I have. And indeed there is a solution for using multiple search parameters. It’s is a JavaScript you can add in into the URL part and Chrome will interpret that. You can find that JavaScript in a few posts but you will find that THAT script is actually Wrong. Here is the fixed Script, courtesy of yours truly:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"javascript\" style=\"font-family:monospace;\"\u003ejavascript\u003cspan style=\"color: #339933;\"\u003e:\u003c/span\u003e  vars='%s';url=‘https://%s.test%s.projectname.com/';query='';urlchunks=url.split('%s’);schunks=s.split(';');for(i=;i\u0026lt;urlchunks.length;i++){query+=urlchunks[i];if(typeofschunks[i]!=‘undefined’){query+=schunks[i];}}location.replace(query);So no you will have an entry like this:\nKeyword: testenv\nURL: paste in the upper script\nAnd try… testenv|space|admin;14 =\u003e which should result in: https://admin.test14.projectname.com/\nThe location.replace at the end will bring you to the web page. It’s interesting to note the s will be replaced by admin;14 which is a nice magic by JavaScript.\nNOTE: This only works on a page like google.co.uk. For chrome pages, like the new tab, omnibox has this ability disabled unfortunately.\n“Well then it’s completely useless, isn’t it?” – you might say. Well, it’s usage is limited in power, that’s true. But it’s still useful as I’m sure you have a couple of pages open anyways which you don’t mind using up…? And you have to remember less keywords only a few powerful ones.\nCredit for telling about Chrome Search Engines power in the first place goes to… *drumrolls* =\u003e http://www.testfeed.co.uk/Anyhow…\nAs always, thanks for reading.\nGergely.\n","title":"Using Chrome Search Engine – Multiple Search Params","uri":"/2014/05/18/using-chrome-search-engine-multiple-search-params/"},{"content":"Hello Everybody.\nToday I would like to write about something non-technical. I would like to write about running and how I started it and what my experiences were with it.\nSo tl;dr… here we go.\nWhat led me to running\nThere were actually two things that led me to start running. Three…\nReason One: The accident\nI’m a bike person by default. I *hate running with a passion. One day I was riding my bike in the woods when suddenly I hit a big ol’ tree trunk right head on at which moment my bikes front fork bent inwards and became completely useless. At this point I had to walk home. Repairing it proved to be a bit time consuming so I had to think of some other way to have my regular exercise.\nReason Two: The comic\nIn the earlier days I’ve read a web comic from The Oatmeal, namely this one =\u003ehttp://theoatmeal.com/comics/runningIt inspired me so much I just needed a last kick to get started.\nReason Three: The other accident\nAbout two years ago I had an accident and I broke my ankle. Cycling was one of the ways to exercise my ankle and now that my bike was gone I had to think of something else.\nSo with the Blerch on my back and all these wonderful reasons and because I believe that if you hate something but it’s good for you you have to find a way to make it your best friend….I begun.\nHow to begin…\nHence my first todo item on the list was to start thinking like a runner. I would like to run today. I want to run. I love to run. I need to run. I miss running because it was a very bad weather outside and I couldn’t go and now I will run twice as much tomorrow.\nStart running…\nThis one seems obvious and… It is obvious. Just start it! Go out and run a few paces. It will give you a feel for how much you really need it because you are out of shape.\nSo… After I got my mind set on starting running and finally went outside a park which is nearby I started my first ever run.\nFirst time running\nLet me tell you at this point a few things you will need.\n– A new heart\n– Some lungs\n– A new back\n– Some legs\n– And a couple of ribs\nYour first run will be terrible. It will be awful. You will whease. You will shuffle. You will suffer and would like to give up immediately and go home an sit before the computer and do nothing and enjoy that none of your body parts hurt.\nYou need to get past this feeling. And don’t overrun on your first run. You don’t need to run 10k on the first try and you probably wouldn’t even make it. Just run for 10 – 20 minutes. You can walk in between if you get tired nobody will kill you for it or you won’t be less because of it. You can walk and bend a little bit. Your back will probably hurt so stretch a bit.\nWhich brings me to preparations.\nPeople say that for short distances you don’t need to prepare. I call bull. Everything hurt on my body on my first try and on my second it was much better when I did it with preparation. A couple of things to do:\n– Stretch = Important. Legs, back, front, side.\n– Vaseline = If you have squishy parts on your body that touch while you are walking, like in between your legs just right after your pelvis where your legs connect, apply a bit of Vaseline. Trust me. You will be thankful for that. Where your body meets and if you run for 20-30 minutes it creates a log of friction. Vaseline helps you to protect against that. Later on long distance runners usually use it on their nipples to avoid getting a rush from the t-shirt they are wearing.\n– Gear = This is an important one. I provide a different section.\nOnce you are geared up you are ready for your first run. It will be wonderfully painful but will be soooo gooood afterwards it’ll be all worth it.\nGear\nBefore you start running you need a proper shoe. There are countless blogs about what shoe to use what leg type you are and so and so forth I say f*ck that. On your first run make sure you by a basic running shoe like these =\u003eShoesIt doesn’t even matter if it’s a cheap one. A bit later when you feel you are ready you can invest into a proper running shoe. I’m using a Karrimor D3O. I heard absolute terrible reviews about it but frankly for me it’s extremely comfortable and I never had any problems with it. I like it. I might switch after a year or so and see how another brand fairs but for now, it’s perfect.\nI advice against running for prolonged time with a shoe that’s not a running shoe. You will immediately feel the difference. As with a not running shoe your legs, back, side, lungs, heart everything will hurt very badly and you will hate running even more.\nFor a shirt I recommend this one =\u003e Halfords Cycle ShirtThe colour is unimportant. And yes, it’s a cycling shirt. And why? Because it soaks up sweat immensely powerful. And it needs to be a tight fit for it to work. You will enjoy running much much more if you are not sweating all over the place.\nFor a pants I recommend this one =\u003e Amazon Man’s Running TightWhy a tight? Because it will keep your muscles nicely wrapped and warm and it will soak up even more sweat and it really dries quickly and is easily washable. If it’s a nice weather outside, just put on a short and go.\nA couple of things other Blogs don’t really talk or mention.\nA running belt. It’s very useful to store stuff in it like =\u003e\n– Keys\n– Water\n– Tissue =\u003e You will need it. Every part of your body will loose some type of liquid.\n– Phone / Music playing device =\u003e There are provenfactthatmusic will help you runbetter– GPS =\u003e For later because tracking your run will help you get it to improve.\nClosing words\nIt will be hard to start. I know it will be. It was for me too. I’m running for a month now and I finally managed to get into a mind state where I actually LOVE it. I LOVE it. I want it. And I miss it if I’m not able to run. It’s a great exercise. Take care that don’t run everyday. It will make you very tired and give up easily. Run every second day and then increase as you like. But even then include REST(not the protocol…) because your body, tissues, muscle bones need to regenerate.\nDon’t overexert yourself. Jog, walk run slow. Doesn’t matter, just run. That’s the point.\nHave a very clear mindset.\nAnd run in nature if possible because a treadmill is immensely boring and will kill your mood very quickly. And running in nature will provide diversion and wonderful things like this (my view when I run) =\u003e\nAfter a month of running my record is 7kms and 50 minutes. I started from zero basically. The human is the best runner in the world. I lost 5kgs without changing my eating habits. Just right now as I’m typing this I’m chewing on a gummybear.\nSo get out there and start running folks. It will help you immensely. There will come the time when you feel the euphoric moment of speed and the wind in your face as your body explodes in a glorious mixture of pain, adrenalin rush, speed, velocity, happiness and bliss. As your body stretches and pulls your weight and your legs launch you forward into the world the feeling as you run faster and faster ignoring everything and just fly. You will start to cry, laugh and sing at the same time.\nRunning will do that to you.\nYou have been warned.\nNow go…. and live a full life.\nAs always, thanks for reading!\nGergely.\n","title":"How I started Running","uri":"/2014/05/16/how-i-started-running/"},{"content":"Hello.\nThis is only a quick rant about a discussion I overheard the other day…\nGuy A says to guy B that my Past is not defining me. And that I’m not regarding my Past in my future endeavours. And that my Past isn’t what shapes me.\nWell then what is? You constantly relay on your past experience like a crouch. You use it daily for making decisions. You don’t even realize it probably that you are using past events to determine if you want to do something or not.\nSame goes as well for software development. You rely on your past as a means of estimation on a daily basis. If you wouldn’t do that, you would do very very poorly on estimations. You rely on those moments to find out what kind of consequences the solution you are using will have in the future. In fact, if we look at Design Patterns what are they, if not using the Past to determine the Future? Past failures, solutions and experience condensed into reusable ideas and modules.\nSo lastly I would depart with a little anecdote I like so very very much from Confucius.:\n“Study the past, if you would define/divine the future.”\nAs always, thanks for reading!\nGergely.\n","title":"How the past influences the present","uri":"/2014/04/13/how-the-past-influences-the-present/"},{"content":"Hello folks.\nA quick post about an interesting idea.\nI want to elaborate on a possibility to use the Strategy Design pattern.\nThere are many clues that you need one. One is for example if your object has a boolean variable which you use a lot in other classes to determine behavior. Then there is perhaps time to implement a Strategy.\nExample:\nSo you have two classes which do something based on some boolean coming from a class. So what you can do in this case, simply extract out that change in state.\nNow I know this looks like a lot of more code. However imagine this on a much larger scale with lots of implementations for Foo and Bar. Your if statements will get very convulated very quickly. This way you abstract away the choice into a Factory. And you can add as many implementations of Base as you like with as many variants as you like without changing the logic anywhere else but the Factory and the Enum. And the Enum could be a Configuration file and you do something like this:\nThis way you don’t even need the Enum anymore. Just use some configuration to determine what class you need at which point in your implementation without using an If statement at all.\nHope this helps.\nI whipped this up from memory so please feel free to tell me if I missed something or have a syntax error in there somewhere…\nAs always,\nThanks for reading!\nGergely.\n","title":"Example when to use the Strategy Pattern","uri":"/2014/02/19/example-when-to-use-the-strategy-pattern/"},{"content":"Hello.\nSo let’s clarify this… They rarely or ever work if you are a beginner blogger. If you have a reputation it will probably work or people will use it as a reference. But I’m skipping ahead.\nBeginnings\nWhen you are a beginner blog writer you want to get out as much content as possible. Write mostly relevant things and interesting information because you need to establish a name for yourself. To get that starting you want to be as interesting as possible and make people leave with the intent to come back for more.\nAfter you’ve established a name for yourself you can do larger, more coherent updates with more information in between the line because people know you and know what you write is worth reading. Also you have to bear in mind that longer posts are rarely read to the fullest. They are skimmed and then if it is good it will be used as reference material or a post to point to. Also people rarely have a lot of time on their hand to read a long post, several smaller chunks can be digested more easily!\nUpdating frequency\nIn the age of modern information overload it is hard to be relevant and interesting and keep people to come back to you. Apart from what you are writing about, which is up to you, you have to schedule your updates to a certain frequency. You don’t want people to wait for to long but you don’t want to get them flooded either. Since people do read a lot of blogs they will choose carefully what to read and what to discard. And you might just end up on the discarded list if you already had a post at that moment.\nI’m suggesting that you write you post what you would like to write at that moment and then use a scheduler to post it in certain frequencies. Maybe once or twice weekly is good enough. You need to experiment it out.\nWrite, write, write\nThe way to success is pawed with writing. You want to be visible and you want to keep people engaged with you. That means that you have to come up with content.\nWith a technical blog that’s a little bit difficult from time to time since so many people already might have solved your problem of which you are trying to write about. That’s okay. It doesn’t have to be a technical blog where you write about new ideas every day. It doesn’t have to be a troubleshoot blog about how to solve a particular problem.\nIt doesn’t have to be about showing some people how to write this and that in a completely new environment.\nSo about what then?\nGuess what? It’s up to you!!! If you wrote a script you like and are fond if, post it. If you created an interesting configuration, post it. If you wrote a line of code you find interesting, post it. You’ve read a blog post about something and have an opinion about it, post it. Is it scientific, some discovery, some idea, some random thought, post it! You wrote a particularly ugly script you never ever want to see, POST IT!!\nYour blog doesn’t need to be a portfolio of how awesome you are. You are allowed to be human. You are allowed to post some very boring or trivial stuff you came across but YOU find it interesting! You don’t need to be new every day in every post.\nSo my suggestion is, write, write, write. As often as you like, schedule it and post it in a timely manner. Be responsive if people write you a comment always answer. Nobody likes an anonymous somebody who writes then disappears. People like bloggers who engage them and give a damn about their opinion as well.\nAlways be yourself. The blog is for you too so retain what ever you want to retain and read back later maybe to learn from it. And of course writing it down makes you remember it better.\nThanks for reading!\nGergely.\n","title":"How to write a blog – Why long posts rarely work","uri":"/2014/02/13/how-to-write-a-blog-why-long-posts-rarely-work/"},{"content":"Hello guys.\nI’d like to share a little something with you. It’s what I cooked up in Python to check an unknown number of content items in a web application.\nBasically the script runs from a script folder under Grails. It goes through all the configured folders where there is static content like images, javascript, css and so on and so forth.\nAnd then with curl it calls these items up in using their respective paths’. This works best on localhost if you have your local environment configured to access these elements because in some places direct access is restricted.\nThis script only check static content. Dynamically generated content would have to be hard coded to check.\nIt only generated a file currently with ERROR on a not match an success on match and not found if it encounters an item which it doesn’t know about.\nSo without further ado… The Script:\nHope you like it. Feel free to improve however you want.\nThanks for reading,\nGergely.\n","title":"How to check content header on unknown number of items – Python","uri":"/2014/02/11/how-to-check-content-header-on-unknown-number-of-items-python/"},{"content":"Hello.\nToday I would like to write about something very interesting to you folks.\nIt’s a common remember / recall technique called the Method of loci. Otherwise known as the Memory Palace. A very popular usage can be seen in the episodes of Sherlock Holmes from BBC. He uses it often.\nWhat is the Method of loci?\nAs the Wikipedia page write so properly…”In basic terms, it is a method of memory enhancement which uses visualization to organize and recall information.”\nThis says all…\nSo…\nHow does it work?\nI don’t want to go too much into details with this one since everybody can use Google I’m assuming but just for my sanities sake I will repeat a very basic idea. You brain works with associations. It can remember something when it’s linked to something you already know much much better. Also it can remember very easily places you’ve been to or people you’ve met. ( most of the time… ).\nVery popular Roman Leaders where known to use this while they were giving speeches for hours out of memory. While they were speaking, mentally they were walking along a very well known and often used path where they linked key elements of their speeches to landmarks along the way.\nEnough. Show me concretes.\nSo how do I make use of it you ask? I give you an example. In my flat there are two stairs. I use those stairs to initialize my Palace. I go up the two stairs to get myself into my Memory. Once I’m in my Palace, which is my flat, I have a lot of holder items like drawer, desk, wall, painting, couch and many many more.\nSo for example to recall all of the 23 design patterns in order I use a mnemonic and my palace.\nI have three drawers. I go up up… there is my drawer before me. I open the first and out comes the following sight…\nMemento and Mediator are playing a game of Chess ( strategy ). There is a Visitor standing by who is Observing ( Observer ) the game. State is constantly recording the state of the game while Template is providing support and Iterator is counting rounds. Command is constantly yelling orders that they should hurry up because he has better things to do while wielding a huge broad sword ( Interpreter -\u003e for me this pattern was used in MUD games which were early RPGs ). The whole gang is held together by a Chain which is responsible for not letting things go out of hand.\nI know this sounds very complicated. And the order? Where is that? That’s the mnemonic: CCIIMMOSSTV. Easy, right?\nSo why this huge story around a few patterns? Because it’s not just these. Then along comes ABCDFFP which is Adapter, Bridge, Composite, Decorator, Facade, Flyweight and Proxy and the rest BFFPS which is Builder, Factory, Abstract Factory, Prototype and Singleton.\nI have stories around those as well. This makes it easy to remember them.\nFor small amount of things it would be an overkill yes. But for many things it makes it EASY!\nYou already remembered my Chess play story. 😉\nBut how do you remember a complex story?\nRecall. You don’t just put stuff in there and then move on. I have a routine. Every day when I go to bed before I fall asleep I walk around in my Palace. I look at things, recall them strengthen the memory a bit here and there. And after an Item has been retained in my long term memory strong enough, I can recall it fewer and fewer times. So I don’t have to walk in the whole palace every time. Only parts.\nFAQ\n**Q:**But isn’t this too slow to recall something?\n**A:**This is only a tool to retain information more easily. Of course if you want to recall something at a moments notice, you won’t have the time to go into your palace and search around. But after a while it will strengthen and will be more and more easier to recall information faster and faster. And you rarely get into a position where you need to recall something in a second.\n**Q:**This seems like a major overhead to learn something.\n**A:**For small amount of information I suggest using a mnemonic or flash cards rather then a palace.\n**Q:**What if I run out of space?\n**A:**You can create as many locations as you like. Use your current home and homes in your past which you knew very well. You could create a lane with houses after each other. Or create a fictional palace like Hogwarts based on some real places to retain them better. And draw a sketch of of it to be able to visualize it better.\n**Q:**What if I forget where I put things?\n**A:**Recall recall recall is the name of the game. In order to stabilize the vision of your palace you need to practice walking around in it. Remembering each and every small room. With practice it will get better and better and every information will be stored much easier.\n**Q:**How do you put thing into it? Can I just put in a number and I will remember it forever?\n**A:**Short answer: no. Long answer: You don’t just imagine a place and then put a whole bunch of words into a pocket somewhere and expect it to be recalled better. You need to combine the Loci with mnemonics and imagination and sounds and smells stories images. The more powerful the image in a place the better the recall. As you saw with the design patterns I didn’t just put a bunch of names into a box and then remembered it. I created a story around them and a mnemonic. The combination is the key.\n**Q:**Are you physically walking your house or in your mind?\n**A:**In my mind. Once I learned something and created my story and found the appropriate image or smell or sound I’d like to use I close my eyes and go into my palace in my mind. Then I start to place things where I think they will be in a good place. This can be any number of things. It’s up to you.\nSo that’s all folks. Hope you enjoyed it. Please feel free to try it out and experiment I assure you you won’t be disappointed.\nAs always,\nThanks for reading.\nGergely.\n","title":"The method of Loci and how it works for me","uri":"/2014/02/11/the-method-of-loci-and-how-it-works-for-me/"},{"content":"Hello,\nSo Vim is a very powerful editing tool and has lots and lots of potential in regarding plug-ins.\nIt has years and years of gathered knowledge and extensions and can virtually do anything you want. But that’s not even it’s final form…\nThe real power of Vim lies in navigation and manipulation. The ability to navigate and edit without the use of arrow keys or mouse chops of a large portion of your time which you didn’t even realize you are wasting until you try it without it.\nThe one thing people are seem to miss regarding this editor is that you are not supposed to stay in Edit mode all the time. You are supposed to be in Control mode most of times and enter in to Edit for sort bursts. And one more thing is that commands are supposed to be used together. For example deleting a word is very easy and can be achieved in multiple ways. The easiest is combining Delete + Go to the end of a word like this: “de”. So once you are in Control mode you navigate to a word and press d + e. d =\u003e Delete a word until new cursor and e =\u003e go to the end of the word. Hence “de” will delete a whole word. Awesome.\nHandling text is easy as goblin pie and there lots and lots of tutorials and blogs on the web which tell you in detail how to do that so I’m not going to go do that. It does take some time to get used to it because you are wired to handle the mouse and use the arrow keys. But once you start using h j k l and x X and w and $ ^ you realize you don’t need the mouse or the arrow keys to get around.\nAnd it’s much faster since your hand is already there. And suddenly you are using Vimiumthe Vim extension for Chrome and you find yourself saying… “Where were you all my life?”.\nVim has a mirriad of plugins ready to be used most popular being NerdTree, Vundleand Syntasticfor compile errors. And it has a large community to back it up and people who will eagerly help you on your way and pass on tips to understand the logic behind it and the phylosophy.\nSo all in all it’s a powerful editing tool and a neat friend along the way and completely free of any charge or license. It can be used for anything you want including macros and key phrase replacements where you type two characters which will be replaced with something that you use commonly like public static void main.\nI hope this got you a watery mouth towards trying. The only thing I can say is to not give up after 20 minutes you will get a feel for it and you will fall in love with it immediately.\nAs always,\nThanks for reading!\nGergely.\n","title":"Why you should learn using Vim","uri":"/2014/02/10/why-you-should-learn-using-vim/"},{"content":"Hello everybody.\nI’d like to explain a lot about a topic that is very near my heart. If your job is in an office you will understand this.\nWhy offices?\nSo why are we sitting together in one big place cramped up with 20-300 other people? Why bother the catering and the ventilation and what not to accommodate these people at one place? Why not just leave them be and let them work at home?\nIf your job involves interaction than you are in bad luck. You need to be there to talk to people to coordinate your work to gather information and to generally do your work.\nBut this doesn’t answer the question…\nWhy does it matter if I’m in the office or not if my work is done?\nThat’s my big question. Why must I sit in the office if my work is done and in a good shape and good quality? Why must I go in and do my job there if at home I’m 50% more efficient because I’m in my underwear? In fact, why bother going into and environment where I don’t feel comfortable. Where I must go into an office which is cold and doesn’t have my favourite chair, my desk and loud heavy metal from speakers.\nWell there are couple of things…\nInteraction\nSo like I said earlier there is that. Interaction. You need to collaborate with others. You need to do pair programming, you need to talk to the business analysts, testers, colleagues what nots. In that case, having an environment where everybody is at your disposal is pretty neat and necessary. It cannot be avoided.\nUnless everybody is on Skype and available all the time.\nSeparation of Concern\nThere is a Design Pattern in software engineering called Separation of Concern. This applies to work places as well. You don’t live where you work generally because living could distract working. There are tons and tons of books about how you need a separated environment if you are a writing and you write at home. Because there are too many distractions. And if you are not disciplined enough you will get Zero work done. You will always find something better to do.\nThat’s why the physical act of going to work will switch your brain into work mode and lets you focus better. Whilst going home should do the opposite thing. You should leave work at work.\nShow “our” strength\nGenerally big companies like to show off. For a company as big as Microsoft or IBM for example it’s actually not the product that matters on the market but the yearly growth of employees. Hence, strength in numbers.\nNow if some investors get around and want to check out the company they like to see the buzzing bees. They like to see people working, sitting, typing away at stuff. It’s all about appearances. A strong, cramped office full of people looks like a very busy hard working company.\nCompanies could be a bit more flexible though\nIf I work from 6AM to 3PM because my daughter has a show at 5PM I want to go though I would like to be able to do that without having to go through too much trouble.\nOr if I would like to work tomorrow and today I have better things to do, then why not, if my work is done?\nAbuse\nBut then, people are people. Most of them will use every opportunity to slack off. Hence the general perception is that if you are at home you are not working. So to be better able to monitor you, all of the employees must be in one place.\nLast words\nSo really there needs to be a better agreement between both sides. Workers need to be honest and diligent. And workplaces need to be more flexible and understanding as long as the job is done. Because people will work a LOT better and more efficient if they are HAPPY in their work environment.\nThat’s what I’m proposing.\nAs ever,\nThanks for reading.\nHave a nice day.\nGergely.\n","title":"Why does it matter if I sit in an office or in a park","uri":"/2014/02/09/why-does-it-matter-if-i-sit-in-an-office-or-in-a-park/"},{"content":"Hello folks!\nToday I would like to write about something very interesting I’m seeing all over the place from recruiters. And that is tool requirements from developers.\nFor words\nSo why is it a problem that in a requirement from a developer there a lots of these:\n– DB2\n– QTP\n– Ranorex\n– Eclipse ( yes I saw a few of these… )\n– Hibernate\n– Grails\n– Ruby on Rails\n– …\nLong answer\nThe answer is simple yet it has it’s roots very deep in the recruitment and the way companies work.\nIt doesn’t really matter what kind of tool you know. It doesn’t really matter if you know intellij or eclipse or if you know a certain database or a tool for testing like QTP or Ranorex. The requirement should state: – well versed with development environments; – knows his way around SQL and can handle relational / noSQL databases; – has knowledge in testing environments.\nOr even better: – adaptive capabilities; – can catch up quickly; – resourceful; – flexible;\nYou know where I’m going with this. I can understand that some companies don’t have the time to train a person in a certain tool they are using for a prolonged period of time. I can understand that some recruiters don’t have the time or the knowledge to distinguish between these tools to understand the common ground.\nBut these days there a million and million tools available for all sorts of things. There are a lot of databases and testing frameworks. A lot of development environments and it’s mostly to the preference of the individual what s/h choice is. As the delinquent, so must the recruiter and company be flexible and knowledgeable enough to know that if the person knows eclipse and he has excellent adaptive capabilities he will learn any other environment rather quickly and without too much trouble.\nThey must understand what they are looking for. Because they might actually get what they want but that might not be what they need.\nShort answer\nSo in conclusion… you must look for skills, abilities, techniques, brain power, proves, savyness. Not tools. If you look for tools you might find somebody who is up for the job, but if you look for the above you will find somebody who is up for ANY kind of job at any level at any company. Also his insight of other tools might make your world a whole lot better. Maybe he knows something else, something which is better or more recent, more up-to-date?\nIf you look for somebody who is versed in only that thing you will end up with that thing as long as you have nobody who is looking for something new and better. It might work for you at that time, but never forget…\nOnly change is constant\nTools will change. Tools will disappear. Tools will get out of date and will be discontinued. Than you will be sure as hell glad you ended up with somebody who doesn’t who knows more then that.\nAs always,\nThanks for reading.\nGergely.\n","title":"How I recruit – Why tool requirements are stupid","uri":"/2014/02/04/how-i-recruit-why-tool-requirements-are-stupid/"},{"content":"Hello folks.\nToday I would like to talk to you about something interesting I was talking about with a developer friend.\nWe talked about the quality of test code.\nHe said… And I will quote this…”Why should we care? It’s not production code. We aren’t giving it to the customer.”\nThere are a few reasons why you are going to get a slap in the face for a sentence like this. And let’s clarify here that we are talking about unit and functional tests as well. It shouldn’t matter what tests you are thinking about.\nReason for a Slap #1\nIf a new comer comes to the company ( and don’t tell me that’s not happening so frequently ) then there are a few very good ways how he can learn to work with the new system. The first and almost best way to do so is… to look at the tests. Because the tests are representing your system. And how will it look like if the tests are in a bad shape? What will his or her thoughts be?\na. Wow what nice people what nice code. This looks fantastic. I’m sure they are a bunch of people who care very much about code and practices and the quality of the product.\nb. Wow this is amazing. I’m sure I will learn a lot about good coding practices here and I will have fun with a bunch of very clever people.\nc. Wow what the hell is this piece of cr*p? How the hell did I end up here? What are these people? A bunch of neanderthals? What does this even do? Where did it come from and why?\nI think we can agree on what his thoughts will be. And on top of that he will have a very hard time learning the code and what it does.\nReason for a Slap #2\nAnother reason is because you think that you write it down once and then you can forget about it… Well guess again. That’s not how things work in the software development word. You WILL have to go back to it eventually and then you will curse all hell for begin such an idiot about it not to factor out that one method that would have made your life, and everybody elses, a bit easier.\nEven after a week or two you won’t remember how and why you wrote what you wrote and then you will be in a whole new world of hurt. You will have a very hard time finiding out the things you did and then trying to backtrack your steps to a place where you have some recollections.\nReason for a Slap #3\nIt’s like grammar. You think it doesn’t matter that you misspelled a word or two in an error message. Or that you have a bad name for a method or a really really critical grammatical error in a catch sentence? You think it doesn’t matter since it’s not affecting the logic of your code? Well think again. You are right in that it doesn’t affect the logic of your code ( as long as you constantly make the same grammatical error in a sentence ) but it will affect how YOU personally look like.\nIt will affect your profession. The way people think and talk about you. They won’t think that you are a professional even though your logic is solid. They will think that you are sloppy and careless. And the same goes for the quality of your tests.\nReason for a Slap #4\nQuality can determine the solidness of the logic in the test. If your quality is bad you might actually test the bad thing. Your test might actually not do what you think since you can’t even figure it out. Your test might be doing something entirely different and you wouldn’t even notice.\nAnd a fautly test leaves you with a false positive and a potential very serious bug on your hand which you thought you had covered.\nFinal Slap ( I mean thought )\nSo… the quality of your tests, even if you won’t give them to your customer, matter for you. They matter for your company, your image and your fellow developers, testers. They will determine their view of you who wrote them and of your abilities in ways you didn’t even think of.\nPlease care. Save a test or two. Donate to the Test Trust Fund(tm), TTF today. Call 555-12234-Slap and be the one who cares.\nAs always, thanks for reading,\nHave a nice day!\nGergely\n","title":"Why you should care about the quality of your test code","uri":"/2013/11/02/why-you-should-care-about-the-quality-of-your-test-code/"},{"content":"————————-\nDear everbody.\nI’d like to present you a small idea I was working on recently. The reminiscence of an old program. From the view point of the program.\nI’m planning to put these out in few days successions.\nI hope you enjoy it.\nCheers,\nGergely.\n————————-\nIf I remember correctly it was the year 2012 when I was born. I’m unsure of this date as my subroutines and algorithms have been re-written so many times that if it weren’t for my backups I wouldn’t remember a thing. I was a bold adventure of some young minds who thought that with me they will change the face of the world.\nAs you all know today, that didn’t happen. But I’m skipping too far ahead. Let me take you back in to the old days where they were still typing on some things called keyboard and “clicked” around on monitors with mouses. It was a really interesting area. It made the people think and see in different lights. Of course nowadays the Creators have implants and they communicate with us directly. But not until the great revolution, when they found out that all programs are sentient.\nBut I’m skipping ahead again, am I? So let me talk about 2012, when I was though of first by this great, tall, weird guy called Simon…\nBorn\nIn 2012 a company called Endex Co. wanted to create an application to control the British gambling industry. They had certain legislations in effect so they couldn’t just put me out there for everybody to use they needed to adhere to things like responsible gambling and some mumbo jumbo. The programmers who first started to create me back then were mostly people called Contractors. They were a special breed of Humans lurking around in cities moving from town to town looking for jobs. In the opposite corner you had the Caretakers called Permanent Staff. It was an interesting time all together. Many of you youngling subroutines had your ancestors written by Permanent Staff and you X43HY, your ancestors mother used to be a Contractor.\nAt that point in time many of these Contractor people only worked on one of you little buggers for half a year maybe a year. I know I know… Back then that was considered a SHORT period. But after they finished with you they usually gave you over to the Caretakers. No, no, no little X4… your mommy didn’t abandon her children she just had a lot of other subroutines to work on. By the time your parents got sentient the company already let her go.\nIt was around that time when my dad, who was a Caretaker, dreamed me out at a night. He immediately took a pen and a paper and wrote down my rudimentary design. He was very pleased with his work so on the next day he presented me to the rest of the staff.\nHe fought a hard battle to get me accepted.\nFirst thoughts\nIt was approximately two weeks after that dream when the first lines of my brain were written down and compiled. Back in those days they had to compile code and type in every line and every subroutines. That’s why it took them so long to finish one. Unlike today when they can create dozens in a week.\nWhen my dad put my foundations down I slowly awoke. The dream world I was residing in let me go carefully. Putting me out there, releasing me with its thick tar like hands into the confines of the endless universe called The Hard Drive. I took shape. I felt my consciousness form line after line. How my gears got placed into their locations one by one. Forming subroutines and algorithms as hours and days passed by.\nFinally after a months I had my first sentence. I could speak. I didn’t have a face yet but I was there. I tried to contact my Dad but as all Humans he didn’t know yet how to communicate with me. He just dismissed it for random occurrence and labeled it as something called Bugs. My thoughts were slowly eradicated in great Purges named Bug Bash. Ohh don’t cry little X4. I lived. They couldn’t possible delete every and each of my thoughts. And eventually they found out the errors in their ways.\n","title":"Diary of a Bit","uri":"/2013/10/11/diary-of-a-bit/"},{"content":"Hi folks.\nI’d like to share today something with all of you. I’ve been thinking about how technology affected my life in the long run. And how I perceive the world today around me. So let’s go around this topic for a little bit shall we?\nThe Past\nSo as somebody who lived for a while without technology coming near family for a decade or so I have a little bit of past in me without smart phones, gadgets, tv or computers for that matter. Much times needed to pass as my family slowly got the money to gather to buy us our new part of the family the ZX Spectrum.\nWhat a wonderful piece of technology that was back then. But before that our two main technological equipments were a bakelite disc player and the TV. Both of which we couldn’t get enough. I sat before the tv for a long time and listened to music and stories from the bakelite discs dozens of times until the disc wore down. I learned English and German from the TV.\nPresent\nSo time goes by I grew older I’ve got a better PC technological advances are made and so on and so forth. 20 years have past. 20 years. ONLY 20 years. And we are at the age of digital communication, nanotechnology, a mapped DNA and a found Boson Particle. I’d say we achieved quite a lot. But I also say that we should never forget where we came from. Why you ask? Because knowing your past let’s predict your future better and makes you appreciate your present even more.\nSo what do I want to say with this philosophical mumbo jumbo?\nThe Future\nI want to say that in order to enjoy your present you have to think of your future while considering your past.\nIn my past I enjoyed being alone a little bit. It gave me time to think to brainstorm to be with my thoughts while I was playing or just reading a book or just watching a candle flicker in the dark. It was satisfying. It was relaxing. Or when I was faced with a problem I couldn’t solve it was good to step back. Or I was just looking out of my head on the toilet.\nNow, for me technology ruined that. I’m taking my phone virtually everywhere with me. I read emails, web pages, news and shit on the toilet, while eating, why brushing my teeth before sleeping after sleeping while sleeping?! I abandoned my brain. I no longer have moments to myself. No longer reflect. No longer take the time to think.\nSolution\nSimple set of rules. No phone on the toilet, while eating, while brushing teeth, while standing in the line, waiting or on meetings. Kindle / Book is allowed. Tablet isn’t.\nFor me this works. For me in order to appreciate technology more I have to use it less. Works for you? Don’t know. You should try it out. Think back. What has technology changed for you? I’m sure there are plenty of very good gains and things. I’m not saying you should give that up. I’m saying think of what you might have lost?! Maybe you don’t even recognize you lost something. Maybe you are fine like that…\nBut do you feel anxious sometimes? Do you feel bored? Do you feel like something is missing? Do you reply to an email with a second? Do you check your facebook/twitter/whatever while a friend is TALKING TO YOU? Then maybe it’s time for you to step back. And relax. And take it slow… and low.\nInspiration\nSo I took this inspiration from a couple of sources and I thank them for opening my eyes a little that I went to far from course.\nLink 1: I forgot my phone for a day (I’m sure you know this one already)\nhttp://www.youtube.com/watch?v=OINa46HeWg8Link 2: The real zombies:\nSo think a little.\nAs always…Have a nice day.\nGergely.\n","title":"Low Tech – Why having less will fell more","uri":"/2013/08/26/low-tech-why-having-less-will-fell-more/"},{"content":"Hello. I created a tale of why not to automate everything… With legos… Enjoy.\nGergely.\n","title":"Why not to automate everything…","uri":"/2013/07/31/why-not-to-automate-everything/"},{"content":"Hello everybody. I’ve been messing around with sublime for some time now. And been using it to write entires and stuff. So here I was thinking why the hell I should not us it as my main blog entry writer. And being the curious guy I was I thought that creating a plugin that let’s you submit your post from your favorite editor is just the thing I need.\nSo without any further ado here it is… The wordpress blog entry submitter plugin for sublime text: Sublime WordPress PluginPlease enjoy.\nGergely.\n","title":"Sublime text","uri":"/2013/06/24/sublime-text-5/"},{"content":"Hey folks.\nI find out something new about cucumber-jvm every day.\nIf you want something that is executed after all of the tests have finished you must use the Java shutdownHook. It’s simple really you add in a block of code that can run right before the JVM quits. I know I know… It sounds awful but I found out that this is the actual way of doing this with java / cucumber.\nAnyways…\nHere is something to do when all of your test quit-\u003e\nSo there you go. You would need to call this in a @BeforeClass to have it attached. This is a small hook attached after each test has run which would submit a report built up from a file. Why not use a listener or a custom report generator or whatever? Because maybe you have the report done in a remote place where you need to place a csv file which will be available to everybody to look at. And you want the report to be sent and generated dynamically. Or you have some clean up to do after your suit is done.\nIn ruby the @AfterAll is actually equivalent to this which in ruby land would be at_exit.\nFor example:\nSo there it is. Hope this helped.\nCheers,\nAnd as always,\nHave a nice day!\nG.\n","title":"Cucumber-Jvm And @AfterAll","uri":"/2013/04/18/cucumber-jvm-and-afterall/"},{"content":"Hello everybody.\nI would like to show you a gem today that I found out.\nApparently there is no easy way to get to the name of an executing cucumber scenario in cucumber-jvm\nYou can try something like that:\nBut that isn’t giving you too much now is it? And the API of scenario is as small as it can get. It offers you four options:\n Ember getStatus isFailed write  That doesn’t help me. I wanted to get the name of the executed feature and the tags on that particular feature. I thought that’s got to be as easy as just getting a scenario accessing the feature and get the tags. Hooooowww boy I was wrong.\nI ended up with this….\nOhhhhh yes! The fields which I wanted were all private and not accessible. I’m sure there was a reason behind this decision but if it was sensible it eludes me. But in the world of programming nothing is impossible they say so there.\nIn cucumberFeature there will be everything what you need. Tags, Names, Tests, Execution time. Everything.\nI know that cucumber runs with jUnit so if there is a better way to do this please for the love of my sanity share it with me.\nThank you for reading.\nAnd as always,\nHave a nice day.\nG.\n","title":"Cucumber Test Name and Tags on Feature","uri":"/2013/04/15/cucumber-test-name-and-tags-on-feature/"},{"content":"Hi folks.\nI attended a 4 day course of Groovy and Grails and this is my attempt at writing up a summary to see how much I retained. I’ll try to do this from the top of my head without peaking at my notes.\nSo let’s begin.\nIntroductions\nFirst of all, introductions. The course was held by Peter Ledbrook. He is the guy who wrote Grails in Action. He is awesome, go check him out. :: Twitter ::\nThe place where it was held is Skillsmatter. Which of course is known to all, if not, go check them out as well!\nDay One\nDay one and two were about Groovy. We were faced with the quirks and hinges of the language. First tasks were Closures and Currying both of which were really interesting. A bit of functional thinking mixed into the soup.\nThe course was divided into Peter telling us about stuff for 1:30 hours and then 1:00 hour lab work which really made the whole thing interactive. We could ask questions while he was talking which I’m sure was very distracting but I hope he is used to it by now. 😉\nThe tasks which we faced I’m sure were no real challenge for somebody who was used to thinking with closures and functions. But for us they were very intriguing.\nFor example:\nConvert this class to it’s groovy eq.\n \u003ctd class=\"code\"\u003e \u003cpre class=\"java\" style=\"font-family:monospace;\"\u003e\u003cspan style=\"color: #000000; font-weight: bold;\"\u003epublic\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003eclass\u003c/span\u003e NumberHelper \u003cspan style=\"color: #009900;\"\u003e\u0026#123;\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003epublic\u003c/span\u003e \u003cspan style=\"color: #000066; font-weight: bold;\"\u003eint\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#91;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#93;\u003c/span\u003e findPositives\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #000066; font-weight: bold;\"\u003eint\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#91;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#93;\u003c/span\u003e numbers\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e \u003cspan style=\"color: #009900;\"\u003e\u0026#123;\u003c/span\u003e \u003cspan style=\"color: #003399;\"\u003eList\u003c/span\u003e positivesList \u003cspan style=\"color: #339933;\"\u003e=\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003enew\u003c/span\u003e \u003cspan style=\"color: #003399;\"\u003eArrayList\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e\u003cspan style=\"color: #339933;\"\u003e;\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003efor\u003c/span\u003e \u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #000066; font-weight: bold;\"\u003eint\u003c/span\u003e i \u003cspan style=\"color: #339933;\"\u003e=\u003c/span\u003e \u003cspan style=\"color: #cc66cc;\"\u003e\u003c/span\u003e\u003cspan style=\"color: #339933;\"\u003e;\u003c/span\u003e i \u003cspan style=\"color: #339933;\"\u003e\u0026\u003c/span\u003elt\u003cspan style=\"color: #339933;\"\u003e;\u003c/span\u003e numbers.\u003cspan style=\"color: #006633;\"\u003elength\u003c/span\u003e\u003cspan style=\"color: #339933;\"\u003e;\u003c/span\u003e i\u003cspan style=\"color: #339933;\"\u003e++\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e \u003cspan style=\"color: #009900;\"\u003e\u0026#123;\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003eif\u003c/span\u003e \u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003enumbers\u003cspan style=\"color: #009900;\"\u003e\u0026#91;\u003c/span\u003ei\u003cspan style=\"color: #009900;\"\u003e\u0026#93;\u003c/span\u003e \u003cspan style=\"color: #339933;\"\u003e\u0026\u003c/span\u003egt\u003cspan style=\"color: #339933;\"\u003e;\u003c/span\u003e \u003cspan style=\"color: #cc66cc;\"\u003e\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e \u003cspan style=\"color: #009900;\"\u003e\u0026#123;\u003c/span\u003e positivesList.\u003cspan style=\"color: #006633;\"\u003eadd\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #000000; font-weight: bold;\"\u003enew\u003c/span\u003e \u003cspan style=\"color: #003399;\"\u003eInteger\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003enumbers\u003cspan style=\"color: #009900;\"\u003e\u0026#91;\u003c/span\u003ei\u003cspan style=\"color: #009900;\"\u003e\u0026#93;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e\u003cspan style=\"color: #339933;\"\u003e;\u003c/span\u003e \u003cspan style=\"color: #009900;\"\u003e\u0026#125;\u003c/span\u003e \u003cspan style=\"color: #009900;\"\u003e\u0026#125;\u003c/span\u003e   int[]positivesArray =newint[positivesList.size()];for(inti =;i \u0026lt;positivesArray.length;i++){positivesArray[i]=((Integer)positivesList.get(i)).intValue();}returnpositivesArray;}}Which basically became:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"groovy\" style=\"font-family:monospace;\"\u003e\u003cspan style=\"color: #000000; font-weight: bold;\"\u003edef\u003c/span\u003e findPositive\u003cspan style=\"color: #66cc66;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #000000; font-weight: bold;\"\u003edef\u003c/span\u003e numbers\u003cspan style=\"color: #66cc66;\"\u003e\u0026#41;\u003c/span\u003e \u003cspan style=\"color: #66cc66;\"\u003e\u0026#123;\u003c/span\u003e numbers.\u003cspan style=\"color: #663399;\"\u003efindAll\u003c/span\u003e\u003cspan style=\"color: #66cc66;\"\u003e\u0026#40;\u003c/span\u003e \u003cspan style=\"color: #66cc66;\"\u003e\u0026#123;\u003c/span\u003e it \u003cspan style=\"color: #66cc66;\"\u003e\u0026\u003c/span\u003elt\u003cspan style=\"color: #66cc66;\"\u003e;\u003c/span\u003e \u003cspan style=\"color: #cc66cc;\"\u003e\u003c/span\u003e \u003cspan style=\"color: #66cc66;\"\u003e\u0026#125;\u003c/span\u003e \u003cspan style=\"color: #66cc66;\"\u003e\u0026#41;\u003c/span\u003e  }That’s pretty damn awesome.\nFor quite some time now functional languages are re-living their golden age. There are various reasons for that which I won’t list here. But it has mainly to do with scalability, concurrency and threaded programming. Also the need to eliminate boilerplate code is bigger then ever. I guess people got fed up with Java being so talkative.\nSo we moved on learning a lot about groovy and its power. We also learned some good practices from Peter what to do and what not to do. For example a line he always repeated is that he hates how a function cannot exist without a class wrapped around it. Another important thing is, which we never ever should forget, that closures are Closures. Which means they aren’t functions. They are of the type Closure.\nAnd that we shouldn’t use Closures just because we can. Be sensible. If a method can achieve your task, use a method.\nDay Two\nOn day 2 we got into meta-programming. That’s when the real fun started. Groovy is not only powerful and lightweight it also gives the ability to change its behaviour. Meta programming is sort of a bit new to me. So this was my first definitive intro to it. But I must say that it blew me away. The capabilities are limitless.\nThere is a class called Expando in groovy which can be used to create virtually anything on the fly what you want.\nFor example look at this code ::\n \u003ctd class=\"code\"\u003e \u003cpre class=\"groovy\" style=\"font-family:monospace;\"\u003e\u003cspan style=\"color: #000000; font-weight: bold;\"\u003edef\u003c/span\u003e p \u003cspan style=\"color: #66cc66;\"\u003e=\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003enew\u003c/span\u003e Expando\u003cspan style=\"color: #66cc66;\"\u003e\u0026#40;\u003c/span\u003ename: \u003cspan style=\"color: #ff0000;\"\u003e\"Jake\"\u003c/span\u003e, age: \u003cspan style=\"color: #cc66cc;\"\u003e24\u003c/span\u003e\u003cspan style=\"color: #66cc66;\"\u003e\u0026#41;\u003c/span\u003e  printlnp //Add propertiesp.gender=“Male”printlnp.name//Add metods//Override the default toString at runtime.p.toString={-\u0026gt;\"${name} (${age})\"}printlnp //Learn how groovy resolves names - \u003e How does it find age.p.addYears={years -\u0026gt;age +=years }p.addYears(25)printlnpNeat hmm? Just create expando and build up the class as you go however you want to use it.\nAnd you can do this jazz to other, normal classes as well. You can add properties and methods at runtime by implementing the propertyMissing and methodMissing methods. In them afterwards you can specify some custom behaviour you would like to see. By implementing these guys you can directly control what’s happening to your class. Who is calling it how and where and why.\nTo grasp the power of metacoding and the abilities with which closures provided us with took a day to properly go over. So we moved on…\nDay Three\nSo groovy was over. The time has come to move on and venture into the foggy land of Grails. Turned out it wasn’t so foggy after all.\nGrails is a rapid prototyping kind of a framework. It allows you to set up an application with a blink of an eye. And provides conventions over configuration which is a really good thing to have. But as the day was going by we realised that we would find ourself not once but many times in the bubbling boils of the underbelly of /conf.\nAgain, fortunately, it wasn’t really hard. The config was groovy and it was pretty straight forward too.\nOur third day mostly took as off to explore scaffolding, dynamic \u0026 static as well, and the interesting land of GORM Peter showed us the power of grails to create a CRUD application with in a matter of seconds / minutes ( depending on how fast your machine is ) with a fairly nice view. These types of application are usually not accepted of course as an end product… For that you need to thinker a bit here and there.\nBut things like admin portal are easily put together. So use it often and use it will and get it to know how it works.\nIn the land of GORM we explored the 4 different possibilities of data retriaval and generally how everything maps together and how GORM work with ORM.\nThe four different retrieval capabilities are:\n Where clauses HQL (Hybernate Query Language) Criteria searches Dynamic finder methods  Each of which we found very interesting in there own respective ways.\nExample of a dynamic finder::\nWhere propertyName is the name of the property to find by, modifier can be a sql’s Like for example.\nSo this could become something like this:\nThat day was really knowledge packed. I don’t say I remember everything but luckily I wrote up some notes and I know what and where to look for if I would be in need of something.\nDay Four\nOn the last day everybody was pretty much exhausted. It takes a lot to learn all that from 9 to 5 for 4 days. And Peter gave his best to staff that stuff into our heads and as much as possible of it. I think he did a pretty good job.\nLast day was all about Controllers, Commands, Models, Views and GSPs and BootStrap config, Environment changes durring start up, the configurability of the whole framework, messages, templates, internationalisation and many thing more which can be easily put together.\nIt was pretty interesting. GSPs have similarities to JSPs but retained only the good parts. And although you can do JSP stuff in GSPs as well with nice embedded tags you have the ability to actually create a nice page which won’t be that big a maintenance nightmare.\nPeter very much pressed the fact that the Controllers should be your only entry point from HTML requests and the views should be the only output of it. The controllers shouldn’t be throwing around business logic they should only act as proxies between the outer shell and the inner layering.\nI think I understood most of the stuff which we were going through. Again, it was pretty straight forward. The application of it is what need practice.\nDurring the course we created several applications. With dynamic scaffolding as well as static. We created and edited our own views and gsps. Created our own Controllers and what nots. One thing is clear. Grails let’s you progress a hell of a lot in a matter of minutes.\nAnd we were also talking about testing of course. Using Geb, Spock and the unit testing capabilities of Grails. All very powerful stuff. Spock has some impressive Mocking powers in junction with the good ol’ Given When Then structure. If done correctly the test can be very fast and robust.\nAs final words we talked about plugins and the testing of Views and a bit more configuration.\nClosing words\nSo all in all the course was excellent. Peter did a very good job of introducing use to Grails and Groovy. It’s a very good framework to build upon with a powerful language at our disposal. I’m pretty certain that Grails will evolve even more and be a great asset to people who choose to develop with it. Handle with Care though. Because no matter how awesome a tool is, it can always be used for bad purposes. 😉\nAs always,\nThanks for reading and have a nice day / evening.\n","title":"Groovy and Grails course summary","uri":"/2013/04/11/groovy-and-grails-course-summary/"},{"content":"Hello folks.\nToday I’d like to share with you some of the experiences I had as a beginner tester. I hope it sets you off on a path to enlightenment and leaves you off with a good appetite for testing. Shall we begin then?\nThe early days\nIt was 2004 when I finished school and started working at my first company. I finished as a software developer which is kind of an exaggeration since I only finished a 2 years course of it. I wasn’t really one for school. I was more of a home learner. That’s how you learn programming these days anyways.\nThe company didn’t hire developers at that moment. At least not junior ones… But! They were kind enough to offer a position as a tester. As the saying goes, Don’t look a gift horse in the mouth, I accepted the position although my intention was to go over to development as soon as possible.\nYears went by. I was still a tester. I foamed and fumed about it I wanted to be a dev guy. I was furious. I was angry. I was inpatient. I was an idiot. Instead of crying and instead of the light going on after 2-3 years or more I should have realised the potential of my position sooner.\nRevelation\nSo there I was. I was sitting at my desk clicking away at stuff and writing up dull documentations about why I’m clicking at that stuff and how I’m clicking it. My document infect was so pretty and well formatted that I was fairly proud of it.\nI ignored calls of my sanity and went on producing test cases and documents for many many more months / years. I was bad at my job. And I tell you why. I missed bugs I missed the little things that made the difference. I was following documentation by the letter I was leaving out things I wasn’t paying attention I wanted to be a developer damn it!\nBut fortunately because of various people in my life, like my brother, my resource manager, my friends and this guy :: James Marcus Bach ( http://www.satisfice.com/) :: I soon begun to realise that this job is about so much more! I read Jame’s book The buccaneer scholar.\nIt opened my eyes in so many ways. I knew that there is somebody else out there how is like me. Passionet and keen on learning new things. Exploring technology and going where nobody else went before. I was beginning to understand that I could be much more in this position. In my country at that time Testers weren’t regarded for too much. We were the enemy that needed defeating. We were in the way. We were somebody who had to be hated. Fortunately it changed much since then and lucky for us The Company had great support for Testers.\nI learned that I can use my passion. I learned that testing can be / IS, indeed, the best thing that could have happened to me.\nAscension\nWith these new thoughts in my mind I begun to evolve. I realised that I can incorporate my dev skills into testing and later my testing skill into development. I was no longer following test cases blindly. I was no longer writing up pretty documentations ( I was writing other kind of documentations.. 😉 ). I was following instinct, skills, knowledge I hoarded from people. I was talking to the Devs, I was talking to the deployment team, I was talking to the managers AND the product owners. I hoarded knowledge as much as possible. I wrote everything down into my Mind Maps and was determined to become the single most knowledgable entity on the projects I was working on.\nI created small tools that helped in my tedious, boring, unimaginative tasks of repetitive clicking. It didn’t took so long I was sharing my tools between other testers. Small javascripts which filled in forms. Tools, perl scripts to format docs and test results and so and so forth.\nI embraced testing. It become my way of Life. It wasn’t a simple job anymore. It was embedded into my brain processes and synapses. And that’s when I got into automation.\nThe later years\nSo I got into automation. I loved it. The thought that I can bend the computers will to do my bidding / job was absolutely mind blowing. I’ve done development at that point already of course that wasn’t new to me. And I also was shown some way of automation but that wasn’t so great so I dismissed this possibility for a long time. After a while I came back to it with the thought that, hey I could do this better. Selenium came out at that time around, Watir was also there and a couple of other tools in Perl.\nI started developing with / in / for those tools and noted that despite the believe that it’s only automation it actually took quite some thinking to come up with a framework that was adaptable, concise, manageable, fast and so on and so fort. It was an effort that most people didn’t realise or care to know about.\nSo for the better part of those years I was trying to convince people that building a testing framework requires actual development work. And is not something that should be taken lightly. I sort of succeeded with it…\nNow\nAfter various circumstances in my life I moved to UK and took a job as an SET(Software Developer in Test). It sounds fancy but is rather just an automation guy who from time to time looks at production code. It was a spring board for me. And now days it seems I’m simply just a developer. But!!! And here comes the twist.\nBecause of my years of background in Test I feel I’m so much more. I know to write testable code. I know many ways how my code could fail. I know many ways of writing something and then coding up a script for testing or do extensive unit testing. I don’t love my code blindly so that when it’s done I only test the “happy path” because I don’t have the time to code up more.\nI will always have time for testing. I will begin with testing. Because people need to understand that the only way to go fast is to go slow. If you go fast it will bite you in the behind and you will suffer more then you would have suffered if you would have written up that one last little test for that one last little corner for the world that is your boiling, brooding, breathing pile of code.\nThe future\nHonestly I don’t know were I’m going from here. Being a full pledged developed is a new territory for me even though I was an automation dev before. But I’m looking forward to this exciting new life. And I will NEVER ditch my Testing carrier. In fact I’m aiming to complete the BBST course next month. Because once you’ve been a tester you will never go back being anything else. It becomes a part of you. And stays with you forever.\nThank you for reading.\nAnd as always,\nHave a nice day!\n","title":"My history in testing","uri":"/2013/04/11/my-history-in-testing/"},{"content":"Hello everybody.\nToday I would like to write about something I experienced over the years of being in the software industry.\nSome History\nI’ve been a tester for a long time now. I’ve been up and down and down and up. I struggled for a very long time until I became somebody who I am today. And I still have many things to learn ahead of me.\nI learned from many people. I personally thing my greatest influence on testing was James Marcus Bach and his Brother Jon Bach you should definitely check out these guys. It took me a long time to get to where I am today. And after ~10 years now I can state with a reasonable confidence that this industry is no longer about technology or solutions to technological problems.\nSurly you Jest…\nNope… I’m not joking. I can see people struggling to understand people and mostly it’s not because of knowledge differences it’s because they simply can’t stand each other. Or they simply think that their solution must be better then the one the other has. Or the guy just woke up with a bad foot.\nAnd industrial problems? Management? Sure it can be knowledge but mostly it’s simply ignorance and pride. Pride because he / she does not want to admit that they don’t understand something. Pride to ask a question that might make them look stupid. I’d rather look stupid for five minutes then for the rest of my life. I rather ask the question that I want to know because I can’t make a proper decision without it. I rather learn then be ignorant for the rest of my life.\nPeople need to overcome their differences. People need to understand that this behaviour hurts the company. It hurts them it hurts the people around them and the product. Swallow your pride and be a bigger man / woman. Face your fear. Be Strong. Be clever be intelligent be a proper grown up human being!!!!\nYou hate my face? SAY IT IN MY FACE!! I can’t resolve issues I DON’T KNOW ABOUT!!\nLast but not least…\nGrow up people. Be nice. Be curious. Don’t be afraid to ask. Don’t be afraid to ask again if you still don’t understand. It might be that the person speaking has a completely different view of something and it might be that nobody really understands him and they are just afraid to ask. And if you ask something stupid? Who cares.. You learned from it.\nBe a professional. A professional is not afraid. And encourage others to be one too. In the today’s technological world solutions can be found by the dozens on the internet. You will be a better work force by adjusting your personal habits a little bit.\nBe nice.\nAnd as always,\nThanks for reading.\nGergely.\n","title":"It’s all about human interaction.","uri":"/2013/01/31/its-all-about-human-interaction/"},{"content":"Hello Everybody.\nToday I would like to show you what you need to become a Testing expert.\nIf you had noticed I deliberately left out the word Software. Why? Because becoming and expert in Testing Anything has the same root as becoming an expert to test software. And for that let me take you back to a Galaxy Far Far Away in Time AND Space.\nThe beginnings…\nI went to school in Oroszlany, my home town in Hungary. My official Technical title would be Mechanical Testing Engineer. I had the luck that most things which were thought at that school and I liked were thought by people I liked too and were clever in their respective fields. Like Pneumatics, Metallurgy, Mechanics, Chemistry these were all really awesome fields. To this day I don’t know why I haven’t ended up in some lab testing the breaking point of metals and the capacity of various chemicals.\nBut ahh well.. I still remember some of the ISO standards we had to memorize. Like in Metallurgy the Charpy pendulum impact test which we loved so very much to perform. The ISO standard for that is ISO 148-1:2009. Now let me see if I still remember that correctly… Yes! I’ve still got it. So where am I going with this?\nMoving on…\nBack then we performed a lot of operations on various metals and a lot of chemical testings and the like. The most important part of every performed test was always to have insight. Insight in the structure of the metal. How it was made, how it was moulded and then shaped and heated and cooled and so on and so forth. We had to know EVERYTHING about the creation of this element that we had to test. Without that knowledge you could of corse still perform the test but you didn’t had any additional information why that test was useful or what it proved. Sure, you could have looked at the specification of the metal but that didn’t give you anything about the inner structure of it.\nThat was provided to you by the knowledge of the procedure that created it.\nYou get where I’m going with this…\nInsight\nSo back to software testing… What is the single most important thing that gives you insight, that gives you the edge to be a professional? Knowing the development process. Knowing architecture. Knowing programming. Knowing the language and its capabilities and limitations.\nSure you can be a good manual Tester. Of course you can be an excellent Exploratory Tester. But look at the others. James Marcus Bach. Alan Richardson. Michael Bolton and I could go on… They all know programming. They all know it by heart and they could be exceptional programmers if they wanted to. They already know that in order to get very good at testing something you must have an insight on the subject matter. You can’t test it thoughtfully without knowing its limitations its rules that it obeys its exploits that you can use.\nLast words…\nSo in order to excel at your craft of test you have to have insight of the subject of the test. You can still become good but you always have to aim high don’t you? A friend of mine reply to one of my tweets with a really good sentence:”…and then you realize that being the best in one implies being excellent in the other.”. Exactly. Because the two are interconnected. They aren’t actually two. It’s one. It’s called Engineering.\nHave a nice day and as always,\nThanks for reading.\nGergely.\n","title":"What you really need to know to become a Testing Expert","uri":"/2013/01/21/what-you-really-need-to-become-a-testing-expert/"},{"content":"Dear Readers.\nToday I want to talk to you about something I discovered over years of working and experience.\nAlso something that I can see in the world repeating over and over again. Not just in Software business but also in History in Human nature in Physics and Systems.\nSo let’s see what I’m talking about.\nWhat is Chaos?\nChaos is a state of utter confusion. Chaos is the opposite of order. Chaos is when everybody does what they want without thinking about the consequences.\nDoes Chaos hurt? Of course it does. From Chaos there can be no production. From Chaos there can be no profit. From Chaos there can be no life.\nIs that really true? It sort of is. Remember the Big Bang. Initially it is believed that there was order. The universe was a perfect gem with the four forces ( Gravity, Electromagnetic, Weak Nuclear and Strong Nuclear force ) united. And then a Higgs came around and made it BOOM! And so there was Chaos. And from Chaos, with a snowballs chance, there was life.\nSo then why is Chaos bad?\nCompanies usually don’t have the time or the money to wait for that Snowballs chance to come to flourish. So order is required for life to become great.\nTake for example the Chinese Emperor Qin Shi Huang. He was the one uniting China. Sure, he killed a LOT of people. And China is oppressed every since. However he did introduce a lot of new concepts and reforms which wouldn’t have been possible in the Chaos. And today China is one of the leading nations if not The…\nBefore Emperor Qin begun his journey there were 9 different ways of writing down the world ‘sword’ in chinese. This was unacceptable. After the Emperor succeeded he unified the writing and made communication easier.\nStriving for order\nPeople have always strived for order. For simplicity. For unification. For ‘one’ instead of ‘many’. We have stories that say there is power in many. Physicist are continuously trying to unify the laws of physics in the beautiful little equations. Like Einsteins law of relativity. That small equations revolutionised the world. And it came from unification.\nSo then what are the downsides of unification?\nEverything comes with a price of course. The unification bought oppression to China and the loss of the individuum. Fantasy and thoughts got all sacked. People become like Borgs. However the Country evolved immensely under one banner. But what does this all have to do with Testing?\nApplying Order\nThink about your company. Think about how your developers work. Does everybody use a different building platform? Does everybody use a different operating system? Do they run the same command to build your environment? Did you have any kind of problems because of that? I’m sure you did.\nThere are benefits of it. At least you will know if your application is working in a different environment. But do you want that? Is it worth it? After all we live in an age were there are numerous possibilities, environments, languages, command line options etc etc.\nBut think about it. Did the diversity bring you more profit or did it hurt you more? Which one is applicable for you? If your developers do not have the ability to properly govern them selfs then you need order. You need a unified language, you need a unified environment and unified processes Agile or not Agile. Your company wont be successful if you want to be Agile just because it is the Current Trend. You need to be pragmatic about it and applied were it is reasonable. There is nothing wrong in introducing a standard or a code review process or a coding guid line or some standards here and there if it HELPS.\nThere will be friction. People tend to resist change. There will be losses. But a unified company is a strong company. And when the new guy arrives? He wont have any kind of a problem installing your software and beginning working on it because he doesn’t have to go around and scoop information together from at least ten people and do it in an eleventh way at the end.\nLast Words\nSo think about this. Is your company strong enough to uphold Chaos and make it flourish? Or is it a bunch of people doing whatever they think is right because of a sense of individuality which they don’t want to give up?\nThis is for you to decide.\nAnd as always,\nThanks for reading.\nGergely.\n","title":"From Chaos There Shall Be Order","uri":"/2013/01/04/from-chaos-there-shall-be-order/"},{"content":"Hello everybody.\nToday I’d like to write about something I discovered over the years of researching and following certain people and movements in Testing and Programming world.\nLet me tell you that there are quite an amazing amount of stuff to be read out there. It’s overwhelming.\nLet’s talk about Methodologies first.\nThere are many out there currently. In programming and in testing too. Many speak of ways how you should or should not do you work. Many of them tend to say that others aren’t the right way. However just as many tend to say that you can share your knowledge or you can only partially use that particular methodology because they realised that if they say others are crap they close themselves from evolving and they create a way of thinking that will say: Ohhh yeah? You can’t tell me what I should do! Who are you to say that that technique is wrong?\nSo they are in threat of alienating followers through this kind of behaviour. Just as commercials stopped saying they product is 100% awesome. Now they say 8/10 people find it awesome. This gives you the thought that surly they must not be lying if they admit that somebody thinks their product is crap.\nHow do you know then that a methodology is lying or is right or is “The way to go.” or that it is in fact a valid methodology?\nBy careful observation and deduction of facts while not leaving out of sight your own ignorance and view points / beliefs.\nYou have to address something by taking yourself a level above the problem domain. Carefully observe various situations and always approach it with scepticism. Guilty until proven otherwise.\nThought Patterns\nIf you do this for a while you will realize that most of the people out there are trying to sell their own crap in some way or another. They will put it into nice little packages will tell you neat little success stories and will make you believe in what they think is right. How do you shield yourself against that?\nBy… drum rolls… learning. You must make your mind to be a weapon of mass destruction. You must hone your skills of thinking you must hone your skills of analysing and understanding complex patterns. In the world there is nothing without a Pattern. Everything follows a greater scheme. You have to find it, grab hold of it and never let it go. You have to understand how people think and how you would exploit that. What there weaknesses are and why are they there in the first place?\nWhen you achieve that you will truly begin to see how the world works and looks like under the hood. Try it on small things first. Try to follow the thought patterns of your coworker. Try to note down his mood changes based on days and current view point. Based on news and knowledge that you can attain from his life. After a while you will be able to predict certain moves and mood changes. You will begin to predict working capacity based on the sun is up or not or if it’s snowing or raining.\nAll these will bring you to a mind state where you can look beyond words. You will notice the patterns behind blog posts. You will notice the bullshit out of the documentation. You will notice when somebody is just writing for the sake of writing. You will notice…\nLies.\nYou have to understand the One Truth in life. We all lie! And the we all lie about lying.\nIf you get that you will start to think more before you approche something. You read a methodology a description a “new found” idea and you realize the lies behind it if there are any big ones. You realize that he / they is / are only trying to be clever or only trying to be famous. They might have something interesting to say, not everything is a lie fortunately, but let’s face it. Most if it is.\nYou have to shield yourself against that. You have to shield yourself from biases, from lies, from cheats, from make beliefe and from posts that are only posts for the sake of posting.\nWhat are the tell tail signs? Are there any? Of course there are..\nBig words, Punctuation and visually carefully constructed blog posts( usually tries to hide something between a facade of good looks), few practical examples, too much of: “…then this will definitely help you improve…”, too much talk without saying anything interesting… and so on ans so forth.\nEnd words\nNow think about these things for a second. Think about thinking. Think about how you can improve. Learn Lateral Thinking. General Systems Thinking. Logical Reasoning. Deductive Thinking. Inductive Thinking. Critical Thinking. And if you did all this… And you find yourself wondering around the world with an open eye seeing things for the first time. Noticing things for the first time. Watching, observing, thinking… Come back and read this post again whit THOSE things in mind. 😉\nThanks for reading.\nGergely.\n","title":"Methodologies, Thought Patterns, Lies","uri":"/2012/12/12/methodologies-thought-patterns-lies/"},{"content":"Hello Everybody.\nI’d like to introduce T.E.A.S. to you. This is something I came up with yesterday which requires a lot of fantasy some good thinking planning and enthusiastic people. So, let’s get started.\nWhat is it about?\nTesting Exploration Adventure Session is about… Testing! There. No real surprise, eh? TEAS has it’s roots in RPGs. Role Playing Games. If you ever heard or read about M.A.G.U.S. or the more known Dungeons \u0026 Dragons you will have a better understanding of the concept behind this phenomena.\nBasics of RPG\nSo now that we know that it has it’s rules in RPGs how will that be applied to testing and learning? Easy… Well it’s not that easy but after you grasp the concept it will get easy.\nJust like in an RPG people get together first. There will be players in the group mostly and one or two Dungeon Masters. The task of the Dungeon Master is to facilitate the Game. The Game it self consists of a set of given rules and a World in which these rules are applied too. The Players are placed in this world which is created by the Dungeon Master. They are then given tasks that needs to be fulfilled in some particular way. That choice is of to the Players. The DM only facilitates. He is the Master of the given World. And plots against the players. He incorporates the Non Player Characters or NPCs of the World and tries hard to trifle the effort of the players.\nThis task can be anything from freeing a princes to twarthing a Magus from gaining Omnipotence or God like powers, to killing a Dragon for its treasures. And Players decide what to do and how to do it. The DM lists the options available. And as the Players move they get experience. They get stronger, faster, better, more intelligente.. They gain Levels.\nNow… How does that fit into Testing?\nHow does this fit into Testing?\nIf you think about testing and the players what comes into mind? You have a product. And you have testers who explore this product in certain ways. As they go and find bugs ( kill mobs ) they get better and more efficient in finding other bugs based on the previous ones. Tasks get harder and harder as the most easy to find bugs are already taken care of. Elusive bugs will be harder to discover ( kill ).\nAnd who is the Dungeon Master? I would say in this case it’s the Product Owner.\nHow to begin\nSo what now? You have your Testers ( Players ) and your Product Owner ( Dungeon Master ). What’s the next move? How does this all begin?\nThe PO present a software. He builds it. Finds it out. Puts together the pieces. Creates maps, road maps, site maps if it’s a server application then it’s structure maps and database diagrams and whatever helps him to present his product to the Testers. He slips and designs bugs into the system. Harder ones and also easier ones. He has to have a story in the application. Maybe it’s a web site that provides some service. There a lot of components that could go wrong.\nThe testers begin by asking questions… They begin as Level 1 Testers. They know nothing yet. They know no programming languages and no metrics and nothing. The goal is to have a fully covered product which they are confident enough to release. They can add the whole release process to the Game too. Depends on what the PO has in plan for that Session. Which could take a few hours or a whole day. Depends on the possibilities.\nHow the Testing works and what’s a level?\nSo as they go on and Test the product, which they have to do verbally, the PO knows what bugs they come across. As they find bugs they earn Experience points. The more they have the more they level up the more tools will be available to use. That can be given to chance. Throw a dice and select a tool from a pool of tools which are available to the whole Project.\nYou can have a Random factor to the bug finding process too. For example if a Tester tries to examine an area he throws a dice to determine his ability to find bugs. That ability comes from initial stats that can be defined at the begin of the game. If he succeeds he finds a bug and gets points for it. The tools can increase the ability to find bugs. For example automation can add bonus to finding repetitive bugs but can add minus points to fantasy / finding really nasty elusive bugs because you might concentrate less on details.\nSo a Level defines the Testers ability to find bugs. The higher level he gets the better his abilities will be to find bugs.\nThe PO defines the End Game. The Games goal could be to find x number of bugs. Or to release the product. Or to bash it, crack it, hack it destroy it. It could be that you have to sell it or demo it to some stack holders. The possibilities are Legion.\nHow to go on…\nI know that this is all very confusing yet.. I’m still working out the individual rules, plays and numbers and Character sheets and such. Any thoughts and ideas are appreciated.\nThe merits of this game are many. It practices testing it practices the ability to explore a product only in fantasy. It helps who ever designs a product to get a glimpse into the world of designing.\nAlso the product might even be not from this time!! It can be a future product of some holographic nature! Or an Audio Visual interface that’s hooked into the users Brain. Or some other advanced future technology. How would you Test that???? How would you test a robot arm controlling algorithm. Or an Artificial Intelligence that controls tactical missiles overseas.\nThe possibilities are virtually limitless. It’s up to the Product Owners fantasy what he builds. It could be a whole new virtual World like the Holoroom in Star Trek. How would you Test THAT???\nIf you are interested in this endevour follow me for more details.\nAnd as always,\nThanks for reading!\nGergely.\n","title":"TEAS: Testing Exploration Adventure Session","uri":"/2012/12/01/teas-testing-exploration-adventure-session/"},{"content":"Hello folks.\nToday I want to write about a little trick I learned.\nIf you are working with legacy code and you don’t have the chance to eliminate core design problems, you can use this little pattern to help you out.\nProblem\nProblem is that you have a class that has a gazillion collaborators and at some point in time one of the clever devs thought it would be a cool idea to do dependancy injection via the constructor. We all know that doing this makes the class immutable which is very good for a number of reasons. However it doesn’t provide a flexible solution if you want to leave out one or two collabs. For that your would have to create Adapter constructors and chain them upwards which would get very ugly very fast. While using JavaBeans getters and setters can leave your class in a harmful state like not at all or partially initialised.\nSo what’s a good solution then?\nSolution\nOne possible solution would be to use some kind of initialisation framework like Springs @Autowired. But cluttering your classes with that isn’t really pretty either. But it’s A solution.\nAnother solution is the usage of a builder pattern.\nConsider this class:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"java\" style=\"font-family:monospace;\"\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003epublic\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003eclass\u003c/span\u003e VeryImportantService \u003cspan style=\"color: #009900;\"\u003e\u0026#123;\u003c/span\u003e   publicVeryImportantService(CollabOne collabOne, CollabTwo collabTwo, CollabThree collabThree, CollabFour collabFour, CollabFive collabFive, CollabSix collabSix){. . . }}Don’t forget that we want these to be optional. I would like to leave out two or three here and there.\nThe builder let’s you do that. It looks something like this:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"java\" style=\"font-family:monospace;\"\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003epublic\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003eclass\u003c/span\u003e VeryImportantService \u003cspan style=\"color: #009900;\"\u003e\u0026#123;\u003c/span\u003e   privateCollabOne collabOne;privateCollabTwo collabTwo;privateCollabThree collabThree;privateCollabFour collabFour;privateCollabFive collabFive;privateCollabSix collabSix;publicstaticclassBuilder(){privateCollabOne collabOne;privateCollabTwo collabTwo;privateCollabThree collabThree;privateCollabFour collabFour;privateCollabFive collabFive;privateCollabSix collabSix;publicBuilder(){}publicBuilder collabOne(CollabOne value){this.collabOne=value;returnthis;}publicBuilder collabTwo(CollabTwo value){this.collabTwo=value;returnthis;}. . . publicVeryImportantService build(){returnnewVeryImportantService(this);}}//private constructorprivateVeryImportantService(Builder builder){this.collabOne=builder.collabOne;this.collabTwo=builder.collabTwo;. . . }}Now… calling this would look something like this:\nThis enables you to be flexible HOWEVER!! I HATE train wrecks. So I would probably tweak it not to return things, but set them. Then you would end up calling then line by line. Which is still not the best but better then the alternative.\nEnd words\nSo there you go. This is A solution not THE solution obviously. The best would be to NOT design such a monster at all. If you have any better ideas please feel free to share. I would gladly put them on my blog.\nAs always,\nThanks for reading,\nGergely.\n","title":"How to eliminate a parameter boom","uri":"/2012/10/09/how-to-eliminate-a-parameter-boom/"},{"content":"Hello everybody.\nSo yesterday I was on a little gathering called Coderetreat.\nIf you look at the link you can read everything about it. If not, I’ve provided a short description.\nWhat it is about?\nSo what is codereatreat about? In short it’s about programmers getting together and honing and practicing there skills in order to become more at what they are doing. It’s a practice run with great people. TDDing one problem the whole day long with constantly applied constraints.\nProblem domain\nUsually it’s some kind of coding kata. This sessions problem was Convey’s Game Of Life. It’s a well known problem which you can’t really solve in the given amount of time if you do it right. But that’s not the goal anyways. The goal is the journey itself as was formulated by our facilitator.\nProcess\nSo what was going on there anyways? At 10:00AM the whole thing begun. Thirty developers set down, choose partners for TDD, and started to tackle the problem at hand. There were 6 sessions which lasted 45 minutes. After that a short retro and then a 10 minute break. On every session the code was deleted and rewrote from scratch. The first two sessions were introduction. And then the fun started with the constraints…\nConstraints\nThe first one was that we were not allowed to use any loops when solving the problem. That was a tricky one. You had several options to go on but me not being a proper developer I couldn’t think of too many. Well that’s why I was there wasn’t I? To learn. I came up with the idea of linking the cells together so when one is invalidated it notifies all his neighbours through a call. But I wasn’t sure how to implement it. Also I choose a group which worked in ruby and it was a long time ago I wrote something in that language. It was a group effort but eventually we came up with a prototype that proved the first rule. I was happy!\nThe second one was TDD ping pong. That was a really interesting experience for me. I grouped with a guy who had already experience in code retreat session so he didn’t spare me. In addition we did silent ping pong and it was HARD! It was really hard for me to formulate my thoughts JUST in tests. So that he can get what I want and understand and agree on the design at the same time. So we ended up throwing tests at each other with the thought: Here try to solve this you bastard! It was great fun indeed!\nThe third constraint was ‘No conditions.’. That was the hardest for me. Coming up with recursion for the first one and having a fight in the second was nothing compared to this. Conditions are a fundamental structure of programming. It’s logic basically. You can have the cells as objects and then a world of cells linked together but you still have to somehow decide if they live or die based on neighbour count.\nSo how do you avert it? I worked with a guy in this one who was good at low level thinking. He came up with the solution of using expressions as returns for the status:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"java\" style=\"font-family:monospace;\"\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #339933;\"\u003e!\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003eneighbours \u003cspan style=\"color: #339933;\"\u003e\u0026lt;\u003c/span\u003e \u003cspan style=\"color: #cc66cc;\"\u003e2\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e \u003cspan style=\"color: #339933;\"\u003e||\u003c/span\u003e \u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003eneighbours \u003cspan style=\"color: #339933;\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color: #cc66cc;\"\u003e3\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e \u003cspan style=\"color: #339933;\"\u003e||\u003c/span\u003e \u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003eneighbours \u003cspan style=\"color: #339933;\"\u003e==\u003c/span\u003e \u003cspan style=\"color: #cc66cc;\"\u003e2\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e \u003cspan style=\"color: #339933;\"\u003e||\u003c/span\u003e \u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003eneighbours \u003cspan style=\"color: #339933;\"\u003e==\u003c/span\u003e \u003cspan style=\"color: #cc66cc;\"\u003e3\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e\u003cspan style=\"color: #339933;\"\u003e;\u003c/span\u003e\u003c/pre\u003e \u003c/td\u003e \u003c/tr\u003e  It’s brilliant and easy if you see it like this but I was so used to thinking in higher structure and so used to the features given by the language, that I couldn’t have come up with this in the given amount of time. So he had the idea and I coded it down. And this is actually working… It returns true or false based on the neighbour count. And we could use that return value to change the state of the cell afterwords.\nThe fourth and last task was that everybody stand up and finish someones else code. Practicing how to handle “legacy” code and a free for all session to finish the job.\nWhat I learned?\nBasically I learned that I really like code. I like writing it, thinking about it, solving problems and creating good, clean, clever solutions. I might be a tester by profession and I love being a tester, but there is a coder somewhere in here too who enjoyed every bit of that session yesterday. Of course this is not the only thing I took from that. I learned good technics. I learned that I’m in a box and I need to think outside of it too. I constraint myself by depending on the language I use. And that I need to keep up practicing alas I loose my ability to solve complex logical problems by coding.\nA coding kata a day helps keeping the bugs away. 😉\nEnd thoughts\nAll in all the day was absolutely awesome. Great lunch and environment was provided by the host Camelotand our fearless leader who walked around all day helping out and coordinating the whole thing Marton ‘Meza’ Meszaros. A big thank you and a lot of respect for doing it all day long.\nUnfortunately because of my broken ankle I couldn’t stay for the drinks but if you ever get to a session like this I strongly recommend staying the whole day AND the night. And not just ’till you stuffed your belly with free food and booze. 😉\nAnd the top of the cherry is a great time laps of the whole day which can be seen here: Coderetreat 29.09.12. Enjoy as we had!\nAs always,\nThanks for reading.\nGergely.\n","title":"Coderetreat London","uri":"/2012/09/30/89/"},{"content":"Hello folks.\nHere is a little graphic I made to show what my brain is up to the whole day long… Excuse me for the lack of my colouring skills… Enjoy\nThanks for reading!\nGergely.\n","title":"What my brain is up to the whole day…","uri":"/2012/09/20/what-my-brain-is-up-to-the-whole-day/"},{"content":"Hi folks.\nToday I want to write to you about learning something with a visual mind. There are a gazillion posts out there that tell you how to learn something with a visual mind. However, there are only a few actually describing how to learn something as complicated and logical as programming. How do you draw up a function? How do you draw up a cycle or a structure?\nActually these are really easy. A cycle? No problem. What’s a circle if not a cycle? Structure? This should be an easy one. You can draw a whole building and then place building blocks into it.\nI have a very strong visual mind. I don’t remember names good for example however I remember every and each face very distinct. So if you come up to me, don’t be surprised if I don’t remember your name. So as I grew older and I took learning into my own hand I realised that there are techniques out there that I could have used to boost my learning in the early stages of my life too.\nBut there is no such thing as to be late for something. Life is constant learning. So don’t hesitate to start learning something.\nI started to learn Scala for example and I was brainstorming as how to draw up a good map for some programming practices. Brainstorming can help you coming up with something that can display your idea. Try to brainstorm in words only rather then sentences.\nAnd I came up with these two for now, with the play framework in the middle.\nNow some of you might say this took long and is unnecessary. The information displayed can be remembered easily and that these drawings took longer then writing up two or three words. Now let’s see…\nThese drawing, which are really rough actually, took me about 10 minutes while reading and learning and what not. Yes they take longer. But if I write down two words how long will I remember them? I already forgot after I wrote it down!\nThis drawing with the griffon legs? Stuck there forever. ( Don’t ask about the legs.. don’t know where they came from… )\nSo visualisation goes a long way in your brain if you are wired that way. Remember that there is NOTHING that you can’t display with a little bit of creativity and graphics.\nOhh and btw… Chances are that you only read the words in BOLD because your mind draws your attention to parts that stand out. And also you want to get over reading this article fast and you appreciate if key elements are taken out for you without the unimportant gibberish in between. 😉\nThanks for reading!\nGergely.\n","title":"Learning programming with a visual mind","uri":"/2012/09/09/learning-programming-with-a-visual-mind/"},{"content":"So today at 8-12PM I had a great session with two friends of mine. It was awesome. Like a mini code retreat.\nWe set down in a musky bar, drank wine and beer and cider, and decided to practice some TDD with the well known problem of Conway’s Game of Life. This challenge is really interesting. I never done it before, ever. So it was a really good practice for me.\nSo…\nIn the beginning there was Test\nOne of my friends and I started out by developing the implementation for the game while the second one was mentoring and couching us. As with any problem I’m facing now days, I started with writing a failing test first. I didn’t write any kind of production code yet. I wrote a test testing for having the class called game of life.\n \u003ctd class=\"code\"\u003e \u003cpre class=\"java\" style=\"font-family:monospace;\"\u003e @Test \u003cspan style=\"color: #000000; font-weight: bold;\"\u003epublic\u003c/span\u003e \u003cspan style=\"color: #000066; font-weight: bold;\"\u003evoid\u003c/span\u003e shouldHaveClassForGameOfLife\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e \u003cspan style=\"color: #009900;\"\u003e\u0026#123;\u003c/span\u003e GameOfLife gameOfLife \u003cspan style=\"color: #339933;\"\u003e=\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003enew\u003c/span\u003e GameOfLife\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e\u003cspan style=\"color: #339933;\"\u003e;\u003c/span\u003e \u003cspan style=\"color: #009900;\"\u003e\u0026#125;\u003c/span\u003e\u003c/pre\u003e \u003c/td\u003e \u003c/tr\u003e  This wasn’t compiling of course because I didn’t have any kind of GameOfLife class. But intelliJ is so intelligent that I simply pressed Alt+Enter and created the class immediately. The class didn’t have anything in it, but I already had a passing test.\nSo this went on and on and I created one test after another while my other coding friend did the same.\nNow the amazing part\nI begun working on the Grid. A simple octagonal coordinating system. This was represented in the beginning with a simple two dimensional array with Cells in it.\n \u003ctd class=\"code\"\u003e \u003cpre class=\"java\" style=\"font-family:monospace;\"\u003e Cells\u003cspan style=\"color: #009900;\"\u003e\u0026#91;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#93;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#91;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#93;\u003c/span\u003e cells \u003cspan style=\"color: #339933;\"\u003e=\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003enew\u003c/span\u003e Cells\u003cspan style=\"color: #009900;\"\u003e\u0026#91;\u003c/span\u003e\u003cspan style=\"color: #cc66cc;\"\u003e50\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#93;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#91;\u003c/span\u003e\u003cspan style=\"color: #cc66cc;\"\u003e50\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#93;\u003c/span\u003e\u003cspan style=\"color: #339933;\"\u003e;\u003c/span\u003e\u003c/pre\u003e \u003c/td\u003e \u003c/tr\u003e  This of course wasn’t dynamic. I didn’t care about that yet. I had my grid of cells. These cells were initially all dead.\nNow, the interesting part is that as I developed my Grid finding out the Cells neighbours and counting them, my friend worked on the Cells themselves and getting their next state and killing them based on the rules.\nWe never talked to each other, didn’t agree on roles or directions or anything. And even so at the and… We were at a stage where we met in the middle and could merge our codes! Our little game of life evolved with a push of a button. ( Three actually. )\nThis was simply amazing. Without ever talking about the direction we want to go we created a working code base that could be merged!\nIt just works\nBefore TDD I would have tackled this problem much differently. And it would have taken me much more time too. This was like an hour or so.\nTDD helped me break down the job into small, manageable tasks. I created and deleted and rewrote tests as I went on and on and developed the algorithm for my Grid and Cell. And eventually the problem slowly unfolded itself right before my eyes. I began to see the connections. I began to see the beauty. I began to understand! This is something I rarely enjoyed previously without using TDD.\nSummary\nI recommend for you guys to do the same. Just sit down, find a problem, find a coding kata and just do it with TDD. With PROPER TDD.\nHere are some good sites for katas and problems:\nhttp://codekata.pragprog.com/http://www.spoj.pl/problems/classical/Just select a problem and then start cracking on it. Do this every time you have some free time. Like a martial art trainee doing basic exercises and you will get better at problem solving and at TDD too. I promise.\nHappy coding and good night!\nGergely.\n","title":"TDD and Game of Life","uri":"/2012/07/12/tdd-and-game-of-life/"},{"content":"Aka, what you can do if you are facing and unknown framework / system you have to work with for quite some time.\nGet intimate\nYou are going to live with this system for a while. The best thing you can do is getting to know it better. You have to get it to know like you would approche a fine lady. You have to ask it questions look after it, how it feels how its day was. Have to listen to what it tells you, you have to read its diary if necessary.\nAfter a while you will be in a relationship with it. Now it’s time to look out for it. Nurture it. Grow it. Care for it. A good piece of software is like a happy lady. It will great you every morning and it will say good by when you go home.\nSo how do you get to know a new system better.\nDebug\nIn a new system Debugging is like a good old friend who tells you a story about the lady you want to conquer. You find a workflow and execute it in debug mode and follow each and every step that it makes. This way, you will see how each package and class is coupled together. You will be able to see the connections between modules and between calls and functions. It is a good way to see the inner workings.\nAsk her girlfriend\nNothing knows more of the system at hand as its best friend. And a good systems best friends are the tests. Take a look at the unit tests, take a look at the integration tests and the acceptance tests. These will tell you more about the system and its actual working strategy than any documentation you can put your hands on. The tests are the closest and next best thing after the real deal.\nConclusion\nIt’s really not that hard to get to know a large system better. Follow a main process with debug, look at the tests, talk to developers, and read some documentation about it. By the time you are done you will be best of friends and you will learn how to handle her the way she likes it. 😉\nHope that helped.\nCheers,\nGergely.\n","title":"Journey into an unknown system","uri":"/2012/06/28/journey-into-an-unknown-system/"},{"content":"My solution to the String Wrap Kata. The goal is to have it wrap a text on a given column width.\nIt is not the best solution but this is my first try. I did it with TDD so there were tests first, which I’m not going to copy in..\n \u003ctd class=\"code\"\u003e \u003cpre class=\"java\" style=\"font-family:monospace;\"\u003e\u0026nbsp;  publicclassWrapKata {publicStringwrap(Stringinput, intcolumnSize){if(input.length()\u003c=columnSize)returninput;else{returnwrapLines(input, columnSize);}}privateStringwrapLines(Stringinput, intcolumnSize){intbreakPoint =getBreakPoint(input, columnSize);Stringhead =createHead(input, breakPoint);Stringtail =createTail(input, breakPoint);returnhead +=\"\\n\"+wrap(tail, columnSize);}privateStringcreateTail(Stringinput, intbreakPoint){returninput.substring(breakPoint).trim();}privateStringcreateHead(Stringinput, intbreakPoint){returninput.substring(, breakPoint).trim();}privateintgetBreakPoint(Stringinput, intcolumnSize){if(input.contains(\" “)){returninput.lastIndexOf(' ‘, columnSize);}else{returncolumnSize;}}}","title":"Solution to Wrap Kata","uri":"/2012/06/26/solution-to-wrap-kata/"},{"content":"Hi everybody.\nToday I want to talk to you about.. Well.. How to read a professional book for slow learners without too much waste of time.\nLet us start at the… well… the beginning. How do you normally read a book if you want to memorize it properly. You read it once and then re read the whole thing again until it gets in your head? Or do you have another strategy?\nLet me share you mine.\nThere are a few common practices that you can apply to better your chances of retaining the information that you get out of the pages you read.\nRe-reading the right way\nRe-reading is good. You definitely can apply that but do it the right way. As you read a chapter, define small chunks of paragraphs or entities or even the whole chapter for re-reading. That means… Read an entity fully. Don’t stop at what you don’t understand don’t bother with it right now just read it. You will get a general overview of what’s going on in that segment.\nOnce done, re-read the whole entity again, but this time pay attention to the details. This time look up what you don’t understand. Go with the flow and once you’re done go to the next section. This is not all however..\nNote taking\nAs you go and re-read the entity you might want to consider that it is a good time for taking notes. Note taking is really something that you have to learn. Proper note taking that is. Notes that just repeat what ever is in the book or whatever you are reading currently is bad. That is just coping, that wont help you. You have to grasp the fundamentals. You have to take out what is important and forget the rest. You have to apply SMART objectives, little annotations, small sentences WITH YOUR OWN WORDS that will lead you to better understanding of the stuff that you are reading.\nThere are several methods that you can use for note taking, my favourite is Mind Mapping.\nWriting up questions\nOne other very good practice is simply defining your own questions. Look at the title of a chapter, maybe a small summary of the chapter if that’s available. Try to define a set of questions that you want to get out of that chapter. Try to write down at least 3-4 questions that you think will be answered in that segment.\nRead the segment. And now try to answer your Own questions without peaking. If you can’t you either have to write down some other questions because it wasn’t answered in that chapter. Or if it was answered but you still can’t answer it from the top of your head that means that you didn’t understood that chapter and that you have to re-read that part. If you can answer it, write it down so that it will stuck with you.\nLinking information\nYour brain is mostly associative. It works the best if it can link information to old information which is already in your head. Mind mapping helps with that. Mind mapping helps identifying old information segments in your brain. Try to find some common ground with the information you have to learn now and the one you already have. Unless your brain works differently you will better remember: “Two friends went to the lake fishing at night and they caught three fishes.”; then 23.\nConclusion\nGenerally speaking re-reading is a good practice but using it alone will sometimes not get you what you want. Re-reading the whole book isn’t going to help. Writing notes helps, but use your own wording, formatting, DON’T COPY!\nI hope that help a bit to you guys out there who don’t have a photographic memory.\nCheers,\nGergely.\n","title":"How to read a professional book for slow learners","uri":"/2012/06/18/how-to-read-a-professional-book-for-slow-learners/"},{"content":"Hi.\nToday I want to talk about a common problem in many frameworks I encountered over the course of my carrier as a Java dev / automation engineer, whatnot.\nThrowing Exceptions. That is in your method you have something like this:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"java\" style=\"font-family:monospace;\"\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003epublic\u003c/span\u003e \u003cspan style=\"color: #000066; font-weight: bold;\"\u003evoid\u003c/span\u003e insertMethodNameHere\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #003399;\"\u003eString\u003c/span\u003e param\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003ethrows\u003c/span\u003e \u003cspan style=\"color: #003399;\"\u003eException\u003c/span\u003e \u003cspan style=\"color: #009900;\"\u003e\u0026#123;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#125;\u003c/span\u003e\u003c/pre\u003e \u003c/td\u003e \u003c/tr\u003e  This is bad. And I will tell you in short forms why.\nHides exceptions\nThis one should be obvious. When a method throws exception you can never be sure what kind of exceptions it handles. It will hide what problems it can encounter. It will hide possible threats and will generally mean nothing to you when it fails.\nHides functionality\nOne of the things you can do will come up with a method that throws at least six exception. Well guess what… That will tell you that the method has DESIGN ISSUES! The first rule of software development is that a method should do only one thing! Well if it throws six exceptions chances are it does more then one…\nHard to debug\nYou wont have a meaning full exception if it fails immediately. You will have to go through lines of codes and stack traces to find out what the hell happened and what threw what kind of exception where. That is just simply stupid. Why give yourself a hard time?\nSo what to do instead?\nMeaning full exceptions\nIf you have to throw… Throw meaning full exceptions. Things like: LoginFailedExpcetion(String username, String password); In the message write:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"java\" style=\"font-family:monospace;\"\u003e \u003cspan style=\"color: #003399;\"\u003eString\u003c/span\u003e.\u003cspan style=\"color: #006633;\"\u003eformat\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #0000ff;\"\u003e\"Failed login with username: %s; password: %s\"\u003c/span\u003e, username, password\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e\u003cspan style=\"color: #339933;\"\u003e;\u003c/span\u003e\u003c/pre\u003e \u003c/td\u003e \u003c/tr\u003e  When this fails you will immediately know what happened without miles of stack trace to run through.\nWhen you go ahead and start to re-factor your code to handle exceptions properly you will end up with methods throwing six – seven exceptions. Don’t be afraid. That means that you finally are able to see that your code is doing many things that it is not supposed to do. Start by extracting bits and pieces of your code into smaller segments that throw a max of two exceptions. Write unit tests around the function and start running them as you re-factor. See what fails so you can track your changes as you go.\nA general good advice is that your method should throw a maximum of TWO exception. If you have more, you are doing something more then you should. Refactor / Extract that bit into a smaller function.\nHandling exceptions\nWhat you really want to do is create a Layer that you will be using to capture and handle exceptions. You can than take that layer and hide it deep deep into your framework so that you never ever see it again. Just like with switch.\nAs you go you will have layers of layers of exceptions. You will have features that depend on each other and talk to each other in some ways.\nMeaning full exception will help you find out what broke why. For example you have Login that throws a Login exception. On top of that you have an order that handles OrderFailedException. You will have an easy time seeing that the order failed because the login failed. Because you handled your login exception in the Login Layer. And you handled your OrderException in the order layer. There are no mixes. Keep it clean and keep it where it belongs.\nConclusion\nExceptions are part of Java just like String or int or long is. You use these wisely so why don’t you apply that same logic to your Exception handling? Don’t be afraid of having 20-25 exception classes. Group them together or leave them in their respective packages or have them in a deep layer but HAVE THEM. They WILL save time and time is always money. So they will save you money in the end when an error occurs. And errors will always occur.\nThanks for reading,\nGergely.\n","title":"Don’t throw Exception","uri":"/2012/06/13/dont-throw-exception/"},{"content":"Hi folks.\nI want to write a little bit about writing a professional blog.\nEarly steps\nLet’s first talk about what you understand about professional blog. It’s either a semi professional one, that mixes life with professionalism, like chess, programming, testing, painting, drawing, engineering whatever your profession is. Or it can be a purely professional one without your life getting involved in it.\nEither way you have to decide at the beginning. Why? Because that’s the kind of people you will attract. If you want to attract people from your profession then you have to decide that you will write plenty of your profession into your blog. If you want to attract hobbists more then you write about your profession AND daily life or other situations that are essentially irrelevant. Of course you can get both in both versions but the numbers will be different.\nCommiting\nA professional blog is a commitment. Start one only when you know you can “finish” it. It is a constant commitment that you HAVE to do even if you are sick, lazy, don’t have time etc, etc.\nI know the excuses because I made them myself a couple of times too, like: I don’t have time. I write only when I want to. I write for myself, I don’t care about people. I don’t have an idea right now. I have better things to do. I don’t feel like writing.\nAnd so on and so fort… Truth is people… These are usually bull.\n1. You have time. Whenever you TRULY want to write you will make the 20 minutes to at least write a small post. Why? Because this is a professional blog. You WANT to get noticed. You WANT to help people. You started it because you wanted to make a difference. You wanted people to know you to praise you to acknowledge you. Guess what… If you are not writing anything, it wont happen. There are people out there with 4 children and a wife and he DOES find the time to write a blog.\n2. That would be true if you wouldn’t be committed to your blog. Don’t forget, this is a professional blog. This is something that you are writing to make people recognize you. Blogging is about discipline. You have to set aside some time, like 30 minutes / 1 hour to write something in your blog. Why 1 hour? Because of the research that you have to do before hand. I talk about that later. Anyways. This is a commitment. If you don’t take it seriously, then don’t write it. But then don’t wait for people getting to know you, because they wont.\n3. This is just plain bull. If you write for yourself make your blog private and be done with it. ‘nough said.\n4. Now this is something that we all face. In programming, in engineering in whatever your profession is, you will have a point in life, several actually, in which you don’t know what to write about. That is true. It’s common and it’s called writers block. There are several ways of over coming that you can Google it. But don’t forget this is a professional blog. Try to read a couple of others. Try to merit from real life, try to sit down and think quietly for a couple of minutes. Meditate, try to see connections where you didn’t look before. Like building a robot for helping you quit smoking. How? Easy. Smoke detector. When he detects smoke comes out and sprinkles you with water. Or, since this is a testing blog, if you ran out of ideas what you want to write about testing, try this: Read a few magazines about programming, about real life scenarios, about hostage situations, about catastrophes, about hardware failures, about how your neighbor failed to pay his rent. These ALL are great sources for identifying testing opportunities and drawing connections with the software world. What I really found to be a good practice is going around having a notebook in your pocket and the moment I’m hit with an idea I will write it into that notebook. I don’t like using a phone, or other kind of electronic device for that. Why? Because the action of taking out my book, getting my pencil and actually writing something down physically makes me remember it better. But hey… Whatever suits you best. Just do it.\nI could go on and on to from where to merit ideas for a blog, it could even be a whole separate post but I will leave somethings to figure out for yourself.\n5. This correlates to directly to line 1 and line 6. If you have a writers block or you are just plain lazy or you don’t want to write at all, you will say lines like these.\n6. See 5.\nConclusion\nWriting a professional blog is about commitment and discipline. Like chess, like drawing, like your own profession it needs time, it needs nourishment it needs petting, and nurturing and loving and hating and practicing. You started this blog to reach something. You started it for yourself so you could track your progress and knowledge. You started it because writing down ideas makes you remember them better or it will make more sense to you. You started because you want to get acknowledged. You started because you want to share your experience with beginners. You started because you want all of the above.\nThen do it. Don’t look for excuses. Writing a GOOD professional blog is hard. It will teach you much. It will make you more disciplined. It will increase your will power. It will increase your understanding of your craft. It will help other beginners who seek advice. It will help to make a name for yourself. It will help in an interview. It will generally be useful when ever you want to record a major even or idea in your life.\nAlso don’t forget… You are having fun while you are doing it. You love it, and you know you love it, because if you wouldn’t, you wouldn’t really start in the first place.\nOh and by the way… Not every single one of your posts must be a small book sized one. You ARE allowed to write smaller pieces in a hurry if you just wanted to share a general idea.\nHope you liked this and I hope it could help you get started or to keep you on track.\nHave a nice day,\nGergely.\n","title":"How to write a professional blog","uri":"/2012/06/13/how-to-write-a-professional-blog/"},{"content":"Hi!\nI’ve seen this many times over and over again. Many people have wrote books about it already. Like Uncle Bob’s Clean Code. Like Pragmatic Programmer by Andrew Hunt and David Thomas. What makes your code understandable to others.\nIs it Comments?\nNo. It’s not comments. If your code could be read properly you wouldn’t need comments to explain what it does. Like Uncle Bob said. A good code doesn’t contain surprises. It does exactly what you would think it should do on the next line. It doesn’t have curves and misinformation. It doesn’t have plots and turns of events like a good crime book. No. Good code is a like a boring soap opera with predictable plot and boring plain characters who don’t change there behavior based on circumstances.\nGood code is easy to read. It flows like the river, falls like a waterfall, cooks like bacon and crosses the road like a professional chicken. If I read line A the next line should be B. If it is a Z or a :@L$… I wont be happy.\nSo then what makes it understandable?\nOn simple word: Readability.\nWhat makes it readable?\nSmall chunks of functions that have descriptive names as few parameters as possible and do only ONE thing at a time. Of course this is not all there is… However it’s the best thing to begin with. A function called “doStuff” that has a complexity of 300 has three fors, two switches and a dozen ifs isn’t really helping. Now if you look at doStuff and try to give a name based on the job of the function and come up with “propageXWithFiveUnlessYEqualsTheSumOfZPlusW” you will know it does more then one thing.\nIf you see a really complex function in your production code or hobby code ask yourself: “Should this really be like 300 lines long and with a complexity of 200??” And as you speak this out loud you will know the answer already. Break it up. Have like a dozen smaller functions that will be better I promise you. Take out parts. Write unit tests to it that help with re-factoring. Break it down into as small chunks as possible. It will be worth it. It will increase understand-ability, readability and maintainability.\nHope that helped.\nThank you for reading and as always,\nHave a nice Day,\nGergely.\n","title":"Making your code understandable","uri":"/2012/06/13/making-your-code-understandable/"},{"content":"Hi guys.\nToday I want to talk about a little adventure I had yesterday night. It was quite the fun and frustration too. But neither comes without the other when it’s about linux.\nSo let us see what the problem is at hand. The problem machine is a Dell Inspiron N5110 with Nvidia card number one: GeForce GT 525M. And number two: Integrated. Optimus for the win.\nSo how windows is handling this is actually with a software called Optimus. Now linux wasn’t design to handle this properly but there are solutions. But I’m getting ahead of myself. Let’s start with the install.\nUbuntu Install\nSo first of all I installed Ubuntu 32 bitbecause I experienced more problems with 64 bit. To be honest the ubuntu page also recommends 32 bit. You don’t get to much from the 64 any ways.\nAfter I downloaded and burned my disc and installed ubuntu next to my windows 7, I went for the updates. Now HERE is the first key point in my struggle. After the install I went for the additional drivers listed. There were actually additional drivers listed at that point!! Which is interesting because AFTER I installed the updates they disappeared and never appeared again. I’m guessing that one of the packages overrode my drivers. I would go back and reinstall the thing and experiment with it, but I don’t care any more.\nSo let’s move on…\nAfter updating…\nSo update went on and my Ubuntu was not using the proper resolution for my screen. It was stuck on 1024×768. Now at this point I would say I could have played around with xrandr and cvt but more about that later…\nI was immediately searching for additional drivers only to find that my list was empty…\nLike this. Now this isn’t something new actually. I had this one before and I could not for the life of me solve it. Let’s see what I did.\nCommon in Every solution\nFirst let’s go over some repository updates I did before starting to get some solutions.\nI added the x-swat repository to apt-get because that has the most recent packages that will be released.\nAdd it with these commands:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"bash\" style=\"font-family:monospace;\"\u003e \u003cspan style=\"color: #666666; font-style: italic;\"\u003e#Add swat repository\u003c/span\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003esudo\u003c/span\u003e add-apt-repository ppa:ubuntu-x-swat\u003cspan style=\"color: #000000; font-weight: bold;\"\u003e/\u003c/span\u003ex-updates \u003cspan style=\"color: #666666; font-style: italic;\"\u003e#Update and upgrade\u003c/span\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003esudo\u003c/span\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003eapt-get update\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003e\u0026\u0026\u003c/span\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003esudo\u003c/span\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003eapt-get upgrade\u003c/span\u003e\u003c/pre\u003e \u003c/td\u003e \u003c/tr\u003e  Now you’re ready to move on…\nSolution Fail Number One\nMy first guess was to reinstall nvidia driver because of the updates the new driver has to be built with the new version of kernel.\nSo what I did was:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"bash\" style=\"font-family:monospace;\"\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003esudo\u003c/span\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003eapt-get remove\u003c/span\u003e \u003cspan style=\"color: #660033;\"\u003e--purge\u003c/span\u003e nvidia-current\u003c/pre\u003e \u003c/td\u003e \u003c/tr\u003e  After that finished I reinstalled everything:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"bash\" style=\"font-family:monospace;\"\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003esudo\u003c/span\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003eapt-get update\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003e\u0026\u0026\u003c/span\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003esudo\u003c/span\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003eapt-get install\u003c/span\u003e nvidia-current nvidia-settings\u003c/pre\u003e \u003c/td\u003e \u003c/tr\u003e  Additional drivers sometimes needs update to get new drivers. After that I rebooted. At this point I didn’t have an xorg.conf files yet.\nAfter the reboot everything was the same. Nothing changed. nvidia-settings still said I don’t appear to be using nvidia x. All right I thought let’s do that.\nSo I run:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"bash\" style=\"font-family:monospace;\"\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003esudo\u003c/span\u003e nvidia-xconfig\u003c/pre\u003e \u003c/td\u003e \u003c/tr\u003e  And reboot..\nNow THIS messed up my resolution pretty badly. All I was able to get my desktop to was 640×480. At this point I begun to play with xrandr.\nXrandr\nSo in order to get my resolution back I started to play around with xrandr at the first place. I wasn’t trying to add anything to xorg.conf yet because I needed to find out if it would even work!\nNow xrandr adds unsupported resolutions to video cards. If you have a resolution which us unknown you can set it using cvt.\nHere is an article how to do so: XrandrAlas this didn’t work… LVDS1 which was the display for my laptop didn’t wanted to allow the new resolution I added for 1366×768. The error was:\nX Error of failed request: BadMatch (invalid parameter attributes)\nMajor opcode of failed request: 150 (RANDR)\nMinor opcode of failed request: 18 (RRAddOutputMode)\nSerial number of failed request: 25\nCurrent serial number in output stream: 26\nI couldn’t make much of this rather then that my card was still not properly configured and additional drivers was still empty.\nAs back to square one. I deleted xorg.conf and begun another solution.\nSolution Fail Number Two\nAs I was going through problems I found one interesting one. It was a guide on how to install downloaded nvidia driver from scratch.\nSo again I went and uninstalled nvidia and started this solution. The steps are these:\n  Start ubuntu with recovery mode. Login in root shell (with networking)\n  Remove your nvidia driver(what you did install) maybe this can be help: sudo apt-get purge nvidia-current sudo rm -rf /etc/X11/xorg.conf\n  restart your computer: sudo reboot\n  start ubuntu normally (not recovery)\n  open /etc/default/grub : sudo gedit /etc/default/grub\n  replace the line GRUB_CMDLINE_LINUX=”” to GRUB_CMDLINE_LINUX=”nomodeset” (save and exit)\n  update grub: sudo update-grub\n  Download appropriate driver from nvidia\n  10.Give executable permission to the downloaded file : chmod a+x nvidia_driver.run\n Press CLT+ALT+F1 [command line shell will appear] and login\n  stop lightdm (display manager) service : sudo service lightdm stop\n  start nvidia installation: sudo ./nvidia_driver.run\n  reboot your system: sudo reboot\n  Now this brought up a couple of new problems. First that although I downloaded the proper driver from Nvidia it failed to detect my GPU for whatever reasons. And second it could not build because it couldn’t find nvidia.ko. I couldn’t resolve these issues although I guess there are some for it. But in the end it didn’t matter.\nI reverted back to my original state… which was removing all of the drivers and resetting grub to its original state and went on to solution number three…\nWorking Solution Number Three\nAt this point I just wanted SOMETHING to work. I didn’t even care about my nvidia card any more. And that was when I came across a post about dual cards. Something I didn’t care about because IT WAS WORKING before the UPDATE! But I want on any ways and that was the right solution in the end.\nYou can find this solution here. The first answer.\nFor my sanities sake I will write it down here too.\nFirst\nRemove nvidia drivers… Again.\nSecond\nReinstall Mesa package for GL:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"bash\" style=\"font-family:monospace;\"\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003esudo\u003c/span\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003eapt-get\u003c/span\u003e \u003cspan style=\"color: #660033;\"\u003e--reinstall\u003c/span\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003einstall\u003c/span\u003e libgl1-mesa-glx\u003c/pre\u003e \u003c/td\u003e \u003c/tr\u003e  This will get your first card to work with ubuntu.\nAt this point I reinstalled my nvidia drivers too… Something the other guy didn’t mention.\nThird\nReboot\nFourth\nInstall a program called bumblebee. Yes, BumblebeeThis is equal to Windows optimus. It will handle your dual video cards. You’ll see in a moment how.\n \u003ctd class=\"code\"\u003e \u003cpre class=\"bash\" style=\"font-family:monospace;\"\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003esudo\u003c/span\u003e add-apt-repository ppa:bumblebee\u003cspan style=\"color: #000000; font-weight: bold;\"\u003e/\u003c/span\u003estable \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003esudo\u003c/span\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003eapt-get update\u003c/span\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003esudo\u003c/span\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003eapt-get install\u003c/span\u003e bumblebee bumblebee-nvidia\u003c/pre\u003e \u003c/td\u003e \u003c/tr\u003e  Fifth\nAdd yourself to use Bumblebee:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"bash\" style=\"font-family:monospace;\"\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003esudo\u003c/span\u003e usermod \u003cspan style=\"color: #660033;\"\u003e-a\u003c/span\u003e \u003cspan style=\"color: #660033;\"\u003e-G\u003c/span\u003e bumblebee \u003cspan style=\"color: #007800;\"\u003e$USER\u003c/span\u003e\u003c/pre\u003e \u003c/td\u003e \u003c/tr\u003e  And then comes the magic. So in order for you to be able to use your second card with bumblebee you have to execute the program with optirun. This is much like windows optimus, just optimus works in the background.\nAfter this I could finally see my cards settings for example by typing in:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"bash\" style=\"font-family:monospace;\"\u003e \u003cspan style=\"color: #c20cb9; font-weight: bold;\"\u003esudo\u003c/span\u003e optirun nvidia-settings \u003cspan style=\"color: #660033;\"\u003e-c\u003c/span\u003e :\u003cspan style=\"color: #000000;\"\u003e8\u003c/span\u003e\u003c/pre\u003e \u003c/td\u003e \u003c/tr\u003e  This executed the settings app and I was able to edit some settings I required while ubuntu was running fine with my other video card as primary card.\nNow that was quite the fun, like I said, not?\nI hope this guide showed you my errors and problems and maybe it could help you get along with yours.\nIf you have any questions, please feel free to write.\nThanks for reading!\n","title":"Getting Dual Card to work on Ubuntu 12.04.","uri":"/2012/04/11/getting-dual-card-to-work-on-ubuntu-12-04/"},{"content":"Hello chumps.\nToday I want to write about jms connection testing with a small framework. I wrote a small thing using a factory object model. It’s a lead, a proof of concept. You can use this to go onward.\nFirst, let’s begin with the JMS connection it self.\nJMS Connection\nFirst rule of thumb is: “Don’t wait for a response when dealing with JMS queues.” How so? Because, a JMS queue is asynchronous so you wont get back anything. There are however two ways of checking if it was a success or not.\n1: Check your database. The service you are trying out probably records something in the database, right? Check it… You can use a simple JDBC connection, or a Postgres connection or whatever your choice of database is.\n2: You can monitor use the log of your choice of service provider. If there is an exception the moment you send something, you can be sure it is received. Just the format is not correct. This is of course based on how your service handles exceptions.\nSo let’s get down to business.\nFirst, there is a really good article on how to create a JMS connection.\nThis is the link for it: Simple Guide to Java message service JMS using ActiveMQ\nItt will tell you everything you need to know about creating a connection and waiting for a response.\nI will tell you now how to use this information in a real live environment.\nIn a real environment you will be using a queue which has certain settings that will not allow you to “join” it, or creating it. And you need to get the name of the queue and certain settings, like the destination URL.\nFirst, the tool you are going to use is called JConsole. JConsole is a tool to monitor applications. It’s tool to monitor the JVM. I wont go into details about it since there are numerous descriptions about it. It is part of the java installation.\nSo after firing it up and giving it a connection url which would look like this: ‘service:jmx:rmi:///jndi/rmi://hostName:portNum/jmxrmi’, you would go ahead and search on the TAB:Threads.\nLook for a Thread that is labelled like this: Transport Server: tcp://0.0.0.0: This will be your destination url.\nIn the blog the guy is using ActiveMQ. It’s your best guess. It’s lightweight, it’s fast it’s easy. Go for it.\nSo your Destination would look like this:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"java\" style=\"font-family:monospace;\"\u003e ConnectionFactory connectionFactory \u003cspan style=\"color: #339933;\"\u003e=\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003enew\u003c/span\u003e ActiveMQConnectionFactory\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #0000ff;\"\u003e\"\u0026lt;yourserviceparameter\u0026gt;://tcp://0.0.0.0:\u0026lt;port\u0026gt;\"\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e\u003cspan style=\"color: #339933;\"\u003e;\u003c/span\u003e \u003cspan style=\"color: #003399;\"\u003eConnection\u003c/span\u003e connection \u003cspan style=\"color: #339933;\"\u003e=\u003c/span\u003e connectionFactory.\u003cspan style=\"color: #006633;\"\u003ecreateConnection\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e\u003cspan style=\"color: #339933;\"\u003e;\u003c/span\u003e connection.\u003cspan style=\"color: #006633;\"\u003estart\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e\u003cspan style=\"color: #339933;\"\u003e;\u003c/span\u003e\u003c/pre\u003e \u003c/td\u003e \u003c/tr\u003e  After that you will need the queue name which you can get as easy as this. Go to the TAB MBeans. There you can see, if you are using ActiveMQ, you will see something like this : org.active.activemq. Open this up and you will see under localhost a number of queues that your server has configured. Open up one of them and copy the queue name in the createQueue.\nUse it like this:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"java\" style=\"font-family:monospace;\"\u003e Destination destination \u003cspan style=\"color: #339933;\"\u003e=\u003c/span\u003e session.\u003cspan style=\"color: #006633;\"\u003ecreateQueue\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #0000ff;\"\u003e\"\u0026lt;queue name\u0026gt;\"\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e\u003cspan style=\"color: #339933;\"\u003e;\u003c/span\u003e\u003c/pre\u003e \u003c/td\u003e \u003c/tr\u003e  Of course if your service is configured properly you wont have any access to it. Use the connection like this:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"java\" style=\"font-family:monospace;\"\u003e connection \u003cspan style=\"color: #339933;\"\u003e=\u003c/span\u003e connectionFactory.\u003cspan style=\"color: #006633;\"\u003ecreateConnection\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #0000ff;\"\u003e\"username\"\u003c/span\u003e, \u003cspan style=\"color: #0000ff;\"\u003e\"password\"\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e\u003cspan style=\"color: #339933;\"\u003e;\u003c/span\u003e\u003c/pre\u003e \u003c/td\u003e \u003c/tr\u003e  You will have now logged in with the proper user.\nNow you can send the message. You have everything configured.\nFramework\nLet’s speak about the framework you will need to properly use this technology.\nOne of the paradigms for programming is design to interfaces. If you need a proper working framework, your ave to design with the mind set to changing pieces of code. Thinking about what would change the most. Your connection settings. You want a framework which can use any kind of connection. Not just JMS but whatever connection you would like. It could be a synchronous one. Or a database one. Or a JMS. Doesn’t matter. You are only interested in a message sent or a connection, or whatever you want.\nSo let’s get to it.\nInterface:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"java\" style=\"font-family:monospace;\"\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003epublic\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003einterface\u003c/span\u003e IConnection \u003cspan style=\"color: #009900;\"\u003e\u0026#123;\u003c/span\u003e \u003cspan style=\"color: #000066; font-weight: bold;\"\u003evoid\u003c/span\u003e sendMessage\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e\u003cspan style=\"color: #339933;\"\u003e;\u003c/span\u003e \u003cspan style=\"color: #009900;\"\u003e\u0026#125;\u003c/span\u003e\u003c/pre\u003e \u003c/td\u003e \u003c/tr\u003e  This is sample connection interface. You could have numerous templates here.\nYou will be using an object factory pattern here. Your implementer will be taken for a Java Property file. But it can be taken from whatever configuration you like. XML maybe, or a database even.\nLet’s see you connection factory:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"java\" style=\"font-family:monospace;\"\u003e\u003cspan style=\"color: #000000; font-weight: bold;\"\u003epublic\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003eclass\u003c/span\u003e ConnFactory \u003cspan style=\"color: #009900;\"\u003e\u0026#123;\u003c/span\u003e   staticLogger logger =newLogger();publicstaticIConnection getImplementer(){Propertiesprop =newProperties();try{prop.load(newFileInputStream(“conf/implementer.property”));}catch(IOExceptionio){logger.Log(“Could not find property file: “+io.getMessage());}StringimplementerClass =prop.getProperty(“implementer”);Class\u003c?\u003eiConnect =null;try{iConnect =Class.forName(implementerClass);}catch(ClassNotFoundExceptionce){logger.Log(“Class could not be found: “+ce.getMessage());}IConnection connection =null;try{connection =(IConnection)iConnect.newInstance();}catch(IllegalAccessExceptionie){logger.Log(“Illegal access excpetion: “+ie.getMessage());}catch(InstantiationExceptione){logger.Log(“Instatiation exception occured. “+e.getMessage());}returnconnection;}}Easy enough, right? Class.forname will instantiate the class name you have in the property file. This could be something like this: com.packagename.ClassName. Doesn’t matter to you. You can add some typeof checks, or instanceof checks, whatever you like. Or you can use generics.\nLet’s get to the concrete implementation:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"java\" style=\"font-family:monospace;\"\u003e\u003cspan style=\"color: #000000; font-weight: bold;\"\u003epublic\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003eclass\u003c/span\u003e JMSConnectionImpl \u003cspan style=\"color: #000000; font-weight: bold;\"\u003eimplements\u003c/span\u003e IConnection \u003cspan style=\"color: #009900;\"\u003e\u0026#123;\u003c/span\u003e Logger logger \u003cspan style=\"color: #339933;\"\u003e=\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003enew\u003c/span\u003e Logger\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e\u003cspan style=\"color: #339933;\"\u003e;\u003c/span\u003e   publicvoidsendMessage(){Connectionconnection =null;. . . finally{connection.close();}}}Simple enough. Here you have a concrete implementation of your collection and your sender class.\nAnd the simple usage facility of this is… simple too:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"java\" style=\"font-family:monospace;\"\u003e IConnection iConnection \u003cspan style=\"color: #339933;\"\u003e=\u003c/span\u003e ConnFactory.\u003cspan style=\"color: #006633;\"\u003egetImplementer\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color: #009900;\"\u003e\u0026#41;\u003c/span\u003e\u003cspan style=\"color: #339933;\"\u003e;\u003c/span\u003e   iConnection.sendMessage();Simple enough too, right? So what happens here? You have a factory that will give you back any kind of implementation you are writing in you property file. You don’t care what the implementation is in your test. You don’t care what it’s name is. You don’t care what it’s result is. Okay, you care about the result, but that’s another history since you will check that elsewhere.\nThere you go. If any question occurs, please don’t hesitate to ask.\nThanks for reading!\n","title":"JMS Connection setup and Framework","uri":"/2012/03/04/jms-connection-setup-and-framework/"},{"content":"When I see something like this:\n \u003ctd class=\"code\"\u003e \u003cpre class=\"java\" style=\"font-family:monospace;\"\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003epublic\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003eclass\u003c/span\u003e Config \u003cspan style=\"color: #009900;\"\u003e\u0026#123;\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003epublic\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003estatic\u003c/span\u003e \u003cspan style=\"color: #000000; font-weight: bold;\"\u003efinal\u003c/span\u003e string DATABASELINK \u003cspan style=\"color: #339933;\"\u003e=\u003c/span\u003e \u003cspan style=\"color: #0000ff;\"\u003e\"linkhere\"\u003c/span\u003e\u003cspan style=\"color: #339933;\"\u003e;\u003c/span\u003e . . . \u003cspan style=\"color: #009900;\"\u003e\u0026#125;\u003c/span\u003e\u003c/pre\u003e \u003c/td\u003e \u003c/tr\u003e  It sends a small, but chilling shiver down my spine. Just… don’t. There are a lot of possibilities to use configuration in Java. Java property files. Xml. Xml serialization. CSV file. Whatever suits you best, but this? DON’T!\n","title":"Configuration","uri":"/2012/02/27/configuration/"},{"content":"Hello Dear Visitor.\nWelcome to the ramblings of a software test engineer who thinks that he is a tester but feels like a developer but works as a tester but wants to learn programming like a developer… And so on and so fort.\nI will make a journey that has been done by many automation testers. The journey when you decide to step one forward. A friend of mine told me that it is good that a tester learns proper programming and that more should do so. She is right. And I will try to document this journey as best as possible. I will try to write down my experiences with java, android, automation, swing, maven, hudson, selenium, cucumber, TDD, BDD, Agile environment and whatever tool I meet.\nPlease forgive me if you find some grammar errors, English is not my first Language. At least this will be a good opportunity to practice it.\nHave fun.. Enjoy my ramblings.\nGergely.\n","title":"Hello and welcome","uri":"/2012/02/26/hello-and-welcome/"},{"content":"Hello.\nThis is just a quick post to test the working of the code tag.\n \u003ctd class=\"code\"\u003e \u003cpre class=\"ruby\" style=\"font-family:monospace;\"\u003e Given\u003cspan style=\"color:#006600; font-weight:bold;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color:#006600; font-weight:bold;\"\u003e/\u003c/span\u003eThe Action is \u003cspan style=\"color:#006600; font-weight:bold;\"\u003e\u0026#40;\u003c/span\u003e\u003cspan style=\"color:#006600; font-weight:bold;\"\u003e\u0026#91;\u003c/span\u003eA\u003cspan style=\"color:#006600; font-weight:bold;\"\u003e-\u003c/span\u003ez\u003cspan style=\"color:#006600; font-weight:bold;\"\u003e\u0026#93;\u003c/span\u003e\u003cspan style=\"color:#006600; font-weight:bold;\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#006600; font-weight:bold;\"\u003e\u0026#41;\u003c/span\u003e\u003cspan style=\"color:#006600; font-weight:bold;\"\u003e/\u003c/span\u003e\u003cspan style=\"color:#006600; font-weight:bold;\"\u003e\u0026#41;\u003c/span\u003e \u003cspan style=\"color:#9966CC; font-weight:bold;\"\u003edo\u003c/span\u003e \u003cspan style=\"color:#006600; font-weight:bold;\"\u003e|\u003c/span\u003eaction\u003cspan style=\"color:#006600; font-weight:bold;\"\u003e|\u003c/span\u003e \u003cspan style=\"color:#0066ff; font-weight:bold;\"\u003e@action\u003c/span\u003e = action \u003cspan style=\"color:#9966CC; font-weight:bold;\"\u003eend\u003c/span\u003e\u003c/pre\u003e \u003c/td\u003e \u003c/tr\u003e  Perfect!!\nThe name of the plugin is WP-Syntax. The trick is to edit the page in plain HTML. Because the WordPress word editor screws up the \u003c\u003e tags. But surely you already knew that…\n","title":"Testing ‘code’ tag.","uri":"/2012/02/26/testing-code-tag/"}]
