<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ramblings of a cloud engineer &middot; Ramblings of a cloud engineer</title>

    <meta name="description" content="Ramblings of a Cloud Engineer">

    <meta name="generator" content="Hugo 0.54-DEV" />
    <meta name="twitter:card" content="summary">
    
    <meta name="twitter:title" content="Ramblings of a cloud engineer &middot; Ramblings of a cloud engineer">
    <meta name="twitter:description" content="Ramblings of a Cloud Engineer">

    <meta property="og:type" content="article">
    <meta property="og:title" content="Ramblings of a cloud engineer &middot; Ramblings of a cloud engineer">
    <meta property="og:description" content="Ramblings of a Cloud Engineer">

    <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700|Oxygen:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/pure-min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/grids-responsive-min.css">

    <link rel="stylesheet" href="https://skarlso.github.io//css/all.min.css">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

    <link rel="alternate" type="application/rss+xml" title="Ramblings of a cloud engineer" href="https://skarlso.github.io//index.xml" />
</head>
<body>


<div id="layout" class="pure-g">
    <div class="sidebar pure-u-1 pure-u-md-1-4">
    <div class="header">
        <hgroup>
            <h1 class="brand-title"><a href="https://skarlso.github.io/">Ramblings of a cloud engineer</a></h1>
            <h2 class="brand-tagline"> Ramblings of a Cloud Engineer </h2>
        </hgroup>

        <nav class="nav">
            <ul class="nav-list">
                
                
                
                <li class="nav-item">
                    <a class="pure-button" href="https://skarlso.github.io//index.xml"><i class="fa fa-rss"></i> rss</a>
                </li></br>
                <div style="box-shadow: 0px 0px 0px 1px #000; overflow-y: scroll; height:250px;" align="left">
                    
                        <a href="https://skarlso.github.io/2018/10/29/go-plugin-tutorial/">Extensive tutorial on go-plugin.</a></br>
                    
                        <a href="https://skarlso.github.io/2018/09/17/furnace-plugin-update/">Furnace with a new Plugin System</a></br>
                    
                        <a href="https://skarlso.github.io/2018/09/13/gotp/">TOTP generator with account handling for multiple tokens</a></br>
                    
                        <a href="https://skarlso.github.io/2018/06/08/fork-updater/">Keep your git forks updated all the time</a></br>
                    
                        <a href="https://skarlso.github.io/2018/03/15/kubernetes-distributed-application/">Kubernetes distributed application deployment with sample Face Recognition App</a></br>
                    
                        <a href="https://skarlso.github.io/2018/03/15/kubernetes-distributed-application/">Kubernetes distributed application deployment with sample Face Recognition App</a></br>
                    
                        <a href="https://skarlso.github.io/2018/02/06/go-budapest-meetup/">Go Budapest Meetup</a></br>
                    
                        <a href="https://skarlso.github.io/2018/02/06/go-budapest-meetup/">Go Budapest Meetup</a></br>
                    
                        <a href="https://skarlso.github.io/2018/01/23/nginx-certbot-ansible/">Ansible &#43; Nginx &#43; LetsEncrypt &#43; Wiki &#43; Nagios</a></br>
                    
                        <a href="https://skarlso.github.io/2018/01/23/nginx-certbot-ansible/">Ansible &#43; Nginx &#43; LetsEncrypt &#43; Wiki &#43; Nagios</a></br>
                    
                        <a href="https://skarlso.github.io/2018/01/13/furnace-massive-update/">Huge Furnace Update</a></br>
                    
                        <a href="https://skarlso.github.io/2018/01/13/furnace-massive-update/">Huge Furnace Update</a></br>
                    
                        <a href="https://skarlso.github.io/2017/12/04/commit-build-deploy/">Commit-Build-Deploy With AWS CodeBuild and Lambda</a></br>
                    
                        <a href="https://skarlso.github.io/2017/12/04/commit-build-deploy/">Commit-Build-Deploy With AWS CodeBuild and Lambda</a></br>
                    
                        <a href="https://skarlso.github.io/2017/11/06/furnace-ikea-manual/">Furnace Ikea Manual</a></br>
                    
                        <a href="https://skarlso.github.io/2017/11/06/furnace-ikea-manual/">Furnace Ikea Manual</a></br>
                    
                        <a href="https://skarlso.github.io/2017/09/03/furnace-binaries/">Furnace Binaries</a></br>
                    
                        <a href="https://skarlso.github.io/2017/09/03/furnace-binaries/">Furnace Binaries</a></br>
                    
                        <a href="https://skarlso.github.io/2017/05/31/notetaking/">Notetaking</a></br>
                    
                        <a href="https://skarlso.github.io/2017/05/31/notetaking/">Notetaking</a></br>
                    
                        <a href="https://skarlso.github.io/2017/05/28/replace-eval-with-object-send-and-a-parser/">Replacing Eval with Object.send and a self written Parser</a></br>
                    
                        <a href="https://skarlso.github.io/2017/05/28/replace-eval-with-object-send-and-a-parser/">Replacing Eval with Object.send and a self written Parser</a></br>
                    
                        <a href="https://skarlso.github.io/2017/04/16/building-furnace-part-4/">Furnace - The building of an AWS CLI Tool for CloudFormation and CodeDeploy - Part 4</a></br>
                    
                        <a href="https://skarlso.github.io/2017/04/16/building-furnace-part-4/">Furnace - The building of an AWS CLI Tool for CloudFormation and CodeDeploy - Part 4</a></br>
                    
                        <a href="https://skarlso.github.io/2017/03/22/building-furnace-part-3/">Furnace - The building of an AWS CLI Tool for CloudFormation and CodeDeploy - Part 3</a></br>
                    
                        <a href="https://skarlso.github.io/2017/03/22/building-furnace-part-3/">Furnace - The building of an AWS CLI Tool for CloudFormation and CodeDeploy - Part 3</a></br>
                    
                        <a href="https://skarlso.github.io/2017/03/19/building-furnace-part-2/">Furnace - The building of an AWS CLI Tool for CloudFormation and CodeDeploy - Part 2</a></br>
                    
                        <a href="https://skarlso.github.io/2017/03/19/building-furnace-part-2/">Furnace - The building of an AWS CLI Tool for CloudFormation and CodeDeploy - Part 2</a></br>
                    
                        <a href="https://skarlso.github.io/2017/03/17/test-new-hugo/">Testing new Hugo if posts are generated properly</a></br>
                    
                        <a href="https://skarlso.github.io/2017/03/17/test-new-hugo/">Testing new Hugo if posts are generated properly</a></br>
                    
                        <a href="https://skarlso.github.io/2017/03/16/building-furnace-part-1/">Furnace - The building of an AWS CLI Tool for CloudFormation and CodeDeploy - Part 1</a></br>
                    
                        <a href="https://skarlso.github.io/2017/03/16/building-furnace-part-1/">Furnace - The building of an AWS CLI Tool for CloudFormation and CodeDeploy - Part 1</a></br>
                    
                        <a href="https://skarlso.github.io/2017/03/03/images-on-old-posts/">Images on older posts</a></br>
                    
                        <a href="https://skarlso.github.io/2017/03/03/images-on-old-posts/">Images on older posts</a></br>
                    
                        <a href="https://skarlso.github.io/2017/02/15/how-to-https-with-hugo-letsencrypt-haproxy/">How to HTTPS with Hugo LetsEncrypt and HAProxy</a></br>
                    
                        <a href="https://skarlso.github.io/2017/02/15/how-to-https-with-hugo-letsencrypt-haproxy/">How to HTTPS with Hugo LetsEncrypt and HAProxy</a></br>
                    
                        <a href="https://skarlso.github.io/2016/11/02/google-signin-with-go-part2/">How to do Google Sign-In with Go - Part 2</a></br>
                    
                        <a href="https://skarlso.github.io/2016/11/02/google-signin-with-go-part2/">How to do Google Sign-In with Go - Part 2</a></br>
                    
                        <a href="https://skarlso.github.io/2016/10/06/rscrap-ruby-scraping-with-cronjob-scripts/">RScrap scraper</a></br>
                    
                        <a href="https://skarlso.github.io/2016/10/06/rscrap-ruby-scraping-with-cronjob-scripts/">RScrap scraper</a></br>
                    
                        <a href="https://skarlso.github.io/2016/09/17/simple-hometheater-with-remote-and-flirc/">Budget Home Theather with a Headless Raspberry Pi and Flirc for Remote Controlling</a></br>
                    
                        <a href="https://skarlso.github.io/2016/09/17/simple-hometheater-with-remote-and-flirc/">Budget Home Theather with a Headless Raspberry Pi and Flirc for Remote Controlling</a></br>
                    
                        <a href="https://skarlso.github.io/2016/08/19/always-go-with-bytes/">Always Go with []byte</a></br>
                    
                        <a href="https://skarlso.github.io/2016/08/19/always-go-with-bytes/">Always Go with []byte</a></br>
                    
                        <a href="https://skarlso.github.io/2016/08/16/never-changing-regex/">Global variable for never changing regex</a></br>
                    
                        <a href="https://skarlso.github.io/2016/08/16/never-changing-regex/">Global variable for never changing regex</a></br>
                    
                        <a href="https://skarlso.github.io/2016/08/13/drupal-missing-toolbar-and-settings-not-saving/">Drupal missing ToolBar and settings not saving</a></br>
                    
                        <a href="https://skarlso.github.io/2016/08/13/drupal-missing-toolbar-and-settings-not-saving/">Drupal missing ToolBar and settings not saving</a></br>
                    
                        <a href="https://skarlso.github.io/2016/07/28/jenkins-best-practices/">Jenkins Best Practices Talk</a></br>
                    
                        <a href="https://skarlso.github.io/2016/07/28/jenkins-best-practices/">Jenkins Best Practices Talk</a></br>
                    
                        <a href="https://skarlso.github.io/2016/07/12/ruby-sieve/">Ruby Sieve</a></br>
                    
                        <a href="https://skarlso.github.io/2016/07/12/ruby-sieve/">Ruby Sieve</a></br>
                    
                        <a href="https://skarlso.github.io/2016/07/12/simple-hook-to-rid-of-trouble/">Simple hook to rid of trouble</a></br>
                    
                        <a href="https://skarlso.github.io/2016/07/12/simple-hook-to-rid-of-trouble/">Simple hook to rid of trouble</a></br>
                    
                        <a href="https://skarlso.github.io/2016/06/12/google-signin-with-go/">How to do Google sign-in with Go</a></br>
                    
                        <a href="https://skarlso.github.io/2016/06/12/google-signin-with-go/">How to do Google sign-in with Go</a></br>
                    
                        <a href="https://skarlso.github.io/2016/04/17/minecraft-server-aws-s3-backup-part2/">Minecraft world automatic backup to AWS S3 bucket - Part 2 (Custom functions)</a></br>
                    
                        <a href="https://skarlso.github.io/2016/04/17/minecraft-server-aws-s3-backup-part2/">Minecraft world automatic backup to AWS S3 bucket - Part 2 (Custom functions)</a></br>
                    
                        <a href="https://skarlso.github.io/2016/04/16/minecraft-server-aws-s3-backup/">Minecraft world automatic backup to AWS S3 bucket</a></br>
                    
                        <a href="https://skarlso.github.io/2016/04/16/minecraft-server-aws-s3-backup/">Minecraft world automatic backup to AWS S3 bucket</a></br>
                    
                        <a href="https://skarlso.github.io/2016/03/29/minecraft-server-with-docker-and-osx/">Minecraft Server with Docker on OSX &#43; Mods</a></br>
                    
                        <a href="https://skarlso.github.io/2016/03/29/minecraft-server-with-docker-and-osx/">Minecraft Server with Docker on OSX &#43; Mods</a></br>
                    
                        <a href="https://skarlso.github.io/2016/03/09/wercker-fixed/">Wercker Fixed</a></br>
                    
                        <a href="https://skarlso.github.io/2016/03/09/wercker-fixed/">Wercker Fixed</a></br>
                    
                        <a href="https://skarlso.github.io/2016/03/04/wercker-test-2/">Wercker Test</a></br>
                    
                        <a href="https://skarlso.github.io/2016/03/04/wercker-test-2/">Wercker Test</a></br>
                    
                        <a href="https://skarlso.github.io/2016/02/10/hugo-autodeploy-with-wercker/">Hugo Autodeploy with Wercker and Github - Pages</a></br>
                    
                        <a href="https://skarlso.github.io/2016/02/10/hugo-autodeploy-with-wercker/">Hugo Autodeploy with Wercker and Github - Pages</a></br>
                    
                        <a href="https://skarlso.github.io/2016/02/10/wercker-test/">Wercker Test</a></br>
                    
                        <a href="https://skarlso.github.io/2016/02/10/wercker-test/">Wercker Test</a></br>
                    
                        <a href="https://skarlso.github.io/2016/02/02/doing-cors-in-go-with-gin-and-json/">Doing CORS in Go with Gin and JSON</a></br>
                    
                        <a href="https://skarlso.github.io/2016/02/02/doing-cors-in-go-with-gin-and-json/">Doing CORS in Go with Gin and JSON</a></br>
                    
                        <a href="https://skarlso.github.io/2016/01/22/my-journey-in-advent-of-code/">My Journey in advent of code</a></br>
                    
                        <a href="https://skarlso.github.io/2016/01/22/my-journey-in-advent-of-code/">My Journey in advent of code</a></br>
                    
                        <a href="https://skarlso.github.io/2016/01/05/improving-performance-with-byte-slice-and-int-map/">Improving performance with byte slice and int map</a></br>
                    
                        <a href="https://skarlso.github.io/2016/01/05/improving-performance-with-byte-slice-and-int-map/">Improving performance with byte slice and int map</a></br>
                    
                        <a href="https://skarlso.github.io/2016/01/01/byte-arrays-and-channels/">Byte arrays and Channels</a></br>
                    
                        <a href="https://skarlso.github.io/2016/01/01/byte-arrays-and-channels/">Byte arrays and Channels</a></br>
                    
                        <a href="https://skarlso.github.io/2015/12/29/use-byte-array-instead-of-strings/">Use Byte Array Instead of Strings</a></br>
                    
                        <a href="https://skarlso.github.io/2015/12/29/use-byte-array-instead-of-strings/">Use Byte Array Instead of Strings</a></br>
                    
                        <a href="https://skarlso.github.io/2015/12/29/use-byte-slice-instead-of-strings/">Use Byte Slice Instead of Strings</a></br>
                    
                        <a href="https://skarlso.github.io/2015/12/29/use-byte-slice-instead-of-strings/">Use Byte Slice Instead of Strings</a></br>
                    
                        <a href="https://skarlso.github.io/2015/12/23/recursive-freq-count/">Recursive Letter Frequency Count</a></br>
                    
                        <a href="https://skarlso.github.io/2015/12/23/recursive-freq-count/">Recursive Letter Frequency Count</a></br>
                    
                        <a href="https://skarlso.github.io/2015/12/08/go-development-environment/">Go Development Environment</a></br>
                    
                        <a href="https://skarlso.github.io/2015/12/08/go-development-environment/">Go Development Environment</a></br>
                    
                        <a href="https://skarlso.github.io/2015/12/07/welcome-to-my-new-blog/">Welcome To My New Blog</a></br>
                    
                        <a href="https://skarlso.github.io/2015/12/07/welcome-to-my-new-blog/">Welcome To My New Blog</a></br>
                    
                        <a href="https://skarlso.github.io/2015/11/20/go-jira-api-client/">Go JIRA API client</a></br>
                    
                        <a href="https://skarlso.github.io/2015/11/20/go-jira-api-client/">Go JIRA API client</a></br>
                    
                        <a href="https://skarlso.github.io/2015/11/15/the-one-hundred-day-github-challenge/">The One Hundred Day GitHub Challenge</a></br>
                    
                        <a href="https://skarlso.github.io/2015/11/15/the-one-hundred-day-github-challenge/">The One Hundred Day GitHub Challenge</a></br>
                    
                        <a href="https://skarlso.github.io/2015/11/09/go-progress-quest/">Go Progress Quest</a></br>
                    
                        <a href="https://skarlso.github.io/2015/11/09/go-progress-quest/">Go Progress Quest</a></br>
                    
                        <a href="https://skarlso.github.io/2015/10/26/kill-a-program-on-connecting-to-a-specific-wifi-osx/">Kill a Program on Connecting to a specific WiFi – OSX</a></br>
                    
                        <a href="https://skarlso.github.io/2015/10/26/kill-a-program-on-connecting-to-a-specific-wifi-osx/">Kill a Program on Connecting to a specific WiFi – OSX</a></br>
                    
                        <a href="https://skarlso.github.io/2015/10/15/circular-buffer-in-go/">Circular buffer in Go</a></br>
                    
                        <a href="https://skarlso.github.io/2015/10/15/circular-buffer-in-go/">Circular buffer in Go</a></br>
                    
                        <a href="https://skarlso.github.io/2015/10/15/jenkins-job-dsl-and-groovy-goodness/">Jenkins Job DSL and Groovy goodness</a></br>
                    
                        <a href="https://skarlso.github.io/2015/10/15/jenkins-job-dsl-and-groovy-goodness/">Jenkins Job DSL and Groovy goodness</a></br>
                    
                        <a href="https://skarlso.github.io/2015/10/04/datamunger-kata-with-go/">DataMunger Kata with Go</a></br>
                    
                        <a href="https://skarlso.github.io/2015/10/04/datamunger-kata-with-go/">DataMunger Kata with Go</a></br>
                    
                        <a href="https://skarlso.github.io/2015/10/02/how-to-aggregate-tests-with-jenkins-with-aggregate-plugin-on-non-relating-jobs/">How to Aggregate Tests with Jenkins with Aggregate Plugin on non-relating jobs</a></br>
                    
                        <a href="https://skarlso.github.io/2015/10/02/how-to-aggregate-tests-with-jenkins-with-aggregate-plugin-on-non-relating-jobs/">How to Aggregate Tests with Jenkins with Aggregate Plugin on non-relating jobs</a></br>
                    
                        <a href="https://skarlso.github.io/2015/09/07/i-used-to-have-great-ideas-on-the-toilet-but-i-no-longer-do/">I used to have great ideas on the toilet, but I no longer do.</a></br>
                    
                        <a href="https://skarlso.github.io/2015/09/07/i-used-to-have-great-ideas-on-the-toilet-but-i-no-longer-do/">I used to have great ideas on the toilet, but I no longer do.</a></br>
                    
                        <a href="https://skarlso.github.io/2015/07/30/sieve-of-eratosthenes-in-go/">Sieve of Eratosthenes in Go</a></br>
                    
                        <a href="https://skarlso.github.io/2015/07/30/sieve-of-eratosthenes-in-go/">Sieve of Eratosthenes in Go</a></br>
                    
                        <a href="https://skarlso.github.io/2015/07/22/quick-tip-for-debugging-headless-locally/">Quick Tip for Debugging Headless Locally</a></br>
                    
                        <a href="https://skarlso.github.io/2015/07/22/quick-tip-for-debugging-headless-locally/">Quick Tip for Debugging Headless Locally</a></br>
                    
                        <a href="https://skarlso.github.io/2015/07/19/converting-numbers-into-string-representations/">Converting numbers into string representations</a></br>
                    
                        <a href="https://skarlso.github.io/2015/07/19/converting-numbers-into-string-representations/">Converting numbers into string representations</a></br>
                    
                        <a href="https://skarlso.github.io/2015/07/16/selenium-testing-with-packer-and-vagrant/">Selenium Testing with Packer and Vagrant</a></br>
                    
                        <a href="https://skarlso.github.io/2015/07/16/selenium-testing-with-packer-and-vagrant/">Selenium Testing with Packer and Vagrant</a></br>
                    
                        <a href="https://skarlso.github.io/2015/07/15/bitwise-operator/">Bitwise &amp; Operator</a></br>
                    
                        <a href="https://skarlso.github.io/2015/07/15/bitwise-operator/">Bitwise &amp; Operator</a></br>
                    
                        <a href="https://skarlso.github.io/2015/07/01/packer-0-8-1/">Packer 0.8.1.</a></br>
                    
                        <a href="https://skarlso.github.io/2015/07/01/packer-0-8-1/">Packer 0.8.1.</a></br>
                    
                        <a href="https://skarlso.github.io/2015/06/30/powershell-can-also-be-nice-or-installing-java-silently-and-waiting/">Powershell can also be nice -Or Installing Java silently and waiting</a></br>
                    
                        <a href="https://skarlso.github.io/2015/06/30/powershell-can-also-be-nice-or-installing-java-silently-and-waiting/">Powershell can also be nice -Or Installing Java silently and waiting</a></br>
                    
                        <a href="https://skarlso.github.io/2015/06/27/the-packer-the-windows-and-the-vagrant-box/">The Packer, The Windows, and the Vagrant box</a></br>
                    
                        <a href="https://skarlso.github.io/2015/06/27/the-packer-the-windows-and-the-vagrant-box/">The Packer, The Windows, and the Vagrant box</a></br>
                    
                        <a href="https://skarlso.github.io/2015/06/06/docker-ruby-lotus-go-cd/">Docker &#43; Java &#43; Vagrant&#43; GO.CD</a></br>
                    
                        <a href="https://skarlso.github.io/2015/06/06/docker-ruby-lotus-go-cd/">Docker &#43; Java &#43; Vagrant&#43; GO.CD</a></br>
                    
                        <a href="https://skarlso.github.io/2015/05/21/setting-up-a-new-laptop-with-puppet/">Setting up a new Laptop with Puppet</a></br>
                    
                        <a href="https://skarlso.github.io/2015/05/21/setting-up-a-new-laptop-with-puppet/">Setting up a new Laptop with Puppet</a></br>
                    
                        <a href="https://skarlso.github.io/2015/05/19/busy-building-the-future/">Busy building the future</a></br>
                    
                        <a href="https://skarlso.github.io/2015/05/19/busy-building-the-future/">Busy building the future</a></br>
                    
                        <a href="https://skarlso.github.io/2015/04/21/django-rpg-part-3/">Django – RPG – Part 3</a></br>
                    
                        <a href="https://skarlso.github.io/2015/04/21/django-rpg-part-3/">Django – RPG – Part 3</a></br>
                    
                        <a href="https://skarlso.github.io/2015/04/12/django-rpg-part-2/">Django – RPG – Part 2</a></br>
                    
                        <a href="https://skarlso.github.io/2015/04/12/django-rpg-part-2/">Django – RPG – Part 2</a></br>
                    
                        <a href="https://skarlso.github.io/2015/04/10/django-rpg-2/">Django – RPG – Part 1</a></br>
                    
                        <a href="https://skarlso.github.io/2015/04/10/django-rpg-2/">Django – RPG – Part 1</a></br>
                    
                        <a href="https://skarlso.github.io/2015/04/10/small-python-gui-to-calculate-lever-distance/">Small Python GUI to Calculate Lever Distance</a></br>
                    
                        <a href="https://skarlso.github.io/2015/04/10/small-python-gui-to-calculate-lever-distance/">Small Python GUI to Calculate Lever Distance</a></br>
                    
                        <a href="https://skarlso.github.io/2015/03/15/python-and-my-math-commitment/">Python and my Math commitment</a></br>
                    
                        <a href="https://skarlso.github.io/2015/03/15/python-and-my-math-commitment/">Python and my Math commitment</a></br>
                    
                        <a href="https://skarlso.github.io/2015/03/02/sphere-judge-online-python-kivy-android-app-part-2/">Sphere Judge Online – Python Kivy Android app – Part 2</a></br>
                    
                        <a href="https://skarlso.github.io/2015/03/02/sphere-judge-online-python-kivy-android-app-part-2/">Sphere Judge Online – Python Kivy Android app – Part 2</a></br>
                    
                        <a href="https://skarlso.github.io/2015/02/26/sphere-judge-online-python-kivy-android-app/">Sphere Judge Online – Python Kivy Android app</a></br>
                    
                        <a href="https://skarlso.github.io/2015/02/26/sphere-judge-online-python-kivy-android-app/">Sphere Judge Online – Python Kivy Android app</a></br>
                    
                        <a href="https://skarlso.github.io/2015/02/08/why-lock-picking-is-like-testing/">Why Lock Picking is like Testing</a></br>
                    
                        <a href="https://skarlso.github.io/2015/02/08/why-lock-picking-is-like-testing/">Why Lock Picking is like Testing</a></br>
                    
                        <a href="https://skarlso.github.io/2015/02/01/building-an-rpg-app-with-meteor-part-one-the-struggle/">Building an RPG App with Meteor – Part One – The struggle</a></br>
                    
                        <a href="https://skarlso.github.io/2015/02/01/building-an-rpg-app-with-meteor-part-one-the-struggle/">Building an RPG App with Meteor – Part One – The struggle</a></br>
                    
                        <a href="https://skarlso.github.io/2015/01/29/javascript-web-framework-meteor/">JavaScript Web Framework – Meteor</a></br>
                    
                        <a href="https://skarlso.github.io/2015/01/29/javascript-web-framework-meteor/">JavaScript Web Framework – Meteor</a></br>
                    
                        <a href="https://skarlso.github.io/2015/01/28/when-cucumber-goes-wrong/">When cucumber goes wrong</a></br>
                    
                        <a href="https://skarlso.github.io/2015/01/28/when-cucumber-goes-wrong/">When cucumber goes wrong</a></br>
                    
                        <a href="https://skarlso.github.io/2015/01/27/from-zero-to-hundred-in-four-seconds/">From Zero to Hundred in Four seconds</a></br>
                    
                        <a href="https://skarlso.github.io/2015/01/27/from-zero-to-hundred-in-four-seconds/">From Zero to Hundred in Four seconds</a></br>
                    
                        <a href="https://skarlso.github.io/2015/01/26/why-testers-are-like-scientists/">Why Testers are, like scientists</a></br>
                    
                        <a href="https://skarlso.github.io/2015/01/26/why-testers-are-like-scientists/">Why Testers are, like scientists</a></br>
                    
                        <a href="https://skarlso.github.io/2014/11/15/why-the-expressiveness-of-your-tests-is-important-part-one/">Why the expressiveness of your Tests is important – Part One</a></br>
                    
                        <a href="https://skarlso.github.io/2014/11/15/why-the-expressiveness-of-your-tests-is-important-part-one/">Why the expressiveness of your Tests is important – Part One</a></br>
                    
                        <a href="https://skarlso.github.io/2014/11/07/updating-all-jenkins-jobs-via-jenkins-api-python/">Updating All Jenkins Jobs Via Jenkins API – Python</a></br>
                    
                        <a href="https://skarlso.github.io/2014/11/07/updating-all-jenkins-jobs-via-jenkins-api-python/">Updating All Jenkins Jobs Via Jenkins API – Python</a></br>
                    
                        <a href="https://skarlso.github.io/2014/10/23/the-seven-guidelines-of-context-driven-testing/">The seven Principles of Context Driven Testing – Mind Map</a></br>
                    
                        <a href="https://skarlso.github.io/2014/10/23/the-seven-guidelines-of-context-driven-testing/">The seven Principles of Context Driven Testing – Mind Map</a></br>
                    
                        <a href="https://skarlso.github.io/2014/08/25/python-course-review/">Python Course Review</a></br>
                    
                        <a href="https://skarlso.github.io/2014/08/25/python-course-review/">Python Course Review</a></br>
                    
                        <a href="https://skarlso.github.io/2014/05/31/five-reasons-why-a-tester-should-learn-sql/">Five reasons why a tester should learn SQL</a></br>
                    
                        <a href="https://skarlso.github.io/2014/05/31/five-reasons-why-a-tester-should-learn-sql/">Five reasons why a tester should learn SQL</a></br>
                    
                        <a href="https://skarlso.github.io/2014/05/26/tdd-is-dead-not-really/">TDD is Dead – Not really</a></br>
                    
                        <a href="https://skarlso.github.io/2014/05/26/tdd-is-dead-not-really/">TDD is Dead – Not really</a></br>
                    
                        <a href="https://skarlso.github.io/2014/05/23/five-reasons-why-a-front-end-tester-should-learn-javascript/">Five reasons why a front-end tester should learn Javascript</a></br>
                    
                        <a href="https://skarlso.github.io/2014/05/23/five-reasons-why-a-front-end-tester-should-learn-javascript/">Five reasons why a front-end tester should learn Javascript</a></br>
                    
                        <a href="https://skarlso.github.io/2014/05/20/note-taking-what-when-how-often/">Note taking what when how often</a></br>
                    
                        <a href="https://skarlso.github.io/2014/05/20/note-taking-what-when-how-often/">Note taking what when how often</a></br>
                    
                        <a href="https://skarlso.github.io/2014/05/18/using-chrome-search-engine-multiple-search-params/">Using Chrome Search Engine – Multiple Search Params</a></br>
                    
                        <a href="https://skarlso.github.io/2014/05/18/using-chrome-search-engine-multiple-search-params/">Using Chrome Search Engine – Multiple Search Params</a></br>
                    
                        <a href="https://skarlso.github.io/2014/05/16/how-i-started-running/">How I started Running</a></br>
                    
                        <a href="https://skarlso.github.io/2014/05/16/how-i-started-running/">How I started Running</a></br>
                    
                        <a href="https://skarlso.github.io/2014/04/13/how-the-past-influences-the-present/">How the past influences the present</a></br>
                    
                        <a href="https://skarlso.github.io/2014/04/13/how-the-past-influences-the-present/">How the past influences the present</a></br>
                    
                        <a href="https://skarlso.github.io/2014/02/19/example-when-to-use-the-strategy-pattern/">Example when to use the Strategy Pattern</a></br>
                    
                        <a href="https://skarlso.github.io/2014/02/19/example-when-to-use-the-strategy-pattern/">Example when to use the Strategy Pattern</a></br>
                    
                        <a href="https://skarlso.github.io/2014/02/13/how-to-write-a-blog-why-long-posts-rarely-work/">How to write a blog – Why long posts rarely work</a></br>
                    
                        <a href="https://skarlso.github.io/2014/02/13/how-to-write-a-blog-why-long-posts-rarely-work/">How to write a blog – Why long posts rarely work</a></br>
                    
                        <a href="https://skarlso.github.io/2014/02/11/how-to-check-content-header-on-unknown-number-of-items-python/">How to check content header on unknown number of items – Python</a></br>
                    
                        <a href="https://skarlso.github.io/2014/02/11/how-to-check-content-header-on-unknown-number-of-items-python/">How to check content header on unknown number of items – Python</a></br>
                    
                        <a href="https://skarlso.github.io/2014/02/11/the-method-of-loci-and-how-it-works-for-me/">The method of Loci and how it works for me</a></br>
                    
                        <a href="https://skarlso.github.io/2014/02/11/the-method-of-loci-and-how-it-works-for-me/">The method of Loci and how it works for me</a></br>
                    
                        <a href="https://skarlso.github.io/2014/02/10/why-you-should-learn-using-vim/">Why you should learn using Vim</a></br>
                    
                        <a href="https://skarlso.github.io/2014/02/10/why-you-should-learn-using-vim/">Why you should learn using Vim</a></br>
                    
                        <a href="https://skarlso.github.io/2014/02/09/why-does-it-matter-if-i-sit-in-an-office-or-in-a-park/">Why does it matter if I sit in an office or in a park</a></br>
                    
                        <a href="https://skarlso.github.io/2014/02/09/why-does-it-matter-if-i-sit-in-an-office-or-in-a-park/">Why does it matter if I sit in an office or in a park</a></br>
                    
                        <a href="https://skarlso.github.io/2014/02/04/how-i-recruit-why-tool-requirements-are-stupid/">How I recruit – Why tool requirements are stupid</a></br>
                    
                        <a href="https://skarlso.github.io/2014/02/04/how-i-recruit-why-tool-requirements-are-stupid/">How I recruit – Why tool requirements are stupid</a></br>
                    
                        <a href="https://skarlso.github.io/2013/11/02/why-you-should-care-about-the-quality-of-your-test-code/">Why you should care about the quality of your test code</a></br>
                    
                        <a href="https://skarlso.github.io/2013/11/02/why-you-should-care-about-the-quality-of-your-test-code/">Why you should care about the quality of your test code</a></br>
                    
                        <a href="https://skarlso.github.io/2013/10/11/diary-of-a-bit/">Diary of a Bit</a></br>
                    
                        <a href="https://skarlso.github.io/2013/10/11/diary-of-a-bit/">Diary of a Bit</a></br>
                    
                        <a href="https://skarlso.github.io/2013/08/26/low-tech-why-having-less-will-fell-more/">Low Tech – Why having less will fell more</a></br>
                    
                        <a href="https://skarlso.github.io/2013/08/26/low-tech-why-having-less-will-fell-more/">Low Tech – Why having less will fell more</a></br>
                    
                        <a href="https://skarlso.github.io/2013/07/31/why-not-to-automate-everything/">Why not to automate everything…</a></br>
                    
                        <a href="https://skarlso.github.io/2013/07/31/why-not-to-automate-everything/">Why not to automate everything…</a></br>
                    
                        <a href="https://skarlso.github.io/2013/06/24/sublime-text-5/">Sublime text</a></br>
                    
                        <a href="https://skarlso.github.io/2013/06/24/sublime-text-5/">Sublime text</a></br>
                    
                        <a href="https://skarlso.github.io/2013/04/18/cucumber-jvm-and-afterall/">Cucumber-Jvm And @AfterAll</a></br>
                    
                        <a href="https://skarlso.github.io/2013/04/18/cucumber-jvm-and-afterall/">Cucumber-Jvm And @AfterAll</a></br>
                    
                        <a href="https://skarlso.github.io/2013/04/15/cucumber-test-name-and-tags-on-feature/">Cucumber Test Name and Tags on Feature</a></br>
                    
                        <a href="https://skarlso.github.io/2013/04/15/cucumber-test-name-and-tags-on-feature/">Cucumber Test Name and Tags on Feature</a></br>
                    
                        <a href="https://skarlso.github.io/2013/04/11/groovy-and-grails-course-summary/">Groovy and Grails course summary</a></br>
                    
                        <a href="https://skarlso.github.io/2013/04/11/groovy-and-grails-course-summary/">Groovy and Grails course summary</a></br>
                    
                        <a href="https://skarlso.github.io/2013/04/11/my-history-in-testing/">My history in testing</a></br>
                    
                        <a href="https://skarlso.github.io/2013/04/11/my-history-in-testing/">My history in testing</a></br>
                    
                        <a href="https://skarlso.github.io/2013/01/31/its-all-about-human-interaction/">It’s all about human interaction.</a></br>
                    
                        <a href="https://skarlso.github.io/2013/01/31/its-all-about-human-interaction/">It’s all about human interaction.</a></br>
                    
                        <a href="https://skarlso.github.io/2013/01/21/what-you-really-need-to-become-a-testing-expert/">What you really need to know to become a Testing Expert</a></br>
                    
                        <a href="https://skarlso.github.io/2013/01/21/what-you-really-need-to-become-a-testing-expert/">What you really need to know to become a Testing Expert</a></br>
                    
                        <a href="https://skarlso.github.io/2013/01/04/from-chaos-there-shall-be-order/">From Chaos There Shall Be Order</a></br>
                    
                        <a href="https://skarlso.github.io/2013/01/04/from-chaos-there-shall-be-order/">From Chaos There Shall Be Order</a></br>
                    
                        <a href="https://skarlso.github.io/2012/12/12/methodologies-thought-patterns-lies/">Methodologies, Thought Patterns, Lies</a></br>
                    
                        <a href="https://skarlso.github.io/2012/12/12/methodologies-thought-patterns-lies/">Methodologies, Thought Patterns, Lies</a></br>
                    
                        <a href="https://skarlso.github.io/2012/12/01/teas-testing-exploration-adventure-session/">TEAS: Testing Exploration Adventure Session</a></br>
                    
                        <a href="https://skarlso.github.io/2012/12/01/teas-testing-exploration-adventure-session/">TEAS: Testing Exploration Adventure Session</a></br>
                    
                        <a href="https://skarlso.github.io/2012/10/09/how-to-eliminate-a-parameter-boom/">How to eliminate a parameter boom</a></br>
                    
                        <a href="https://skarlso.github.io/2012/10/09/how-to-eliminate-a-parameter-boom/">How to eliminate a parameter boom</a></br>
                    
                        <a href="https://skarlso.github.io/2012/09/30/89/">Coderetreat London</a></br>
                    
                        <a href="https://skarlso.github.io/2012/09/30/89/">Coderetreat London</a></br>
                    
                        <a href="https://skarlso.github.io/2012/09/20/what-my-brain-is-up-to-the-whole-day/">What my brain is up to the whole day…</a></br>
                    
                        <a href="https://skarlso.github.io/2012/09/20/what-my-brain-is-up-to-the-whole-day/">What my brain is up to the whole day…</a></br>
                    
                        <a href="https://skarlso.github.io/2012/09/09/learning-programming-with-a-visual-mind/">Learning programming with a visual mind</a></br>
                    
                        <a href="https://skarlso.github.io/2012/09/09/learning-programming-with-a-visual-mind/">Learning programming with a visual mind</a></br>
                    
                        <a href="https://skarlso.github.io/2012/07/12/tdd-and-game-of-life/">TDD and Game of Life</a></br>
                    
                        <a href="https://skarlso.github.io/2012/07/12/tdd-and-game-of-life/">TDD and Game of Life</a></br>
                    
                        <a href="https://skarlso.github.io/2012/06/28/journey-into-an-unknown-system/">Journey into an unknown system</a></br>
                    
                        <a href="https://skarlso.github.io/2012/06/28/journey-into-an-unknown-system/">Journey into an unknown system</a></br>
                    
                        <a href="https://skarlso.github.io/2012/06/26/solution-to-wrap-kata/">Solution to Wrap Kata</a></br>
                    
                        <a href="https://skarlso.github.io/2012/06/26/solution-to-wrap-kata/">Solution to Wrap Kata</a></br>
                    
                        <a href="https://skarlso.github.io/2012/06/18/how-to-read-a-professional-book-for-slow-learners/">How to read a professional book for slow learners</a></br>
                    
                        <a href="https://skarlso.github.io/2012/06/18/how-to-read-a-professional-book-for-slow-learners/">How to read a professional book for slow learners</a></br>
                    
                        <a href="https://skarlso.github.io/2012/06/13/dont-throw-exception/">Don’t throw Exception</a></br>
                    
                        <a href="https://skarlso.github.io/2012/06/13/dont-throw-exception/">Don’t throw Exception</a></br>
                    
                        <a href="https://skarlso.github.io/2012/06/13/how-to-write-a-professional-blog/">How to write a professional blog</a></br>
                    
                        <a href="https://skarlso.github.io/2012/06/13/how-to-write-a-professional-blog/">How to write a professional blog</a></br>
                    
                        <a href="https://skarlso.github.io/2012/06/13/making-your-code-understandable/">Making your code understandable</a></br>
                    
                        <a href="https://skarlso.github.io/2012/06/13/making-your-code-understandable/">Making your code understandable</a></br>
                    
                        <a href="https://skarlso.github.io/2012/04/11/getting-dual-card-to-work-on-ubuntu-12-04/">Getting Dual Card to work on Ubuntu 12.04.</a></br>
                    
                        <a href="https://skarlso.github.io/2012/04/11/getting-dual-card-to-work-on-ubuntu-12-04/">Getting Dual Card to work on Ubuntu 12.04.</a></br>
                    
                        <a href="https://skarlso.github.io/2012/03/04/jms-connection-setup-and-framework/">JMS Connection setup and Framework</a></br>
                    
                        <a href="https://skarlso.github.io/2012/03/04/jms-connection-setup-and-framework/">JMS Connection setup and Framework</a></br>
                    
                        <a href="https://skarlso.github.io/2012/02/27/configuration/">Configuration</a></br>
                    
                        <a href="https://skarlso.github.io/2012/02/27/configuration/">Configuration</a></br>
                    
                        <a href="https://skarlso.github.io/2012/02/26/hello-and-welcome/">Hello and welcome</a></br>
                    
                        <a href="https://skarlso.github.io/2012/02/26/hello-and-welcome/">Hello and welcome</a></br>
                    
                        <a href="https://skarlso.github.io/2012/02/26/testing-code-tag/">Testing ‘code’ tag.</a></br>
                    
                        <a href="https://skarlso.github.io/2012/02/26/testing-code-tag/">Testing ‘code’ tag.</a></br>
                    
                </div>
            </ul>
        </nav>
    </div>
</div>


    <div class="content pure-u-1 pure-u-md-3-4">
        <div>
            
            <div class="posts">
                
                <h1 class="content-subhead">29 Oct 2018, 07:01</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://skarlso.github.io/2018/10/29/go-plugin-tutorial/" class="post-title">Extensive tutorial on go-plugin.</a>

                        <p class="post-meta">
                            
                                By <strong class="post-author">hannibal</strong>
                            
                            
                                under 
                                
                                <a class="post-category post-category-Golang" href="https://skarlso.github.io//categories/golang">Golang</a><a class="post-category post-category-go-plugin" href="https://skarlso.github.io//categories/go-plugin">go-plugin</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        

<h1 id="intro">Intro</h1>

<p>If you don&rsquo;t know what go-plugin is, don&rsquo;t worry, here is a small introduction on the subject matter:</p>

<p>Back in the old days when Go didn&rsquo;t have the <code>plugin</code> package, HashiCorp was desperately looking for a way to use plugins.</p>

<p>In the old days, Lua plus Go wasn&rsquo;t really a thing yet, and to be honest, nobody wants to write Lua ( joking!).</p>

<p>And thus Mitchell had this brilliant idea of using RPC over the local network to serve a local interface as something that could easily be implemented with any other language that supported RPC. This sounds convoluted but has many benefits! For example, your code will never crash because of a plugin and the ability to use any language to implement a plugin. Not just Go.</p>

<p>It has been a battle-hardened solution for years now and is being actively used by Terraform, Vault, Consule, and especially Packer. All using go-plugin in order to provide a much needed flexibility. Writing a plugin is easy. Or so they say.</p>

<p>It can get complicated quickly, for example, if you are trying to use GRPC. You can lose sight of what exactly you&rsquo;ll need to implement, where and why; or utilizing various languages or using go-plugins in your own project and extending your CLI with pluggable components.</p>

<p>These are all nothing to sneeze at. Suddenly you&rsquo;ll find yourself with hundreds of lines of code pasted from various examples and yet nothing works. Or worse, it DOES work but you have no idea how. Then you find yourself needing to extend it with a new capability, or you find an elusive bug and can&rsquo;t trace its origins.</p>

<p>Fear not. I&rsquo;ll try to demystify things and draw a clear picture about how it works and how the pieces fit together.</p>

<p>Let&rsquo;s start at the beginning.</p>

<h1 id="basic-plugin">Basic plugin</h1>

<p>Let&rsquo;s start by writing a simple Go GRPC plugin. In fact, we can go through the basic example in the go-plugin’s repository which can be quite confusing when first starting out. We&rsquo;ll go step-by-step, and the switch to GRPC will be much easier!</p>

<h2 id="basic-concepts">Basic concepts</h2>

<h3 id="server">Server</h3>

<p>In the case of plugins, the Server is the one serving the plugin&rsquo;s implementation. This means the server will have to provide the implementation to an interface.</p>

<h3 id="client">Client</h3>

<p>The Client calls the server in order to execute the desired behaviour. The underlying logic will connect to the server running on localhost on a random higher port, call the wanted function’s implementation and wait for a response. Once the response is received provide that back to the calling Client.</p>

<h2 id="implementation">Implementation</h2>

<h3 id="the-main-function">The main function</h3>

<h4 id="logger">Logger</h4>

<p>The plugins defined here use stdout in a special way. If you aren&rsquo;t writing a Go based plugin, you will have to do that yourself by outputting something like this:</p>

<pre><code>1|1|tcp|127.0.0.1:1234|grpc
</code></pre>

<p>We&rsquo;ll come back to this later. Suffice to say the framework will pick this up and will connect to the plugin based on the output. In order to get some output back, we must define a special logger:</p>

<pre><code class="language-go">	// Create an hclog.Logger
	logger := hclog.New(&amp;hclog.LoggerOptions{
		Name:   &quot;plugin&quot;,
		Output: os.Stdout,
		Level:  hclog.Debug,
	})
</code></pre>

<h4 id="newclient">NewClient</h4>

<pre><code class="language-go">	// We're a host! Start by launching the plugin process.
	client := plugin.NewClient(&amp;plugin.ClientConfig{
		HandshakeConfig: handshakeConfig,
		Plugins:         pluginMap,
		Cmd:             exec.Command(&quot;./plugin/greeter&quot;),
		Logger:          logger,
	})
	defer client.Kill()
</code></pre>

<p>What is happening here? Let&rsquo;s see one by one:</p>

<p><code>HandshakeConfig: handshakeConfig,</code>: This part is the handshake configuration of the plugin. It has a nice comment as well.</p>

<pre><code class="language-go">// handshakeConfigs are used to just do a basic handshake between
// a plugin and host. If the handshake fails, a user friendly error is shown.
// This prevents users from executing bad plugins or executing a plugin
// directory. It is a UX feature, not a security feature.
var handshakeConfig = plugin.HandshakeConfig{
	ProtocolVersion:  1,
	MagicCookieKey:   &quot;BASIC_PLUGIN&quot;,
	MagicCookieValue: &quot;hello&quot;,
}
</code></pre>

<p>The <code>ProtocolVersion</code> here is used in order to maintain compatibility with your current plugin versions. It&rsquo;s basically like an API version. If you increase this, you will have two options. Don&rsquo;t accept lower protocol versions nor switch to the version number and use a different client implementation for a lower version than for a higher version. This way you will maintain backwards compatibility.</p>

<p>The <code>MagicCookieKey</code> and <code>MagicCookieValue</code> are used for a basic handshake which the comment is talking about. You have to set this <strong>ONCE</strong> for your application. Never change it again, for if you do, your plugins will no longer work. For uniqueness sake, I suggest using UUID.</p>

<p><code>Cmd</code> is one of the most important parts about a plugin. Basically how plugins work is that they boil down to a compiled binary which is executed and starts an RPC server. This is where you will have to define the binary which will be executed and does all this. Since this is all happening locally, (please keep in mind that Go-plugins only support localhost, and for a good reason), these binaries will most likely sit next to your application&rsquo;s binary or in a pre-configured global location. Something like:  <code>~/.config/my-app/plugins</code>. This is individual for each plugin of course. The plugins can be autoloaded via a discovery function given a path and a glob.</p>

<p>And last but not least is the <code>Plugins</code> map. This map is used in order to identify a plugin called <code>Dispense</code>. This map is globally available and must stay consistent in order for all the plugins to work:</p>

<pre><code class="language-go">// pluginMap is the map of plugins we can dispense.
var pluginMap = map[string]plugin.Pluglin	&quot;greeter&quot;: &amp;example.GreeterPlugin{},
}
</code></pre>

<p>You can see that the key is the name of the plugin and the value is the plugin.</p>

<p>We then proceed to create an RPC client:</p>

<pre><code class="language-go">	// Connect via RPC
	rpcClient, err := client.Client()
	if err != nil {
		log.Fatal(err)
	}
</code></pre>

<p>Nothing fancy about this one&hellip;</p>

<p>Now comes the interesting part:</p>

<pre><code class="language-go">	// Request the plugin
	raw, err := rpcClient.Dispense(&quot;greeter&quot;)
	if err != nil {
		log.Fatal(err)
	}
</code></pre>

<p>What&rsquo;s happening here? Dispense will look in the above created map and search for the plugin. If it cannot find it, it will throw an error at us. If it does find it, it will cast this plugin to an RPC or a GRPC type plugin. Then proceed to create an RPC or a GRPC client out of it.</p>

<p>There is no call yet. This is just creating a client and parsing it to a respective representation.</p>

<p>Now comes the magic:</p>

<pre><code class="language-go">	// We should have a Greeter now! This feels like a normal interface
	// implementation but is in fact over an RPC connection.
	greeter := raw.(example.Greeter)
	fmt.Println(greeter.Greet())
</code></pre>

<p>Here we are type asserting our raw GRPC client into our own plugin type. This is so we can call the respective function on the plugin! Once that&rsquo;s done we will have a {client,struct,implementation} that can be called like a simple function.</p>

<p>The implementation right now comes from greeter_impl.go, but that will change once protoc makes an appearance.</p>

<p>Behind the scenes, go-plugin will do a bunch of hat tricks with multiplexing TCP connections as well as a remote procedure call to our plugin. Our plugin then will run the function, generate some kind of output, and will then send that back for the waiting client.</p>

<p>The client will then proceed to parse the message into a given response type and will then return it back to the client’s callee.</p>

<p>This concludes main.go for now.</p>

<h3 id="the-interface">The Interface</h3>

<p>Now let’s investigate the Interface. The interface is used to provide calling details. This interface will be what defines our plugins’ capabilities. How does our <code>Greeter</code> look like?</p>

<pre><code class="language-go">// Greeter is the interface that we're exposing as a plugin.
type Greeter interface {
	Greet() string
}
</code></pre>

<p>This is pretty simple. It defines a function which will return a string typed value.</p>

<p>Now, we will need a couple of things for this to work. Firstly we need something which defines the RPC workings. go-plugin is working with <code>net/http</code> inside. It also uses something called Yamux for connection multiplexing, but we needn’t worry about this detail.</p>

<p>Implementing the RPC details looks like this:</p>

<pre><code class="language-go">// Here is an implementation that talks over RPC
type GreeterRPC struct {
    client *rpc.Client
}

func (g *GreeterRPC) Greet() string {
	var resp string
	err := g.client.Call(&quot;Plugin.Greet&quot;, new(interface{}), &amp;resp)
	if err != nil {
		// You usually want your interfaces to return errors. If they don't,
		// there isn't much other choice here.
		panic(err)
	}

	return resp
}
</code></pre>

<p>Here the GreeterRPC struct is an RPC specific implementation that will handle communication over RPC. This is Client in this setup.</p>

<p>In case of gRPC, this would look something like this:</p>

<pre><code class="language-go">// GRPCClient is an implementation of KV that talks over RPC.
type GreeterGRPC struct{ client proto.GreeterClient }

func (m *GreeterGRPC) Greet() (string, error) {
    s, err := m.client.Greet(context.Background(), &amp;proto.Empty{})
	return s, err
}
</code></pre>

<p>What is happening here? What&rsquo;s Proto and what is GreeterClient? GRPC uses Google&rsquo;s protoc library to serialize and unserialize data. <code>proto.GreeterClient</code> is generated Go code by protoc. This code is a skeleton for which implementation detail will be replaced on run time. Well, the actual result will be used and not replaced as such.</p>

<p>Back to our previous example. The RPC client calls a specific Plugin function called Greet for which the implementation will be provided by a Server that will be streamed back over the RPC protocol.</p>

<p>The server is pretty easy to follow:</p>

<pre><code class="language-go">// Here is the RPC server that GreeterRPC talks to, conforming to
// the requirements of net/rpc
type GreeterRPCServer struct {
	// This is the real implementation
	Impl Greeter
}
</code></pre>

<p>Impl is the concrete implementation that will be called in the Server&rsquo;s implementation of the Greet plugin. Now we must define Greet on the RPCServer in order for it to be able to call the remote code. This looks like this:</p>

<pre><code class="language-go">func (s *GreeterRPCServer) Greet(args interface{}, resp *string) error {
	*resp = s.Impl.Greet()
	return nil
}
</code></pre>

<p>This is all still boilerplate for the RPC works. Now comes plugin. For this, the comment is actually quite good too:</p>

<pre><code class="language-go">// This is the implementation of plugin.Plugin so we can serve/consume this
//
// This has two methods: Server must return an RPC server for this plugin
// type. We construct a GreeterRPCServer for this.
//
// Client must return an implementation of our interface that communicates
// over an RPC client. We return GreeterRPC for this.
//
// Ignore MuxBroker. That is used to create more multiplexed streams on our
// plugin connection and is a more advanced use case.
type GreeterPlugin struct {
	// Impl Injection
	Impl Greeter
}

func (p *GreeterPlugin) Server(*plugin.MuxBroker) (interface{}, error) {
	return &amp;GreeterRPCServer{Impl: p.Impl}, nil
}

func (GreeterPlugin) Client(b *plugin.MuxBroker, c *rpc.Client) (interface{}, error) {
	return &amp;GreeterRPC{client: c}, nil
}
</code></pre>

<p>What does this mean? So, remember: <code>GreeterRPCServer</code> is the one calling the actual implementation while Client is receiving the result of that call. The <code>GreeterPlugin</code> has the <code>Greeter</code> interface embedded just like the <code>RPCServer</code>. We will use the <code>GreeterPlugin</code> as a struct in the plugin map. This is the plugin that we will actually use.</p>

<p>This is all still common stuff. These things will need to be visible for both. The plugin&rsquo;s implementation will use the interface to see what it needs to implement. The Client will use it see what to call and what API is available. Like, <code>Greet</code>.</p>

<p>How does the implementation look like?</p>

<h3 id="the-implementation">The Implementation</h3>

<p>In a completely separate package, but which still has access to the interface definition, this plugin could be something like this:</p>

<pre><code class="language-go">// Here is a real implementation of Greeter
type GreeterHello struct {
	logger hclog.Logger
}

func (g *GreeterHello) Greet() string {
	g.logger.Debug(&quot;message from GreeterHello.Greet&quot;)
	return &quot;Hello!&quot;
}
</code></pre>

<p>We create a struct and then add the function to it which is defined by the plugin&rsquo;s interface. This interface, since it&rsquo;s required by both parties, could well sit in a common package outside of both programs. Something like a SDK. Both code could import it and use it as a common dependency. This way we have separated the interface from the plugin <strong>and</strong> the calling client.</p>

<p>The <code>main</code> function could look something like this:</p>

<pre><code class="language-go">logger := hclog.New(&amp;hclog.LoggerOptions{
    Level:      hclog.Trace,
    Output:     os.Stderr,
    JSONFormat: true,
})

greeter := &amp;GreeterHello{
    logger: logger,
}
// pluginMap is the map of plugins we can dispense.
var pluginMap = map[string]plugin.Plugin{
    &quot;greeter&quot;: &amp;example.GreeterPlugin{Impl: greeter},
}

logger.Debug(&quot;message from plugin&quot;, &quot;foo&quot;, &quot;bar&quot;)

plugin.Serve(&amp;plugin.ServeConfig{
    HandshakeConfig: handshakeConfig,
    Plugins:         pluginMap,
})
</code></pre>

<p>Notice two things that we need. One is the <code>handshakeConfig</code>. You can either define it here, with the same cookie details as you defined in the client code, or you can extract the handshake information into the SDK. This is up to you.</p>

<p>Then the next interesting thing is the <code>plugin.Serve</code> method. This is where the magic happens. The plugins open up a RPC communication socket and over a hijacked <code>stdout</code>, broadcasts its availability to the calling Client in this format:</p>

<pre><code class="language-bash">CORE-PROTOCOL-VERSION | APP-PROTOCOL-VERSION | NETWORK-TYPE | NETWORK-ADDR | PROTOCOL
</code></pre>

<p>For Go plugins, you don&rsquo;t have to concern yourself with this. <code>go-plugin</code> takes care of all this for you. For non-Go versions, we must take this into account. And before calling serve, we need to output this information to <code>stdout</code>.</p>

<p>For example, a Python plugin must deal with this himself. Like this:</p>

<pre><code class="language-python"># Output information
print(&quot;1|1|tcp|127.0.0.1:1234|grpc&quot;)
sys.stdout.flush()
</code></pre>

<p>For GRPC plugins, it&rsquo;s also mandatory to implement a HealthChecker.</p>

<p>How would all this look like with GRPC?</p>

<p>It gets slightly more complicated but not too much. We need to use <code>protoc</code> to create a protocol description for our implementation, and then we will call that. Let&rsquo;s look at this now by converting the basic greeter example into GRPC.</p>

<h1 id="grpc-basic-plugin">GRPC Basic plugin</h1>

<p>The example that&rsquo;s under GRPC is quite elaborate and perhaps you don&rsquo;t need the Python part. I will focus on the basic RPC example into a GRPC example. That should not be a problem.</p>

<h2 id="the-api">The API</h2>

<p>First and foremost, you will need to define an API to implement with <code>protoc</code>. For our basic example, the protoc file could look like this:</p>

<pre><code class="language-proto3">syntax = &quot;proto3&quot;;
package proto;

message GreetResponse {
    string message = 1;
}

message Empty {}

service GreeterService {
    rpc Greet(Empty) returns (GreetResponse);
}
</code></pre>

<p>The syntax is quite simple and readable. What this defines is a message, which is a response, that will contain a <code>message</code> with the type <code>string</code>. The <code>service</code> defines a service which has a method called <code>Greet</code>. The service definition is basically an interface for which we will be providing the concrete implementation through the plugin.</p>

<p>To read more about protoc, visit this page: <a href="https://developers.google.com/protocol-buffers/">Google Protocol Buffer</a>.</p>

<h2 id="generate-the-code">Generate the code</h2>

<p>Now, with the protoc definition in hand, we need to generate the stubs that the local client implementation can call. That client call will then, through the remote procedure call, call the right function on the server which will have the concrete implementation at the ready. Run it and return the result in the specified format. Because the stub needs to be available by both parties, (the client AND the server), this needs to live in a shared location.</p>

<p>Why? Because the client is calling the stub and the server is implementing the stub. Both need it in order to know what to call/implement.</p>

<p>To generate the code, run this command:</p>

<pre><code class="language-bash">protoc -I proto/ proto/greeter.proto --go_out=plugins=grpc:proto
</code></pre>

<p>I encourage you to read the generated code. Much will make little sense at first. It will have a bunch of structs and defined things that the GRPC package will use in order to server the function. The interesting bits and pieces are:</p>

<pre><code class="language-go">func (m *GreetResponse) GetMessage() string {
	if m != nil {
		return m.Message
	}
	return &quot;&quot;
}
</code></pre>

<p>Which will get use the message inside the response.</p>

<pre><code class="language-go">type GreeterServiceClient interface {
	Greet(ctx context.Context, in *Empty, opts ...grpc.CallOption) (*GreetResponse, error)
}
</code></pre>

<p>This is our ServiceClient interface which defines the Greet function’s topology.</p>

<p>And lastly, this guy:</p>

<pre><code class="language-go">func RegisterGreeterServiceServer(s *grpc.Server, srv GreeterServiceServer) {
	s.RegisterService(&amp;_GreeterService_serviceDesc, srv)
}
</code></pre>

<p>Which we will need in order to register our implementation for the server. We can ignore the rest.</p>

<h2 id="the-interface-1">The interface</h2>

<p>Much like the RPC, we need to define an interface for the client and server to use. This must be in a shared place as both the server and the client need to know about it. You could put this into an SDK and your peers could just get the SDK and implement some function for define and done. The interface definition in the GRPC land could look something like this:</p>

<pre><code class="language-go">// Greeter is the interface that we're exposing as a plugin.
type Greeter interface {
	Greet() string
}

// This is the implementation of plugin.GRPCPlugin so we can serve/consume this.
type GreeterGRPCPlugin struct {
	// GRPCPlugin must still implement the Plugin interface
	plugin.Plugin
	// Concrete implementation, written in Go. This is only used for plugins
	// that are written in Go.
	Impl Greeter
}

func (p *GreeterGRPCPlugin) GRPCServer(broker *plugin.GRPCBroker, s *grpc.Server) error {
	proto.RegisterGreeterServer(s, &amp;GRPCServer{Impl: p.Impl})
	return nil
}

func (p *GreeterGRPCPlugin) GRPCClient(ctx context.Context, broker *plugin.GRPCBroker, c *grpc.ClientConn) (interface{}, error) {
	return &amp;GRPCClient{client: proto.NewGreeterClient(c)}, nil
}
</code></pre>

<p>With this we have the Plugin&rsquo;s implementation for hashicorp what needed to be done. The plugin will call the underlying implementation and serve/consume the plugin. We can now write the GRPC part of it.</p>

<p>Please note that <code>proto</code> is a shared library too where the protocol stubs reside. That needs to be somewhere on the path or in a separate SDK of some sort, but it must be visible.</p>

<h2 id="writing-the-grpc-client">Writing the GRPC Client</h2>

<p>Firstly we define the grpc client struct:</p>

<pre><code class="language-go">// GRPCClient is an implementation of Greeter that talks over RPC.
type GRPCClient struct{ client proto.GreeterClient }
</code></pre>

<p>Then we define how the client will call the remote function:</p>

<pre><code class="language-go">func (m *GRPCClient) Greet() string {
	ret, _ := m.client.Greet(context.Background(), &amp;proto.Empty{})
	return ret.Message
}
</code></pre>

<p>This will take the <code>client</code> in the <code>GRPCClient</code> and will call the method on it. Once that&rsquo;s done we will return to the result <code>Message</code> property which will be <code>Hello!</code>. <code>proto.Empty</code> is an empty struct; we use this if there is no parameter for a defined method or no return value. We can&rsquo;t just leave it blank. <code>protoc</code> needs to be told explicitly that there is no parameter or return value.</p>

<h2 id="writing-the-grpc-server">Writing the GRPC Server</h2>

<p>The server implementation will also be similar. We call <code>Impl</code> here which will have our concrete plugin implementation.</p>

<pre><code class="language-go">// Here is the gRPC server that GRPCClient talks to.
type GRPCServer struct {
	// This is the real implementation
	Impl Greeter
}

func (m *GRPCServer) Greet(
	ctx context.Context,
	req *proto.Empty) *proto.GreeterResponse {
	v := m.Impl.Greet()
	return &amp;proto.GreeterResponse{Message: v}
}
</code></pre>

<p>And we will use the <code>protoc</code> defined message response. <code>v</code> will have the response from <code>Greet</code> which will be <code>Hello!</code> provided by the concrete plugin&rsquo;s implementation. We then transform that into a protoc type by setting the <code>Message</code> property on the <code>GreeterResponse</code> struct provided by the automatically generated protoc stub code.</p>

<p>Easy, right?</p>

<h2 id="writing-the-plugin-itself">Writing the plugin itself</h2>

<p>The whole thing looks much like the RPC implementation with just a few small modifications and changes. This can sit completely outside of everything, or can even be provided by a third party implementor.</p>

<pre><code class="language-go">// Here is a real implementation of KV that writes to a local file with
// the key name and the contents are the value of the key.
type Greeter struct{}

func (Greeter) Greet() error {
	return &quot;Hello!&quot;
}

func main() {
	plugin.Serve(&amp;plugin.ServeConfig{
		HandshakeConfig: shared.Handshake,
		Plugins: map[string]plugin.Plugin{
			&quot;greeter&quot;: &amp;shared.GreeterGRPCPlugin{Impl: &amp;Greeter{}},
		},

		// A non-nil value here enables gRPC serving for this plugin...
		GRPCServer: plugin.DefaultGRPCServer,
	})
}
</code></pre>

<h2 id="calling-it-all-in-the-main">Calling it all in the main</h2>

<p>Once all that is done, the <code>main</code> function looks the same as RPC&rsquo;s main but with some small modifications.</p>

<pre><code class="language-go">	// We're a host. Start by launching the plugin process.
	client := plugin.NewClient(&amp;plugin.ClientConfig{
		HandshakeConfig: shared.Handshake,
		Plugins:         shared.PluginMap,
		Cmd:             exec.Command(&quot;./plugin/greeter&quot;),
		AllowedProtocols: []plugin.Protocol{plugin.ProtocolGRPC},
	})
</code></pre>

<p>The <code>NewClient</code> now defines <code>AllowedProtocols</code> to be <code>ProtocolGRPC</code>. The rest is the same as before calling <code>Dispense</code> and value hinting the plugin to the correct type then calling <code>Greet()</code>.</p>

<h1 id="conclusion">Conclusion</h1>

<p>This is it. We made it! Now our plugin works over GRPC with a defined API by protoc. The plugin&rsquo;s implementation can live where ever we want it to, but it needs some shared data. These are:</p>

<ul>
<li>The generated code by <code>protoc</code></li>
<li>The defined plugin interface</li>
<li>The GRPC Server and Client</li>
</ul>

<p>These need to be visible by both the Client and the Server. The Server here is the plugin. If you are planning on making people be able to extend your application with go-plugin, you should make these available as a separate SDK. So people won&rsquo;t have to include your whole project just to implement an interface and use protoc. In fact, you could also extract the <code>protoc</code> definition into a separate repository so that your SDK can also pull it in.</p>

<p>You will have three repositories:
* Your application;
* The SDK providing the interface and the GRPC Server and Client implementation;
* The protoc definition file and generated skeleton ( for Go based plugins).</p>

<p>Other languages will have to generate their own protoc code, and includ it into the plugin; like the Python implementation example located here: <a href="https://github.com/hashicorp/go-plugin/tree/master/examples/grpc/plugin-python">Go-plugin Python Example</a>. Also, read this documentation carefully: <a href="https://github.com/hashicorp/go-plugin/blob/master/docs/guide-plugin-write-non-go.md">non-go go-plugin</a>. This document will also clarify what <code>1|1|tcp|127.0.0.1:1234|grpc</code> means and will dissipate the confusion around how plugins work.</p>

<p>Lastly, if you would like to have an in-depth explanation about how go-plugin came to be, watch this video by Mitchell:</p>

<p><a href="https://www.youtube.com/watch?v=SRvm3zQQc1Q">go-plugin explanation video</a>.</p>

<p>I must warn you though- it&rsquo;s an hour long. But worth the watch!</p>

<p>That&rsquo;s it. I hope this has helped to clear the confusion around how to use go-plugin.</p>

<p>Happy plugging!</p>

<p>Gergely.</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">17 Sep 2018, 07:01</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://skarlso.github.io/2018/09/17/furnace-plugin-update/" class="post-title">Furnace with a new Plugin System</a>

                        <p class="post-meta">
                            
                                By <strong class="post-author">hannibal</strong>
                            
                            
                                under 
                                
                                <a class="post-category post-category-Golang" href="https://skarlso.github.io//categories/golang">Golang</a><a class="post-category post-category-Furnace" href="https://skarlso.github.io//categories/furnace">Furnace</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <p>Hi.</p>

<p>A quick update, but a very important and interesting one hopefully. Furnace just got a massive boost to its plugin system.</p>

<p>I&rsquo;m using <a href="https://github.com/hashicorp/go-plugin">HashiCorp&rsquo;s Go-Plugins</a> system now to handle plugins. This means one of
two things that are interesting to the plugin author.</p>

<p>One, plugins can be written in any language which is supported by Furnace and supports GRPC. Currently this means that
plugins can be written in the following languages:</p>

<ul>
<li><p>Go</p></li>

<li><p>Python</p></li>

<li><p>Ruby</p></li>
</ul>

<p>Adding new plugins is easy and I&rsquo;m open for suggestions in which language to provide next if the need arrises.</p>

<p>To find out more, please read the README on Furnace about plugins located here: <a href="https://github.com/go-furnace/go-furnace/blob/master/README.md#plugins">Furnace Plugin
System</a>.</p>

<p>I hope to see a bunch of nice plugins pop up here and there if please are interested in writing them. I&rsquo;m listing a couple of
possibilities like, notification after create, or resource cleanup or even preventing the stack from creating in the first place
with a pre-create check for permissions / resource availability / funds constraints.</p>

<p>Have fun writing plugins and making Furnace more powerful then ever.</p>

<p>I&rsquo;m planning on providing some basic plugins that could be used out of the box. Those will probably be in Go though.</p>

<p>Thanks,
Gergely.</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">13 Sep 2018, 08:01</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://skarlso.github.io/2018/09/13/gotp/" class="post-title">TOTP generator with account handling for multiple tokens</a>

                        <p class="post-meta">
                            
                                By <strong class="post-author">hannibal</strong>
                            
                            
                                under 
                                
                                <a class="post-category post-category-Rust" href="https://skarlso.github.io//categories/rust">Rust</a><a class="post-category post-category-totp" href="https://skarlso.github.io//categories/totp">totp</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <p>Hi.</p>

<p>Today, I would like to write about a little tool I put together in Rust.</p>

<p>It&rsquo;s called <a href="https://github.com/Skarlso/totp">gotp</a>. I&rsquo;m calling it gotp mainly
because of crates.io. I published it there as well, you can find it under this
link: <a href="https://crates.io/crates/gotp">crates.io/gotp</a>.</p>

<p>The purpose is clear. It&rsquo;s a totp generator I wrote a while ago in C++ but now
it&rsquo;s in rust. It can generate a token and save it into an account file that is
AES encrypted. The password is never saved, so it&rsquo;s secure enough to use it.</p>

<p>One of it&rsquo;s properties it will have over the c++ implementation is it&rsquo;s safe,
it uses a proper IV and once I&rsquo;m done, it will also support encryption via PGP.</p>

<p>That way a password will no longer be asked, but the gpg-agent will be used instead.</p>

<p>This will give a much needed usability boost in which a password will no longer be
asked for constantly.</p>

<p>There is also a possibility to place the account file into DropBox or Google Drive
and share it between your own devices. This way your home laptop will also have
the same account as your work laptop making it essentially an Authy like service
on the CLI.</p>

<p>Enjoy, and keep an eye out for the PGP update. For more information in usage, please
read the README.</p>

<p>Thank you!
Gergely.</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">08 Jun 2018, 08:01</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://skarlso.github.io/2018/06/08/fork-updater/" class="post-title">Keep your git forks updated all the time</a>

                        <p class="post-meta">
                            
                                By <strong class="post-author">hannibal</strong>
                            
                            
                                under 
                                
                                <a class="post-category post-category-Ruby" href="https://skarlso.github.io//categories/ruby">Ruby</a><a class="post-category post-category-Git" href="https://skarlso.github.io//categories/git">Git</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <p>Hi folks.</p>

<p>Today&rsquo;s is a quick tip for keeping your forks updated.</p>

<p>If you are like me, and have at least a 100 forks in your repository because:
    * You would like to contribute at some point
    * Save it for yourself because you are afraid that it disappears
    * Would like to make modifications for your own benefit
    * Whatever the reason</p>

<p>&hellip;then you probably have a lot of trouble keeping them updated and making sure you always see the latest change.</p>

<p>Upstream can change a lot especially if it&rsquo;s a busy repository.</p>

<p>Fret not. Help is here. This little ruby script will solve your troubles:</p>

<pre><code class="language-ruby">
#!/usr/bin/env ruby

require 'octokit'
require 'logger'

@logger = Logger.new(&quot;output.log&quot;)

def update_fork(repo)
  repo_name = repo.name
  # clone the repository -- octokit doesn't provide this feature as it's a github api library
  @logger.info(&quot;cloning into #{repo.ssh_url}&quot;)
  system(&quot;git clone #{repo.ssh_url} #{repo_name}&quot;)
  # setup upstream for updating
  @logger.info(&quot;setup upstream to #{repo.parent.ssh_url}&quot;)
  system(&quot;cd #{repo_name} &amp;&amp; git remote add upstream #{repo.parent.ssh_url}&quot;)
  # do the update
  @logger.info(&quot;doing the update with push&quot;)
  system(&quot;cd #{repo_name} &amp;&amp; git fetch upstream &amp;&amp; git rebase upstream/master &amp;&amp; git push origin&quot;)
ensure
  # ensure that the folder is cleaned up
  @logger.info(&quot;cleanup: removing the repository folder&quot;)
  system(&quot;rm -fr #{repo_name}&quot;)
end

client = Octokit::Client.new(:access_token =&gt; ENV['GIT_TOKEN'], per_page: 100)
repos = client.repos({}, query: {type: 'owner', sort: 'asc'})

# Go through all the pages and add them to the list of repositories.
repos.concat(client.last_response.rels[:next].get.data)

repos = repos.select{ |r| r.fork }

@logger.info(&quot;going to update '#{repos.length}' repositories&quot;)

repos.each do |repo|
  # get the repositories information
  @logger.info(&quot;updating #{repo.name}&quot;)
  r = client.repository(repo.id)
  update_fork(r)
end
</code></pre>

<p>This script is also available as a Gist located <a href="https://gist.github.com/Skarlso/fd5bd5971a78a5fa9760b31683de690e">here</a>.</p>

<p>Put this into a cron job, or a Jenkins job on a schedule and you should be good to go.</p>

<p>Note two things:
First: <code>ENV['GIT_TOKEN']</code> this should be set to a token which you can acquire by navigating to
<a href="https://github.com/settings/tokens">tokens</a>. Add a token which has <code>repo</code> access.</p>

<p>Second: Obviously this script will push to your local repository. So wherever you run this, make sure git is set-up and can push
to your repository via SSH. This script is using <code>ssh_url</code> for the repositories. It won&rsquo;t ask for a username or a password.</p>

<p>That&rsquo;s it. Enjoy and keep updating.</p>

<p>Thanks for reading</p>

<p>Gergely.</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">15 Mar 2018, 23:01</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://skarlso.github.io/2018/03/15/kubernetes-distributed-application/" class="post-title">Kubernetes distributed application deployment with sample Face Recognition App</a>

                        <p class="post-meta">
                            
                                By <strong class="post-author">hannibal</strong>
                            
                            
                                under 
                                
                                <a class="post-category post-category-Go" href="https://skarlso.github.io//categories/go">Go</a><a class="post-category post-category-Kubernetes" href="https://skarlso.github.io//categories/kubernetes">Kubernetes</a><a class="post-category post-category-FaceRecognition" href="https://skarlso.github.io//categories/facerecognition">FaceRecognition</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        

<h1 id="intro">Intro</h1>

<p>Alright folks. Settle in and get comfortable. This is going to be a long, but hopefully, fun ride.</p>

<p>I&rsquo;m going to deploy a distributed application with <a href="https://kubernetes.io/">Kubernetes</a>. I attempted to create an application that I thought resembled a real world app. Obviously I had to cut some corners due to time and energy constraints.</p>

<p>My focus will be on Kubernetes and deployment.</p>

<p>Shall we delve right in?</p>

<h1 id="the-application">The Application</h1>

<h2 id="tl-dr">TL;DR</h2>

<p><img src="https://skarlso.github.io/img/kube_overview.png" alt="kube overview" /></p>

<p>The application itself consists of six parts. The repository can be found here: <a href="https://github.com/Skarlso/kube-cluster-sample">Kube Cluster Sample</a>.</p>

<p>It’s a face recognition service which identifies images of people, comparing them to known individuals. A simple frontend displays a table of these images whom they belong to. This happens by sending a request to a <a href="https://github.com/Skarlso/kube-cluster-sample/tree/master/receiver">receiver</a>. The request contains a path to an image. This image can sit on an NFS somewhere. The receiver stores this path in the DB (MySQL) and sends a processing request to a queue. The queue uses: <a href="http://nsq.io/">NSQ</a>. The request contains the ID of the saved image.</p>

<p>An <a href="https://github.com/Skarlso/kube-cluster-sample/tree/master/image_processor">Image Processing</a> service is constantly monitoring the queue for jobs to do. The processing consists of the following steps: taking the ID; loading the image; and finally,  sending the image to a <a href="https://github.com/Skarlso/kube-cluster-sample/tree/master/face_recognition">face recognition</a> backend written in Python via <a href="https://grpc.io/">gRPC</a>. If the identification is successful, the backend will return the name of the image corresponding to that person. The image_processor then updates the image’s record with the person’s ID and marks the image as “processed successfully”. If identification is unsuccessful, the image will be left as “pending”. If there was a failure during identification, the image will be flagged as “failed”.</p>

<p>Failed images can be retried  with a cron job, for example:</p>

<p>So how does this all work? Let&rsquo;s check it out .</p>

<h2 id="receiver">Receiver</h2>

<p>The receiver service is the starting point of the process. It&rsquo;s an API which receives a request in the following format:</p>

<pre><code class="language-bash">curl -d '{&quot;path&quot;:&quot;/unknown_images/unknown0001.jpg&quot;}' http://127.0.0.1:8000/image/post
</code></pre>

<p>In this instance, the receiver stores the path using a shared database cluster. The entity will then receive an ID from the database service. This application is based on the model where unique identification for Entity Objects is provided by the persistence layer. Once the ID is procured, the receiver will send a message to NSQ. At this point in the process, the receiver&rsquo;s job is done.</p>

<h2 id="image-processor">Image Processor</h2>

<p>Here is where the excitement begins. When Image Processor first runs it creates two Go routines. These are&hellip;</p>

<h3 id="consume">Consume</h3>

<p>This is an NSQ consumer. It has three integral jobs. Firstly, it listens for messages on the queue. Secondly, when there is a message, it appends the received ID to a thread safe slice of IDs that the second routine processes. And lastly, it signals the second routine that there is work to be do. It does this through <a href="https://golang.org/pkg/sync/#Cond">sync.Condition</a>.</p>

<h3 id="processimages">ProcessImages</h3>

<p>This routine processes a slice of IDs until the slice is drained completely. Once the slice is drained, the routine suspends instead of sleep-waiting on a channel. The processing of a single ID can be seen in the following linear steps:</p>

<ul>
<li>Establish a gRPC connection to the Face Recognition service (explained under Face Recognition)</li>
<li>Retrieve the image record from the database</li>
<li>Setup two functions for the <a href="#circuit-breaker">Circuit Breaker</a>

<ul>
<li>Function 1: The main function which runs  the RPC method call</li>
<li>Function 2: A health check for the Ping of the circuit breaker</li>
</ul></li>
<li>Call Function 1 which sends the path of the image to the face recognition service. This path should be accessible by the face recognition service. Preferably something shared like an NFS</li>
<li>If this call fails, update the image record as FAILED PROCESSING</li>
<li>If it succeeds, an image name should come back which corresponds to a person in the db. It runs a joined SQL query which gets the corresponding person&rsquo;s ID</li>
<li>Update the Image record in the database with PROCESSED status and the ID of the person that image was identified as</li>
</ul>

<p>This service can be replicated. In other words, more than one can run at the same time.</p>

<h3 id="circuit-breaker">Circuit Breaker</h3>

<p>A  system in which replicating resources requires little to no effort, there still can be cases where, for example, the network goes down, or there are communication problems of any kind between two services. I like to implement a little circuit breaker around the gRPC calls for fun.</p>

<p>This is how it works:</p>

<p><img src="https://skarlso.github.io/img/kube_circuit1.png" alt="kube circuit" /></p>

<p>As you can see, once there are 5 unsuccessful calls to the service, the circuit breaker activates, not allowing any more calls to go through. After a configured amount of time, it will send a Ping call to the service to see if it&rsquo;s back up. If that still errors out, it will increase the timeout. If not, it opens the circuit, allowing traffic to proceed.</p>

<h2 id="front-end">Front-End</h2>

<p>This is only a simple table view with Go&rsquo;s own html/template used to render a list of images.</p>

<h2 id="face-recognition">Face Recognition</h2>

<p>Here is where the identification magic happens. I decided to make this a gRPC based service for the  sole purpose of its flexibility. I started writing it in Go but decided that a Python implementation would be much sorter. In fact, excluding the gRPC code, the recognition part is approximately 7 lines of Python code. I&rsquo;m using this fantastic library which contains all the C bindings to OpenCV. <a href="https://github.com/ageitgey/face_recognition">Face Recognition</a>. Having an API contract here means that I can change the implementation anytime as long as it adheres to the contract.</p>

<p>Please note that there exist a great Go library OpenCV. I was about to use it but they had yet to write the C bindings for that part of OpenCV. It&rsquo;s called <a href="https://gocv.io/">GoCV</a>. Check them out! They have some pretty amazing things, like real-time camera feed processing that only needs a couple of lines of code.</p>

<p>The python library is simple in nature. Have a set of images of people you know. I have a folder with a couple of images named, <code>hannibal_1.jpg, hannibal_2.jpg, gergely_1.jpg, john_doe.jpg</code>. In the database I have two tables named, <code>person, person_images</code>. They look like this:</p>

<pre><code class="language-bash">+----+----------+
| id | name     |
+----+----------+
|  1 | Gergely  |
|  2 | John Doe |
|  3 | Hannibal |
+----+----------+
+----+----------------+-----------+
| id | image_name     | person_id |
+----+----------------+-----------+
|  1 | hannibal_1.jpg |         3 |
|  2 | hannibal_2.jpg |         3 |
+----+----------------+-----------+
</code></pre>

<p>The face recognition library returns the name of the image from the known people which matches the person on the unknown image. After that, a simple joined query -like this- will return the person in question.</p>

<pre><code class="language-sql">select person.name, person.id from person inner join person_images as pi on person.id = pi.person_id where image_name = 'hannibal_2.jpg';
</code></pre>

<p>The gRPC call returns the ID of the person which is then used to update the image&rsquo;s ‘person` column.</p>

<h2 id="nsq">NSQ</h2>

<p>NSQ is a nice little Go based queue. It can be scaled and has a minimal footprint on the system. It also has a lookup service that consumers use to receive messages, and a daemon that senders use when sending messages.</p>

<p>NSQ&rsquo;s philosophy is that the daemon should run with the sender application. That way, the sender will send to the localhost only. But the daemon is connected to the lookup service, and that&rsquo;s how they achieve a global queue.</p>

<p>This means that there are as many NSQ daemons deployed as there are senders. Because the daemon has a minuscule resource requirement, it won&rsquo;t interfere with the requirements of the main application.</p>

<h2 id="configuration">Configuration</h2>

<p>In order to be as flexible as possible, as well as making use of Kubernetes&rsquo;s ConfigSet, I&rsquo;m using .env files in development to store configurations like the location of the database service, or NSQ&rsquo;s lookup address. In production- and that means the Kubernetes’s environment- I&rsquo;ll use environment properties.</p>

<h2 id="conclusion-for-the-application">Conclusion for the Application</h2>

<p>And that&rsquo;s all there is to the architecture of the application we are about to deploy. All of its components are changeable and coupled only through the database, a queue and gRPC. This is imperative when deploying a distributed application due to how updating mechanics work. I will cover that part in the Deployment section.</p>

<h1 id="deployment-with-kubernetes">Deployment with Kubernetes</h1>

<h2 id="basics">Basics</h2>

<p>What <strong>is</strong> Kubernetes?</p>

<p>I&rsquo;m going to cover some of the basics here. I won&rsquo;t go too much into detail-  that would require a whole book like this one: <a href="http://shop.oreilly.com/product/0636920043874.do">Kubernetes Up And Running</a>. Also, if you’re daring enough, you can have a look through this documentation: <a href="https://kubernetes.io/docs/">Kubernetes Documentation</a>.</p>

<p>Kubernetes is a containerized service and application manager. It scales easily, employs a swarm of containers, and most importantly, it&rsquo;s highly configurable via yaml based template files. People often compare Kubernetes to Docker swarm, but Kubernetes does way more than that! For example: it&rsquo;s container agnostic. You could use LXC with Kubernetes and it would work the same way as you using it with Docker. It provides a layer above managing a cluster of deployed services and applications. How? Let&rsquo;s take a quick look at the building blocks of Kubernetes.</p>

<p>In Kubernetes, you’ll describe a desired state of the application and Kubernetes will do what it can to reach that state. States could be something such as deployed; paused; replicated twice; and so on and so forth.</p>

<p>One of the basics of Kubernetes is that it uses Labels and Annotations for all of its components. Services, Deployments, ReplicaSets, DaemonSets, everything is labelled. Consider the following scenario. In order to identify what pod belongs to what application, a label is used called <code>app: myapp</code>. Let’s assume you have two containers of this application deployed; if you would remove the label <code>app</code> from one of the containers, Kubernetes would only detect one and thus would launch a new instance of <code>myapp</code>.</p>

<h3 id="kubernetes-cluster">Kubernetes Cluster</h3>

<p>For Kuberenetes to work, a Kubernetes cluster needs to be present. Setting that up might be a tad painful, but luckily, help is on hand. Minikube sets up a cluster for us locally with one Node. And AWS has a beta service running in the form of a Kubernetes cluster in which the only thing you need to do is request nodes and define your deployments. The Kubernetes cluster components are documented here: <a href="https://kubernetes.io/docs/concepts/overview/components/">Kubernetes Cluster Components</a>.</p>

<h3 id="nodes">Nodes</h3>

<p>A Node is a worker machine. It can be anything- from a vm to a physical machine- including all sorts of cloud provided vms.</p>

<h3 id="pods">Pods</h3>

<p>Pods are a logically grouped collection of containers, meaning one Pod can potentially house a multitude of containers. A Pod gets its own DNS and virtual IP address after it has been created so Kubernetes can load balancer traffic to it. You rarely need to deal with containers directly. Even when debugging, (like looking at logs), you usually invoke <code>kubectl logs deployment/your-app -f</code> instead of looking at a specific container. Although it is possible with <code>-c container_name</code>. The <code>-f</code> does a tail on the log.</p>

<h3 id="deployments">Deployments</h3>

<p>When creating any kind of resource in Kubernetes, it will use a Deployment in the background. A deployment describes a desired state of the current application. It&rsquo;s an object you can use to update Pods or a Service to be in a different state, do an update, or rollout new version of your app. You don&rsquo;t directly control a ReplicaSet, (as described later), but control the deployment object which creates and manages a ReplicaSet.</p>

<h3 id="services">Services</h3>

<p>By default a Pod will get an IP address. However, since Pods are a volatile thing in Kubernetes, you&rsquo;ll need something more permanent. A queue, mysql, or an internal API, a frontend; these need to be long running and behind a static, unchanging IP or preferably a DNS record.</p>

<p>For this purpose, Kubernetes has Services for which you can define modes of accessibility. Load Balanced, simple IP or internal DNS.</p>

<p>How does Kubernetes know if a service is running correctly? You can configure Health Checks and Availability Checks. A Health Check will check whether a container is running, but that doesn&rsquo;t mean that your service is running. For that, you have the availability check which pings a different endpoint in your application.</p>

<p>Since Services are pretty important, I recommend that you read up on them later here: <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services</a>. Advanced  warning though, this document is quite dense. Twenty four A4 pages of networking, services and discovery. It&rsquo;s also vital to decide whether you want to seriously employ Kubernetes in production.</p>

<h3 id="dns-service-discovery">DNS / Service Discovery</h3>

<p>If you create a service in the cluster, that service will get a DNS record in Kubernetes provided by special Kubernetes deployments called kube-proxy and kube-dns. These two provide service discover inside a cluster. If you have a mysql service running and set <code>clusterIP: none</code>, then everyone in the cluster can reach that service by pinging <code>mysql.default.svc.cluster.local</code>. Where:</p>

<ul>
<li><code>mysql</code> &ndash; is the name of the service</li>
<li><code>default</code> &ndash; is the namespace name</li>
<li><code>svc</code> &ndash; is services</li>
<li><code>cluster.local</code> &ndash; is a local cluster domain</li>
</ul>

<p>The domain can be changed via a custom definition. To access a service outside the cluster, a DNS provider has to be used, and Nginx (for example), to bind an IP address to a record. The public IP address of a service can be queried with the following commands:</p>

<ul>
<li>NodePort &ndash; <code>kubectl get -o jsonpath=&quot;{.spec.ports[0].nodePort}&quot; services mysql</code></li>
<li>LoadBalancer &ndash; <code>kubectl get -o jsonpath=&quot;{.spec.ports[0].LoadBalancer}&quot; services mysql</code></li>
</ul>

<h3 id="template-files">Template Files</h3>

<p>Like Docker Compose, TerraForm or other service management tools, Kubernetes also provides infrastructure describing templates. What that means is that you rarely need  to do anything by hand.</p>

<p>For example, consider the following yaml template which describes an nginx Deployment:</p>

<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment #(1)
metadata: #(2)
  name: nginx-deployment
  labels: #(3)
    app: nginx
spec: #(4)
  replicas: 3 #(5)
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers: #(6)
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
</code></pre>

<p>This is a simple deployment in which we do the following:</p>

<ul>
<li>(1) Define the type of the template with kind</li>
<li>(2) Add metadata that will identify this deployment and every resource that it would create with a label (3)</li>
<li>(4) Then comes the spec which describes the desired state

<ul>
<li>(5) For the nginx app, have 3 replicas</li>
<li>(6) This is the template definition for the containers that this Pod will contain</li>
<li>nginx named container</li>
<li>nginx:1.7.9 image (docker in this case)</li>
<li>exposed ports</li>
</ul></li>
</ul>

<h3 id="replicaset">ReplicaSet</h3>

<p>A ReplicaSet is a low level replication manager. It ensures that the correct number of replicates are running for a application. However, Deployments are at a higher level and should always manage ReplicaSets. You rarely need to use ReplicaSets directly unless you have a fringe case in which you want to control the specifics of replication.</p>

<h3 id="daemonset">DaemonSet</h3>

<p>Remember how I said Kubernetes is using Labels all the time? A DaemonSet is a controller that ensures that at daemonized application is always running on a node with a certain label.</p>

<p>For example: you want all the nodes labelled with <code>logger</code> or <code>mission_critical</code> to run an logger / auditing service daemon. Then you create a DaemonSet and give it a node selector called <code>logger</code> or <code>mission_critical</code>. Kubernetes will look for a node that has that label. Always ensure that it will have an instance of that daemon running on it. Thus everyone running on that node will have access to that daemon locally.</p>

<p>In case of my application, the NSQ daemon could be a DaemonSet. Make sure it&rsquo;s up on a node which has the receiver component running by labelling a node with <code>receiver</code> and specifying a DaemonSet with a <code>receiver</code> application selector.</p>

<p>The DaemonSet has all the benefits of the ReplicaSet. It&rsquo;s scalable and Kubernetes manages it; which means, all life cycle events are handled by Kube ensuring it never dies, and when it does,  it will be immediately replaced.</p>

<h3 id="scaling">Scaling</h3>

<p>It&rsquo;s trivial to scale in Kubernetes. The ReplicaSets take care of the number of instances of a Pod to run- as seen in the nginx deployment with the setting <code>replicas:3</code>. It&rsquo;s up to us to write our application in a way that allows Kubernetes to run multiple copies of it.</p>

<p>Of course the settings are vast. You can specify which replicates must run on what Nodes, or on various waiting times as to how long to wait for an instance to come up. You can read more on this subject here: <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Scaling</a> and here: <a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/scale-interactive/">Interactive Scaling with Kubernetes</a> and of course the details of a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a> which controls all the scaling made possible in Kubernetes.</p>

<h3 id="conclusion-for-kubernetes">Conclusion for Kubernetes</h3>

<p>It&rsquo;s a convenient tool to handle container orchestration. Its unit of work are Pods and it has a layered architecture. The top level layer is Deployments through which you handle all other resources. It&rsquo;s highly configurable. It provides an API for all calls you make, so potentially, instead of running <code>kubectl</code> you can also write your own logic to send information to the Kubernetes API.</p>

<p>It provides support for all major cloud providers natively by now and it&rsquo;s completely open source. Feel free to contribute! And check the code if you would like to have a deeper understanding on how it works: <a href="https://github.com/kubernetes/kubernetes">Kubernetes on Github</a>.</p>

<h2 id="minikube">Minikube</h2>

<p>I&rsquo;m going to use <a href="https://github.com/kubernetes/minikube/">Minikube</a>. Minikube is a local Kubernetes cluster simulator. It&rsquo;s not great in simulating multiple nodes though, but for starting out and local play without any costs, it&rsquo;s great. It uses a VM that can be fine tuned if necessary using VirtualBox and the likes.</p>

<p>All of the kube template files that I&rsquo;ll be using can be found here: <a href="https://github.com/Skarlso/kube-cluster-sample/tree/master/kube_files">Kube files</a>.</p>

<p><strong>NOTE</strong> If, later on, you would like to play with scaling but notice that the replicates are always in <code>Pending</code> state, remember that minikube employs a single node only. It might not allow multiple replicas on the same node, or just plainly ran out of resources to use. You can check available resources with the following command:</p>

<pre><code class="language-bash">kubectl get nodes -o yaml
</code></pre>

<h2 id="building-the-containers">Building the containers</h2>

<p>Kubernetes supports most of the containers out there. I&rsquo;m going to use Docker. For all the services I&rsquo;ve built, there is a Dockerfile included in the repository. I encourage you to study them. Most of them are simple. For the go services, I&rsquo;m using a multi stage build that has been  recently introduced. The Go services are Alpine Linux based. The Face Recognition service is Python. NSQ and MySQL are using their own containers.</p>

<h2 id="context">Context</h2>

<p>Kubernetes uses namespaces. If you don&rsquo;t specify any, it will use the <code>default</code> namespace. I&rsquo;m going to permanently set a context to avoid polluting the default namespace. You do that like this:</p>

<pre><code class="language-bash">❯ kubectl config set-context kube-face-cluster --namespace=face
Context &quot;kube-face-cluster&quot; created.
</code></pre>

<p>You have to also start using the context once it&rsquo;s created, like so:</p>

<pre><code class="language-bash">❯ kubectl config use-context kube-face-cluster
Switched to context &quot;kube-face-cluster&quot;.
</code></pre>

<p>After this, all <code>kubectl</code> commands will use the namespace <code>face</code>.</p>

<h2 id="deploying-the-application">Deploying the Application</h2>

<p>Overview of Pods and Services:</p>

<p><img src="https://skarlso.github.io/img/kube_deployed.png" alt="kube deployed" /></p>

<h3 id="mysql">MySQL</h3>

<p>The first Service I&rsquo;m going to deploy is my database.</p>

<p>I&rsquo;m using the Kubernetes example located here <a href="https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/#deploy-mysql">Kube MySQL</a> which fits my needs. Please note that this file is using a plain password for MYSQL_PASSWORD. I&rsquo;m going to employ a vault as described here <a href="https://kubernetes.io/docs/concepts/configuration/secret/">Kubernetes Secrets</a>.</p>

<p>I&rsquo;ve created a secret locally as described in that document using a secret yaml:</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: kube-face-secret
type: Opaque
data:
  mysql_password: base64codehere
</code></pre>

<p>I created the  base64 code via the following command:</p>

<pre><code class="language-bash">echo -n &quot;ubersecurepassword&quot; | base64
</code></pre>

<p>And, this is what you&rsquo;ll see in my deployment yaml file:</p>

<pre><code class="language-yaml">...
- name: MYSQL_ROOT_PASSWORD
  valueFrom:
    secretKeyRef:
      name: kube-face-secret
      key: mysql_password
...
</code></pre>

<p>Another thing worth mentioning: It&rsquo;s using a volume to persist the database. The volume definition is as follows:</p>

<pre><code class="language-yaml">...
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
...
      volumes:
      - name: mysql-persistent-storage
        persistentVolumeClaim:
          claimName: mysql-pv-claim
...
</code></pre>

<p><code>presistentVolumeClain</code> is key here. This tells Kubernetes that this resource requires a persistent volume. How it&rsquo;s provided is abstracted away from the user. You can be sure that Kubernetes will provide a volume that will always be there. It is similar to Pods. To read up on the details, check out this document: <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes">Kubernetes Persistent Volumes</a>.</p>

<p>Deploying the mysql Service is done with the following command:</p>

<pre><code class="language-bash">kubectl apply -f mysql.yaml
</code></pre>

<p><code>apply</code> vs <code>create</code>. In short, <code>apply</code> is considered a declarative object configuration command while <code>create</code> is imperative. What this means for now is that ‘create’ is usually for a one of tasks, like running something or creating a deployment. While, when using apply, the user doesn&rsquo;t define the action to be taken. That will be defined by Kubernetes based on the current status of the cluster. Thus, when there is no service called <code>mysql</code> and I&rsquo;m calling <code>apply -f mysql.yaml</code> it will create the service. When running again, Kubernetes won&rsquo;t do anything. But if I would run <code>create</code> again it will throw an error saying the service is already created.</p>

<p>For more information, check out the following docs: <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/">Kubernetes Object Management</a>, <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/">Imperative Configuration</a>, <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/">Declarative Configuration</a>.</p>

<p>To see progress information, run:</p>

<pre><code class="language-bash"># Describes the whole process
kubectl describe deployment mysql
# Shows only the pod
kubectl get pods -l app=mysql
</code></pre>

<p>Output should be similar to this:</p>

<pre><code class="language-bash">...
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   mysql-55cd6b9f47 (1/1 replicas created)
...
</code></pre>

<p>Or in case of <code>get pods</code>:</p>

<pre><code class="language-bash">NAME                     READY     STATUS    RESTARTS   AGE
mysql-78dbbd9c49-k6sdv   1/1       Running   0          18s
</code></pre>

<p>To test the instance, run the following snippet:</p>

<pre><code class="language-bash">kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -pyourpasswordhere
</code></pre>

<p><strong>GOTCHA</strong>: If you change the password now, it&rsquo;s not enough to re-apply your yaml file to update the container. Since the DB is persisted, the password will not be changed. You have to delete the whole deployment with <code>kubectl delete -f mysql.yaml</code>.</p>

<p>You should see the following when running a <code>show databases</code>.</p>

<pre><code class="language-bash">If you don't see a command prompt, try pressing enter.
mysql&gt;
mysql&gt;
mysql&gt; show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| kube               |
| mysql              |
| performance_schema |
+--------------------+
4 rows in set (0.00 sec)

mysql&gt; exit
Bye
</code></pre>

<p>You&rsquo;ll also notice that I’ve mounted a file located here: <a href="https://github.com/Skarlso/kube-cluster-sample/blob/master/database_setup.sql">Database Setup SQL</a> into the container. MySQL container automatically executes these. That file will bootstrap some data and the schema I&rsquo;m going to use.</p>

<p>The volume definition is as follows:</p>

<pre><code class="language-yaml">  volumeMounts:
  - name: mysql-persistent-storage
    mountPath: /var/lib/mysql
  - name: bootstrap-script
    mountPath: /docker-entrypoint-initdb.d/database_setup.sql
volumes:
- name: mysql-persistent-storage
  persistentVolumeClaim:
    claimName: mysql-pv-claim
- name: bootstrap-script
  hostPath:
    path: /Users/hannibal/golang/src/github.com/Skarlso/kube-cluster-sample/database_setup.sql
    type: File
</code></pre>

<p>To check if the bootstrap script was successful, run this:</p>

<pre><code class="language-bash">~/golang/src/github.com/Skarlso/kube-cluster-sample/kube_files master*
❯ kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -uroot -pyourpasswordhere kube
If you don't see a command prompt, try pressing enter.

mysql&gt; show tables;
+----------------+
| Tables_in_kube |
+----------------+
| images         |
| person         |
| person_images  |
+----------------+
3 rows in set (0.00 sec)

mysql&gt;
</code></pre>

<p>This concludes the database service setup. Logs for this service can be viewed with the following command:</p>

<pre><code class="language-bash">kubectl logs deployment/mysql -f
</code></pre>

<h3 id="nsq-lookup">NSQ Lookup</h3>

<p>The NSQ Lookup will run as an internal service. It doesn&rsquo;t need access from the outside, so I&rsquo;m setting <code>clusterIP: None</code> which will tell Kubernetes that this service is a headless service. This means that it won&rsquo;t be load balanced, and it won&rsquo;t be a single IP service. The DNS will be based upon service selectors.</p>

<p>Our NSQ Lookup selector is:</p>

<pre><code class="language-yaml">  selector:
    matchLabels:
      app: nsqlookup
</code></pre>

<p>Thus, the internal DNS will look like this: <code>nsqlookup.default.svc.cluster.local</code>.</p>

<p>Headless services are described in detail here: <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services">Headless Service</a>.</p>

<p>Basically it&rsquo;s the same as MySQL, just with slight modifications. As stated earlier, I&rsquo;m using NSQ&rsquo;s own Docker Image called <code>nsqio/nsq</code>. All nsq commands are there, so nsqd will also use this image just with a different command. For nsqlookupd, the command is:</p>

<pre><code class="language-yaml">command: [&quot;/nsqlookupd&quot;]
args: [&quot;--broadcast-address=nsqlookup.default.svc.cluster.local&quot;]
</code></pre>

<p>What&rsquo;s the <code>--broadcast-address</code> for, you might ask? By default, nsqlookup will use the <code>hostname</code> as broadcast address. When the consumer runs a callback it will try to connect to something like: <code>http://nsqlookup-234kf-asdf:4161/lookup?topics=image</code>. Please note that <code>nsqlookup-234kf-asdf</code> is the hostname of the container. By setting the broadcast-address to the internal DNS, the callback will be: <code>http://nsqlookup.default.svc.cluster.local:4161/lookup?topic=images</code>. Which will work as expected.</p>

<p>NSQ Lookup also requires two ports forwarded: One for broadcasting and one for nsqd callback. These are exposed in the Dockerfile, and then utilized in the Kubernetes template. Like this:</p>

<p>In the container template:</p>

<pre><code class="language-yaml">        ports:
        - containerPort: 4160
          hostPort: 4160
        - containerPort: 4161
          hostPort: 4161
</code></pre>

<p>In the service template:</p>

<pre><code class="language-yaml">spec:
  ports:
  - name: tcp
    protocol: TCP
    port: 4160
    targetPort: 4160
  - name: http
    protocol: TCP
    port: 4161
    targetPort: 4161
</code></pre>

<p>Names are required by Kubernetes.</p>

<p>To create this service, I&rsquo;m using the same command as before:</p>

<pre><code class="language-bash">kubectl apply -f nsqlookup.yaml
</code></pre>

<p>This concludes nsqlookupd. Two of the major players are in the sack!</p>

<h3 id="receiver-1">Receiver</h3>

<p>This is a more complex one. The receiver will do three things:</p>

<ul>
<li>Create some deployments;</li>
<li>Create the nsq daemon;</li>
<li>Expose the service to the public.</li>
</ul>

<h4 id="deployments-1">Deployments</h4>

<p>The first deployment it creates is its own. The receiver’s container is <code>skarlso/kube-receiver-alpine</code>.</p>

<h4 id="nsq-daemon">Nsq Daemon</h4>

<p>The receiver starts an nsq daemon. As stated earlier, the receiver runs an nsqd with it-self. It does this so talking to it can happen locally and not over the network. By making the receiver do this, they will end up on the same node.</p>

<p>NSQ daemon also needs some adjustments and parameters.</p>

<pre><code class="language-yaml">        ports:
        - containerPort: 4150
          hostPort: 4150
        - containerPort: 4151
          hostPort: 4151
        env:
        - name: NSQLOOKUP_ADDRESS
          value: nsqlookup.default.svc.cluster.local
        - name: NSQ_BROADCAST_ADDRESS
          value: nsqd.default.svc.cluster.local
        command: [&quot;/nsqd&quot;]
        args: [&quot;--lookupd-tcp-address=$(NSQLOOKUP_ADDRESS):4160&quot;, &quot;--broadcast-address=$(NSQ_BROADCAST_ADDRESS)&quot;]

</code></pre>

<p>You can see that the lookup-tcp-address and the broadcast-address are set. Lookup tcp address is the DNS for the nsqlookupd service. And the broadcast address is necessary, just like with nsqlookupd, so the callbacks are working properly.</p>

<h4 id="public-facing">Public facing</h4>

<p>Now, this is the first time I&rsquo;m deploying a public facing service. There are two options. I could use a LoadBalancer since this API will be under heavy load. And if this would be deployed anywhere in production, then it should be using one.</p>

<p>I&rsquo;m doing this locally though- with one node- so something called a <code>NodePort</code> is enough. A <code>NodePort</code> exposes a service on each node&rsquo;s IP at a static port. If not specified, it will assign a random port on the host between 30000-32767. But it can also be configured to be a specific port, using <code>nodePort</code> in the template file. To reach this service, use <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code>. If more than one node is configured, a LoadBalancer can multiplex them to a single IP.</p>

<p>For further information, check out this document: <a href="https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services---service-types">Publishing Services</a>.</p>

<p>Putting this all together, we&rsquo;ll get a receiver-service for which the template for is as follows:</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: receiver-service
spec:
  ports:
  - protocol: TCP
    port: 8000
    targetPort: 8000
  selector:
    app: receiver
  type: NodePort
</code></pre>

<p>For a fixed nodePort on 8000 a definition of <code>nodePort</code> must be provided:</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: receiver-service
spec:
  ports:
  - protocol: TCP
    port: 8000
    targetPort: 8000
  selector:
    app: receiver
  type: NodePort
  nodePort: 8000
</code></pre>

<h3 id="image-processor-1">Image processor</h3>

<p>The Image Processor is where I&rsquo;m handling passing off images to be identified. It should have access to nsqlookupd, mysql and the gRPC endpoint of the face recognition service. This is actually quite a boring service. In fact, it&rsquo;s not even a service at all. It doesn&rsquo;t expose anything, and thus it&rsquo;s the first deployment only component. For brevity, here is the whole template:</p>

<pre><code class="language-yaml">---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: image-processor-deployment
spec:
  selector:
    matchLabels:
      app: image-processor
  replicas: 1
  template:
    metadata:
      labels:
        app: image-processor
    spec:
      containers:
      - name: image-processor
        image: skarlso/kube-processor-alpine:latest
        env:
        - name: MYSQL_CONNECTION
          value: &quot;mysql.default.svc.cluster.local&quot;
        - name: MYSQL_USERPASSWORD
          valueFrom:
            secretKeyRef:
              name: kube-face-secret
              key: mysql_userpassword
        - name: MYSQL_PORT
          # TIL: If this is 3306 without &quot; kubectl throws an error.
          value: &quot;3306&quot;
        - name: MYSQL_DBNAME
          value: kube
        - name: NSQ_LOOKUP_ADDRESS
          value: &quot;nsqlookup.default.svc.cluster.local:4161&quot;
        - name: GRPC_ADDRESS
          value: &quot;face-recog.default.svc.cluster.local:50051&quot;

</code></pre>

<p>The only interesting points in this file are the multitude of environment properties that are used to configure the application. Note the nsqlookupd address and the grpc address.</p>

<p>To create this deployment, run:</p>

<pre><code class="language-bash">kubectl apply -f image_processor.yaml
</code></pre>

<h3 id="face-recognition-1">Face - Recognition</h3>

<p>The face recognition service does have a service. It&rsquo;s a simple one. Only needed by image-processor. It&rsquo;s template is as follows:</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: face-recog
spec:
  ports:
  - protocol: TCP
    port: 50051
    targetPort: 50051
  selector:
    app: face-recog
  clusterIP: None
</code></pre>

<p>The more interesting part is that it requires two volumes. The two volumes are <code>known_people</code> and <code>unknown_people</code>. Can you guess what they will contain? Yep, images. The <code>known_people</code> volume contains all the images associated to the known people in the database. The <code>unknown_people</code> volume will contain all new images. And that&rsquo;s the path we will need to use when sending images from the receiver; that is wherever the mount point points too, which in my case is <code>/unknown_people</code>. Basically, the path needs to be one that the face recognition service can access.</p>

<p>Now, with Kubernetes and Docker, this is easy. It can be a mounted S3 or some kind of nfs or a local mount from host to guest. The possibilities are endless (around a dozen or so). I&rsquo;m going to use a local mount for the sake of simplicity.</p>

<p>Mounting a volume is done in two parts. Firstly, the Dockerfile has to specify volumes:</p>

<pre><code class="language-Dockerfile">VOLUME [ &quot;/unknown_people&quot;, &quot;/known_people&quot; ]
</code></pre>

<p>Secondly, the Kubernetes template needs add <code>volumeMounts</code> as seen in the MySQL service; the difference being <code>hostPath</code> instead of claimed volume:</p>

<pre><code class="language-yaml">        volumeMounts:
        - name: known-people-storage
          mountPath: /known_people
        - name: unknown-people-storage
          mountPath: /unknown_people
      volumes:
      - name: known-people-storage
        hostPath:
          path: /Users/hannibal/Temp/known_people
          type: Directory
      - name: unknown-people-storage
        hostPath:
          path: /Users/hannibal/Temp/
          type: Directory
</code></pre>

<p>We also need to set the <code>known_people</code> folder config setting for the face recognition service. This is done via an environment property:</p>

<pre><code class="language-yaml">        env:
        - name: KNOWN_PEOPLE
          value: &quot;/known_people&quot;
</code></pre>

<p>Then the Python code will look up images, like this:</p>

<pre><code class="language-python">        known_people = os.getenv('KNOWN_PEOPLE', 'known_people')
        print(&quot;Known people images location is: %s&quot; % known_people)
        images = self.image_files_in_folder(known_people)
</code></pre>

<p>Where <code>image_files_in_folder</code> is:</p>

<pre><code class="language-python">    def image_files_in_folder(self, folder):
        return [os.path.join(folder, f) for f in os.listdir(folder) if re.match(r'.*\.(jpg|jpeg|png)', f, flags=re.I)]
</code></pre>

<p>Neat.</p>

<p>Now, if the receiver receives a request (and sends it off further down the line) similar to the one below&hellip;</p>

<pre><code class="language-bash">curl -d '{&quot;path&quot;:&quot;/unknown_people/unknown220.jpg&quot;}' http://192.168.99.100:30251/image/post
</code></pre>

<p>&hellip;it will look for an image called unknown220.jpg under <code>/unknown_people</code>, locate an image in the known_folder that corresponds to the person in the unknown image and return the name of the image that matches.</p>

<p>Looking at logs, you should see something like this:</p>

<pre><code class="language-bash"># Receiver
❯ curl -d '{&quot;path&quot;:&quot;/unknown_people/unknown219.jpg&quot;}' http://192.168.99.100:30251/image/post
got path: {Path:/unknown_people/unknown219.jpg}
image saved with id: 4
image sent to nsq

# Image Processor
2018/03/26 18:11:21 INF    1 [images/ch] querying nsqlookupd http://nsqlookup.default.svc.cluster.local:4161/lookup?topic=images
2018/03/26 18:11:59 Got a message: 4
2018/03/26 18:11:59 Processing image id:  4
2018/03/26 18:12:00 got person:  Hannibal
2018/03/26 18:12:00 updating record with person id
2018/03/26 18:12:00 done
</code></pre>

<p>And that concludes all of the services that we need to deploy.</p>

<h3 id="frontend">Frontend</h3>

<p>Lastly, there is a small web-app which displays the information in the db for convenience. This is also a public facing service with the same parameters as the receiver.</p>

<p>It looks like this:</p>

<p><img src="https://skarlso.github.io/img/kube-frontend.png" alt="frontend" /></p>

<h3 id="recap">Recap</h3>

<p>We are now at the point in which I’ve deployed a bunch of services. A recap off the commands I’ve used so far:</p>

<pre><code class="language-bash">kubectl apply -f mysql.yaml
kubectl apply -f nsqlookup.yaml
kubectl apply -f receiver.yaml
kubectl apply -f image_processor.yaml
kubectl apply -f face_recognition.yaml
kubectl apply -f frontend.yaml
</code></pre>

<p>These could be in any order since the application does not allocate connections on start. (Except for image_processor&rsquo;s NSQ consumer. But that re-tries.)</p>

<p>Query-ing kube for running pods with <code>kubectl get pods</code> should show something like this if there were no errors:</p>

<pre><code class="language-bash">❯ kubectl get pods
NAME                                          READY     STATUS    RESTARTS   AGE
face-recog-6bf449c6f-qg5tr                    1/1       Running   0          1m
image-processor-deployment-6467468c9d-cvx6m   1/1       Running   0          31s
mysql-7d667c75f4-bwghw                        1/1       Running   0          36s
nsqd-584954c44c-299dz                         1/1       Running   0          26s
nsqlookup-7f5bdfcb87-jkdl7                    1/1       Running   0          11s
receiver-deployment-5cb4797598-sf5ds          1/1       Running   0          26s
</code></pre>

<p>Running <code>minikube service list</code>:</p>

<pre><code class="language-bash">❯ minikube service list
|-------------|----------------------|-----------------------------|
|  NAMESPACE  |         NAME         |             URL             |
|-------------|----------------------|-----------------------------|
| default     | face-recog           | No node port                |
| default     | kubernetes           | No node port                |
| default     | mysql                | No node port                |
| default     | nsqd                 | No node port                |
| default     | nsqlookup            | No node port                |
| default     | receiver-service     | http://192.168.99.100:30251 |
| kube-system | kube-dns             | No node port                |
| kube-system | kubernetes-dashboard | http://192.168.99.100:30000 |
|-------------|----------------------|-----------------------------|
</code></pre>

<h3 id="rolling-update">Rolling update</h3>

<p>What happens during a rolling update?</p>

<p><img src="https://skarlso.github.io/img/kube_rotate.png" alt="kube rotate" /></p>

<p>As it happens during software development, change is requested/needed to some parts of the system. So what happens to our cluster if I change one of its components without breaking the others whilst also maintaining backwards compatibility with no disruption to user experience? Thankfully Kubernetes can help with that.</p>

<p>What I don&rsquo;t like is that the API only handles one image at a time. Unfortunately there is no bulk upload option.</p>

<h4 id="code">Code</h4>

<p>Currently, we have the following code segment dealing with a single image:</p>

<pre><code class="language-go">// PostImage handles a post of an image. Saves it to the database
// and sends it to NSQ for further processing.
func PostImage(w http.ResponseWriter, r *http.Request) {
...
}

func main() {
    router := mux.NewRouter()
    router.HandleFunc(&quot;/image/post&quot;, PostImage).Methods(&quot;POST&quot;)
    log.Fatal(http.ListenAndServe(&quot;:8000&quot;, router))
}
</code></pre>

<p>We have two options: Add a new endpoint with <code>/images/post</code> and make the client use that, or modify the existing one.</p>

<p>The new client code has the advantage in that it can fall back to submitting the old way if the new endpoint isn&rsquo;t available. The old client code, however, doesn&rsquo;t have this advantage so we can&rsquo;t change the way our code works right now. Consider this: You have 90 servers and you do a slow paced rolling update that will take out servers one step at a time whilst doing an update. If an update lasts around a minute, the whole process will take around one and a half hours to complete, (not counting any parallel updates).</p>

<p>During this time, some of your servers will run the new code and some will run the old one. Calls are load balanced, thus you have no control over which servers will be hit. If a client is trying to do a call the new way but hits an old server, the client will fail. The client can try and fallback, but since you eliminated the old version it will not succeed unless it, by mere chance, hits a server with the new code (assuming no sticky sessions are set).</p>

<p>Also, once all your servers are updated, an old client will not be able to use your service any longer.</p>

<p>Now, you can argue that you don&rsquo;t want to keep old versions of your code forever. And that’s true in a sense. That&rsquo;s why we are going to modify the old code to simply call the new one with some slight augmentations. This way, once all clients have been migrated, the code can simply be deleted without any problems.</p>

<h4 id="new-endpoint">New Endpoint</h4>

<p>Let&rsquo;s add a new route method:</p>

<pre><code class="language-go">...
router.HandleFunc(&quot;/images/post&quot;, PostImages).Methods(&quot;POST&quot;)
...
</code></pre>

<p>Updating the old one to call the new one with a modified body looks like this:</p>

<pre><code class="language-go">// PostImage handles a post of an image. Saves it to the database
// and sends it to NSQ for further processing.
func PostImage(w http.ResponseWriter, r *http.Request) {
    var p Path
    err := json.NewDecoder(r.Body).Decode(&amp;p)
    if err != nil {
      fmt.Fprintf(w, &quot;got error while decoding body: %s&quot;, err)
      return
    }
    fmt.Fprintf(w, &quot;got path: %+v\n&quot;, p)
    var ps Paths
    paths := make([]Path, 0)
    paths = append(paths, p)
    ps.Paths = paths
    var pathsJSON bytes.Buffer
    err = json.NewEncoder(&amp;pathsJSON).Encode(ps)
    if err != nil {
      fmt.Fprintf(w, &quot;failed to encode paths: %s&quot;, err)
      return
    }
    r.Body = ioutil.NopCloser(&amp;pathsJSON)
    r.ContentLength = int64(pathsJSON.Len())
    PostImages(w, r)
}
</code></pre>

<p>Well, the naming could be better, but you should get the basic idea. I&rsquo;m modifying the incoming single path by wrapping it into the new format and sending it over to the new endpoint handler. And that&rsquo;s it! There are a few more modifications. To check them out, take a look at this PR: <a href="https://github.com/Skarlso/kube-cluster-sample/pull/1">Rolling Update Bulk Image Path PR</a>.</p>

<p>Now, the receiver can be called in two ways:</p>

<pre><code class="language-bash"># Single Path:
curl -d '{&quot;path&quot;:&quot;unknown4456.jpg&quot;}' http://127.0.0.1:8000/image/post

# Multiple Paths:
curl -d '{&quot;paths&quot;:[{&quot;path&quot;:&quot;unknown4456.jpg&quot;}]}' http://127.0.0.1:8000/images/post
</code></pre>

<p>Here, the client is curl. Normally, if the client is a service, I would modify it that in case the new end-point throws a 404 it would try the old one next.</p>

<p>For brevity, I&rsquo;m not modifying NSQ and the others to handle bulk image processing; they will still receive it one by one. I&rsquo;ll leave that up to you as homework ;)</p>

<h4 id="new-image">New Image</h4>

<p>To perform a rolling update, I must create a new image first from the receiver service.</p>

<pre><code class="language-bash">docker build -t skarlso/kube-receiver-alpine:v1.1 .
</code></pre>

<p>Once this is complete, we can begin rolling out the change.</p>

<h4 id="rolling-update-1">Rolling update</h4>

<p>In Kubernetes, you can configure your rolling update in multiple ways:</p>

<h5 id="manual-update">Manual Update</h5>

<p>If I was using a container version in my config file called <code>v1.0</code>, then doing an update is simply calling:</p>

<pre><code class="language-bash">kubectl rolling-update receiver --image:skarlso/kube-receiver-alpine:v1.1
</code></pre>

<p>If there is a problem during the rollout we can always rollback.</p>

<pre><code class="language-bash">kubectl rolling-update receiver --rollback
</code></pre>

<p>It will set back the previous version. No fuss, no muss.</p>

<h5 id="apply-a-new-configuration-file">Apply a new configuration file</h5>

<p>The problem with by-hand updates is that they aren&rsquo;t in source control.</p>

<p>Consider this: Something has changed, A couple of servers got updated by hand to do a quick “patch fix”, but nobody witnessed it and it wasn’t documented. A new person comes along and does a change to the template and applies the template to the cluster. All the servers are updated, and then all of a sudden there is a service outage.</p>

<p>Long story short, the servers which got updated are written over because the template doesn’t  reflect what has been done manually.</p>

<p>The recommended way is to change the template in order to use the new version, and than apply the template with the <code>apply</code> command.</p>

<p>Kubernetes recommends that a Deployment with ReplicaSets should handle a rollout. This means there must be at least two replicates present for a rolling update. If less than two replicates are present then the update won&rsquo;t work (unless <code>maxUnavailable</code> is set to 1). I increase the replica count in yaml. I also set the new image version for the receiver container.</p>

<pre><code class="language-yaml">  replicas: 2
...
    spec:
      containers:
      - name: receiver
        image: skarlso/kube-receiver-alpine:v1.1
...
</code></pre>

<p>Looking at the progress, this is what you should see :</p>

<pre><code class="language-bash">❯ kubectl rollout status deployment/receiver-deployment
Waiting for rollout to finish: 1 out of 2 new replicas have been updated...
</code></pre>

<p>You can add in additional rollout configuration settings by specifying the <code>strategy</code> part of the template like this:</p>

<pre><code class="language-yaml">  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
</code></pre>

<p>Additional information on rolling update can be found in the below documents: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-a-deployment">Deployment Rolling Update</a>, <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment">Updating a Deployment</a>, <a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#updating-your-application-without-a-service-outage">Manage Deployments</a>, <a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/">Rolling Update using ReplicaController</a>.</p>

<p><strong>NOTE MINIKUBE USERS</strong>: Since we are doing this on a local machine with one node and 1 replica of an application, we have to set <code>maxUnavailable</code> to <code>1</code>; otherwise Kubernetes won&rsquo;t allow the update to happen, and the new version will remain in <code>Pending</code> state. That’s because we aren’t allowing for a services to exist with no running containers; which basically means service outage.</p>

<h3 id="scaling-1">Scaling</h3>

<p>Scaling is dead easy with Kubernetes. Since it&rsquo;s managing the whole cluster, you basically just need to put a number into the template of the desired replicas to use.</p>

<p>This has been a great post so far, but it&rsquo;s getting too long. I&rsquo;m planning on writing a follow-up where I will be truly scaling things up on AWS with multiple nodes and replicas; plus deploying a Kubernetes cluster with <a href="https://github.com/kubernetes/kops">Kops</a>. So stay tuned!</p>

<h3 id="cleanup">Cleanup</h3>

<pre><code class="language-bash">kubectl delete deployments --all
kubectl delete services -all
</code></pre>

<h1 id="final-words">Final Words</h1>

<p>And that’s it ladies and gentlemen. We wrote, deployed, updated and scaled (well, not yet really) a distributed application with Kubernetes.</p>

<p>If you have any questions, please feel free to chat in the comments below. I&rsquo;m happy to answer.</p>

<p>I hope you’ve enjoyed reading this. I know it&rsquo;s quite long; I was thinking of splitting it up multiple posts, but having a cohesive, one page guide is useful and makes it easy to find, save, and print.</p>

<p>Thank you for reading,
Gergely.</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">15 Mar 2018, 23:01</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://skarlso.github.io/2018/03/15/kubernetes-distributed-application/" class="post-title">Kubernetes distributed application deployment with sample Face Recognition App</a>

                        <p class="post-meta">
                            
                                By <strong class="post-author">hannibal</strong>
                            
                            
                                under 
                                
                                <a class="post-category post-category-Go" href="https://skarlso.github.io//categories/go">Go</a><a class="post-category post-category-Kubernetes" href="https://skarlso.github.io//categories/kubernetes">Kubernetes</a><a class="post-category post-category-FaceRecognition" href="https://skarlso.github.io//categories/facerecognition">FaceRecognition</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        

<h1 id="intro">Intro</h1>

<p>Alright folks. Settle in and get comfortable. This is going to be a long, but hopefully, fun ride.</p>

<p>I&rsquo;m going to deploy a distributed application with <a href="https://kubernetes.io/">Kubernetes</a>. I attempted to create an application that I thought resembled a real world app. Obviously I had to cut some corners due to time and energy constraints.</p>

<p>My focus will be on Kubernetes and deployment.</p>

<p>Shall we delve right in?</p>

<h1 id="the-application">The Application</h1>

<h2 id="tl-dr">TL;DR</h2>

<p><img src="https://skarlso.github.io/img/kube_overview.png" alt="kube overview" /></p>

<p>The application itself consists of six parts. The repository can be found here: <a href="https://github.com/Skarlso/kube-cluster-sample">Kube Cluster Sample</a>.</p>

<p>It’s a face recognition service which identifies images of people, comparing them to known individuals. A simple frontend displays a table of these images whom they belong to. This happens by sending a request to a <a href="https://github.com/Skarlso/kube-cluster-sample/tree/master/receiver">receiver</a>. The request contains a path to an image. This image can sit on an NFS somewhere. The receiver stores this path in the DB (MySQL) and sends a processing request to a queue. The queue uses: <a href="http://nsq.io/">NSQ</a>. The request contains the ID of the saved image.</p>

<p>An <a href="https://github.com/Skarlso/kube-cluster-sample/tree/master/image_processor">Image Processing</a> service is constantly monitoring the queue for jobs to do. The processing consists of the following steps: taking the ID; loading the image; and finally,  sending the image to a <a href="https://github.com/Skarlso/kube-cluster-sample/tree/master/face_recognition">face recognition</a> backend written in Python via <a href="https://grpc.io/">gRPC</a>. If the identification is successful, the backend will return the name of the image corresponding to that person. The image_processor then updates the image’s record with the person’s ID and marks the image as “processed successfully”. If identification is unsuccessful, the image will be left as “pending”. If there was a failure during identification, the image will be flagged as “failed”.</p>

<p>Failed images can be retried  with a cron job, for example:</p>

<p>So how does this all work? Let&rsquo;s check it out .</p>

<h2 id="receiver">Receiver</h2>

<p>The receiver service is the starting point of the process. It&rsquo;s an API which receives a request in the following format:</p>

<pre><code class="language-bash">curl -d '{&quot;path&quot;:&quot;/unknown_images/unknown0001.jpg&quot;}' http://127.0.0.1:8000/image/post
</code></pre>

<p>In this instance, the receiver stores the path using a shared database cluster. The entity will then receive an ID from the database service. This application is based on the model where unique identification for Entity Objects is provided by the persistence layer. Once the ID is procured, the receiver will send a message to NSQ. At this point in the process, the receiver&rsquo;s job is done.</p>

<h2 id="image-processor">Image Processor</h2>

<p>Here is where the excitement begins. When Image Processor first runs it creates two Go routines. These are&hellip;</p>

<h3 id="consume">Consume</h3>

<p>This is an NSQ consumer. It has three integral jobs. Firstly, it listens for messages on the queue. Secondly, when there is a message, it appends the received ID to a thread safe slice of IDs that the second routine processes. And lastly, it signals the second routine that there is work to be do. It does this through <a href="https://golang.org/pkg/sync/#Cond">sync.Condition</a>.</p>

<h3 id="processimages">ProcessImages</h3>

<p>This routine processes a slice of IDs until the slice is drained completely. Once the slice is drained, the routine suspends instead of sleep-waiting on a channel. The processing of a single ID can be seen in the following linear steps:</p>

<ul>
<li>Establish a gRPC connection to the Face Recognition service (explained under Face Recognition)</li>
<li>Retrieve the image record from the database</li>
<li>Setup two functions for the <a href="#circuit-breaker">Circuit Breaker</a>

<ul>
<li>Function 1: The main function which runs  the RPC method call</li>
<li>Function 2: A health check for the Ping of the circuit breaker</li>
</ul></li>
<li>Call Function 1 which sends the path of the image to the face recognition service. This path should be accessible by the face recognition service. Preferably something shared like an NFS</li>
<li>If this call fails, update the image record as FAILED PROCESSING</li>
<li>If it succeeds, an image name should come back which corresponds to a person in the db. It runs a joined SQL query which gets the corresponding person&rsquo;s ID</li>
<li>Update the Image record in the database with PROCESSED status and the ID of the person that image was identified as</li>
</ul>

<p>This service can be replicated. In other words, more than one can run at the same time.</p>

<h3 id="circuit-breaker">Circuit Breaker</h3>

<p>A  system in which replicating resources requires little to no effort, there still can be cases where, for example, the network goes down, or there are communication problems of any kind between two services. I like to implement a little circuit breaker around the gRPC calls for fun.</p>

<p>This is how it works:</p>

<p><img src="https://skarlso.github.io/img/kube_circuit1.png" alt="kube circuit" /></p>

<p>As you can see, once there are 5 unsuccessful calls to the service, the circuit breaker activates, not allowing any more calls to go through. After a configured amount of time, it will send a Ping call to the service to see if it&rsquo;s back up. If that still errors out, it will increase the timeout. If not, it opens the circuit, allowing traffic to proceed.</p>

<h2 id="front-end">Front-End</h2>

<p>This is only a simple table view with Go&rsquo;s own html/template used to render a list of images.</p>

<h2 id="face-recognition">Face Recognition</h2>

<p>Here is where the identification magic happens. I decided to make this a gRPC based service for the  sole purpose of its flexibility. I started writing it in Go but decided that a Python implementation would be much sorter. In fact, excluding the gRPC code, the recognition part is approximately 7 lines of Python code. I&rsquo;m using this fantastic library which contains all the C bindings to OpenCV. <a href="https://github.com/ageitgey/face_recognition">Face Recognition</a>. Having an API contract here means that I can change the implementation anytime as long as it adheres to the contract.</p>

<p>Please note that there exist a great Go library OpenCV. I was about to use it but they had yet to write the C bindings for that part of OpenCV. It&rsquo;s called <a href="https://gocv.io/">GoCV</a>. Check them out! They have some pretty amazing things, like real-time camera feed processing that only needs a couple of lines of code.</p>

<p>The python library is simple in nature. Have a set of images of people you know. I have a folder with a couple of images named, <code>hannibal_1.jpg, hannibal_2.jpg, gergely_1.jpg, john_doe.jpg</code>. In the database I have two tables named, <code>person, person_images</code>. They look like this:</p>

<pre><code class="language-bash">+----+----------+
| id | name     |
+----+----------+
|  1 | Gergely  |
|  2 | John Doe |
|  3 | Hannibal |
+----+----------+
+----+----------------+-----------+
| id | image_name     | person_id |
+----+----------------+-----------+
|  1 | hannibal_1.jpg |         3 |
|  2 | hannibal_2.jpg |         3 |
+----+----------------+-----------+
</code></pre>

<p>The face recognition library returns the name of the image from the known people which matches the person on the unknown image. After that, a simple joined query -like this- will return the person in question.</p>

<pre><code class="language-sql">select person.name, person.id from person inner join person_images as pi on person.id = pi.person_id where image_name = 'hannibal_2.jpg';
</code></pre>

<p>The gRPC call returns the ID of the person which is then used to update the image&rsquo;s ‘person` column.</p>

<h2 id="nsq">NSQ</h2>

<p>NSQ is a nice little Go based queue. It can be scaled and has a minimal footprint on the system. It also has a lookup service that consumers use to receive messages, and a daemon that senders use when sending messages.</p>

<p>NSQ&rsquo;s philosophy is that the daemon should run with the sender application. That way, the sender will send to the localhost only. But the daemon is connected to the lookup service, and that&rsquo;s how they achieve a global queue.</p>

<p>This means that there are as many NSQ daemons deployed as there are senders. Because the daemon has a minuscule resource requirement, it won&rsquo;t interfere with the requirements of the main application.</p>

<h2 id="configuration">Configuration</h2>

<p>In order to be as flexible as possible, as well as making use of Kubernetes&rsquo;s ConfigSet, I&rsquo;m using .env files in development to store configurations like the location of the database service, or NSQ&rsquo;s lookup address. In production- and that means the Kubernetes’s environment- I&rsquo;ll use environment properties.</p>

<h2 id="conclusion-for-the-application">Conclusion for the Application</h2>

<p>And that&rsquo;s all there is to the architecture of the application we are about to deploy. All of its components are changeable and coupled only through the database, a queue and gRPC. This is imperative when deploying a distributed application due to how updating mechanics work. I will cover that part in the Deployment section.</p>

<h1 id="deployment-with-kubernetes">Deployment with Kubernetes</h1>

<h2 id="basics">Basics</h2>

<p>What <strong>is</strong> Kubernetes?</p>

<p>I&rsquo;m going to cover some of the basics here. I won&rsquo;t go too much into detail-  that would require a whole book like this one: <a href="http://shop.oreilly.com/product/0636920043874.do">Kubernetes Up And Running</a>. Also, if you’re daring enough, you can have a look through this documentation: <a href="https://kubernetes.io/docs/">Kubernetes Documentation</a>.</p>

<p>Kubernetes is a containerized service and application manager. It scales easily, employs a swarm of containers, and most importantly, it&rsquo;s highly configurable via yaml based template files. People often compare Kubernetes to Docker swarm, but Kubernetes does way more than that! For example: it&rsquo;s container agnostic. You could use LXC with Kubernetes and it would work the same way as you using it with Docker. It provides a layer above managing a cluster of deployed services and applications. How? Let&rsquo;s take a quick look at the building blocks of Kubernetes.</p>

<p>In Kubernetes, you’ll describe a desired state of the application and Kubernetes will do what it can to reach that state. States could be something such as deployed; paused; replicated twice; and so on and so forth.</p>

<p>One of the basics of Kubernetes is that it uses Labels and Annotations for all of its components. Services, Deployments, ReplicaSets, DaemonSets, everything is labelled. Consider the following scenario. In order to identify what pod belongs to what application, a label is used called <code>app: myapp</code>. Let’s assume you have two containers of this application deployed; if you would remove the label <code>app</code> from one of the containers, Kubernetes would only detect one and thus would launch a new instance of <code>myapp</code>.</p>

<h3 id="kubernetes-cluster">Kubernetes Cluster</h3>

<p>For Kuberenetes to work, a Kubernetes cluster needs to be present. Setting that up might be a tad painful, but luckily, help is on hand. Minikube sets up a cluster for us locally with one Node. And AWS has a beta service running in the form of a Kubernetes cluster in which the only thing you need to do is request nodes and define your deployments. The Kubernetes cluster components are documented here: <a href="https://kubernetes.io/docs/concepts/overview/components/">Kubernetes Cluster Components</a>.</p>

<h3 id="nodes">Nodes</h3>

<p>A Node is a worker machine. It can be anything- from a vm to a physical machine- including all sorts of cloud provided vms.</p>

<h3 id="pods">Pods</h3>

<p>Pods are a logically grouped collection of containers, meaning one Pod can potentially house a multitude of containers. A Pod gets its own DNS and virtual IP address after it has been created so Kubernetes can load balancer traffic to it. You rarely need to deal with containers directly. Even when debugging, (like looking at logs), you usually invoke <code>kubectl logs deployment/your-app -f</code> instead of looking at a specific container. Although it is possible with <code>-c container_name</code>. The <code>-f</code> does a tail on the log.</p>

<h3 id="deployments">Deployments</h3>

<p>When creating any kind of resource in Kubernetes, it will use a Deployment in the background. A deployment describes a desired state of the current application. It&rsquo;s an object you can use to update Pods or a Service to be in a different state, do an update, or rollout new version of your app. You don&rsquo;t directly control a ReplicaSet, (as described later), but control the deployment object which creates and manages a ReplicaSet.</p>

<h3 id="services">Services</h3>

<p>By default a Pod will get an IP address. However, since Pods are a volatile thing in Kubernetes, you&rsquo;ll need something more permanent. A queue, mysql, or an internal API, a frontend; these need to be long running and behind a static, unchanging IP or preferably a DNS record.</p>

<p>For this purpose, Kubernetes has Services for which you can define modes of accessibility. Load Balanced, simple IP or internal DNS.</p>

<p>How does Kubernetes know if a service is running correctly? You can configure Health Checks and Availability Checks. A Health Check will check whether a container is running, but that doesn&rsquo;t mean that your service is running. For that, you have the availability check which pings a different endpoint in your application.</p>

<p>Since Services are pretty important, I recommend that you read up on them later here: <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services</a>. Advanced  warning though, this document is quite dense. Twenty four A4 pages of networking, services and discovery. It&rsquo;s also vital to decide whether you want to seriously employ Kubernetes in production.</p>

<h3 id="dns-service-discovery">DNS / Service Discovery</h3>

<p>If you create a service in the cluster, that service will get a DNS record in Kubernetes provided by special Kubernetes deployments called kube-proxy and kube-dns. These two provide service discover inside a cluster. If you have a mysql service running and set <code>clusterIP: none</code>, then everyone in the cluster can reach that service by pinging <code>mysql.default.svc.cluster.local</code>. Where:</p>

<ul>
<li><code>mysql</code> &ndash; is the name of the service</li>
<li><code>default</code> &ndash; is the namespace name</li>
<li><code>svc</code> &ndash; is services</li>
<li><code>cluster.local</code> &ndash; is a local cluster domain</li>
</ul>

<p>The domain can be changed via a custom definition. To access a service outside the cluster, a DNS provider has to be used, and Nginx (for example), to bind an IP address to a record. The public IP address of a service can be queried with the following commands:</p>

<ul>
<li>NodePort &ndash; <code>kubectl get -o jsonpath=&quot;{.spec.ports[0].nodePort}&quot; services mysql</code></li>
<li>LoadBalancer &ndash; <code>kubectl get -o jsonpath=&quot;{.spec.ports[0].LoadBalancer}&quot; services mysql</code></li>
</ul>

<h3 id="template-files">Template Files</h3>

<p>Like Docker Compose, TerraForm or other service management tools, Kubernetes also provides infrastructure describing templates. What that means is that you rarely need  to do anything by hand.</p>

<p>For example, consider the following yaml template which describes an nginx Deployment:</p>

<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment #(1)
metadata: #(2)
  name: nginx-deployment
  labels: #(3)
    app: nginx
spec: #(4)
  replicas: 3 #(5)
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers: #(6)
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
</code></pre>

<p>This is a simple deployment in which we do the following:</p>

<ul>
<li>(1) Define the type of the template with kind</li>
<li>(2) Add metadata that will identify this deployment and every resource that it would create with a label (3)</li>
<li>(4) Then comes the spec which describes the desired state

<ul>
<li>(5) For the nginx app, have 3 replicas</li>
<li>(6) This is the template definition for the containers that this Pod will contain</li>
<li>nginx named container</li>
<li>nginx:1.7.9 image (docker in this case)</li>
<li>exposed ports</li>
</ul></li>
</ul>

<h3 id="replicaset">ReplicaSet</h3>

<p>A ReplicaSet is a low level replication manager. It ensures that the correct number of replicates are running for a application. However, Deployments are at a higher level and should always manage ReplicaSets. You rarely need to use ReplicaSets directly unless you have a fringe case in which you want to control the specifics of replication.</p>

<h3 id="daemonset">DaemonSet</h3>

<p>Remember how I said Kubernetes is using Labels all the time? A DaemonSet is a controller that ensures that at daemonized application is always running on a node with a certain label.</p>

<p>For example: you want all the nodes labelled with <code>logger</code> or <code>mission_critical</code> to run an logger / auditing service daemon. Then you create a DaemonSet and give it a node selector called <code>logger</code> or <code>mission_critical</code>. Kubernetes will look for a node that has that label. Always ensure that it will have an instance of that daemon running on it. Thus everyone running on that node will have access to that daemon locally.</p>

<p>In case of my application, the NSQ daemon could be a DaemonSet. Make sure it&rsquo;s up on a node which has the receiver component running by labelling a node with <code>receiver</code> and specifying a DaemonSet with a <code>receiver</code> application selector.</p>

<p>The DaemonSet has all the benefits of the ReplicaSet. It&rsquo;s scalable and Kubernetes manages it; which means, all life cycle events are handled by Kube ensuring it never dies, and when it does,  it will be immediately replaced.</p>

<h3 id="scaling">Scaling</h3>

<p>It&rsquo;s trivial to scale in Kubernetes. The ReplicaSets take care of the number of instances of a Pod to run- as seen in the nginx deployment with the setting <code>replicas:3</code>. It&rsquo;s up to us to write our application in a way that allows Kubernetes to run multiple copies of it.</p>

<p>Of course the settings are vast. You can specify which replicates must run on what Nodes, or on various waiting times as to how long to wait for an instance to come up. You can read more on this subject here: <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Scaling</a> and here: <a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/scale-interactive/">Interactive Scaling with Kubernetes</a> and of course the details of a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a> which controls all the scaling made possible in Kubernetes.</p>

<h3 id="conclusion-for-kubernetes">Conclusion for Kubernetes</h3>

<p>It&rsquo;s a convenient tool to handle container orchestration. Its unit of work are Pods and it has a layered architecture. The top level layer is Deployments through which you handle all other resources. It&rsquo;s highly configurable. It provides an API for all calls you make, so potentially, instead of running <code>kubectl</code> you can also write your own logic to send information to the Kubernetes API.</p>

<p>It provides support for all major cloud providers natively by now and it&rsquo;s completely open source. Feel free to contribute! And check the code if you would like to have a deeper understanding on how it works: <a href="https://github.com/kubernetes/kubernetes">Kubernetes on Github</a>.</p>

<h2 id="minikube">Minikube</h2>

<p>I&rsquo;m going to use <a href="https://github.com/kubernetes/minikube/">Minikube</a>. Minikube is a local Kubernetes cluster simulator. It&rsquo;s not great in simulating multiple nodes though, but for starting out and local play without any costs, it&rsquo;s great. It uses a VM that can be fine tuned if necessary using VirtualBox and the likes.</p>

<p>All of the kube template files that I&rsquo;ll be using can be found here: <a href="https://github.com/Skarlso/kube-cluster-sample/tree/master/kube_files">Kube files</a>.</p>

<p><strong>NOTE</strong> If, later on, you would like to play with scaling but notice that the replicates are always in <code>Pending</code> state, remember that minikube employs a single node only. It might not allow multiple replicas on the same node, or just plainly ran out of resources to use. You can check available resources with the following command:</p>

<pre><code class="language-bash">kubectl get nodes -o yaml
</code></pre>

<h2 id="building-the-containers">Building the containers</h2>

<p>Kubernetes supports most of the containers out there. I&rsquo;m going to use Docker. For all the services I&rsquo;ve built, there is a Dockerfile included in the repository. I encourage you to study them. Most of them are simple. For the go services, I&rsquo;m using a multi stage build that has been  recently introduced. The Go services are Alpine Linux based. The Face Recognition service is Python. NSQ and MySQL are using their own containers.</p>

<h2 id="context">Context</h2>

<p>Kubernetes uses namespaces. If you don&rsquo;t specify any, it will use the <code>default</code> namespace. I&rsquo;m going to permanently set a context to avoid polluting the default namespace. You do that like this:</p>

<pre><code class="language-bash">❯ kubectl config set-context kube-face-cluster --namespace=face
Context &quot;kube-face-cluster&quot; created.
</code></pre>

<p>You have to also start using the context once it&rsquo;s created, like so:</p>

<pre><code class="language-bash">❯ kubectl config use-context kube-face-cluster
Switched to context &quot;kube-face-cluster&quot;.
</code></pre>

<p>After this, all <code>kubectl</code> commands will use the namespace <code>face</code>.</p>

<h2 id="deploying-the-application">Deploying the Application</h2>

<p>Overview of Pods and Services:</p>

<p><img src="https://skarlso.github.io/img/kube_deployed.png" alt="kube deployed" /></p>

<h3 id="mysql">MySQL</h3>

<p>The first Service I&rsquo;m going to deploy is my database.</p>

<p>I&rsquo;m using the Kubernetes example located here <a href="https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/#deploy-mysql">Kube MySQL</a> which fits my needs. Please note that this file is using a plain password for MYSQL_PASSWORD. I&rsquo;m going to employ a vault as described here <a href="https://kubernetes.io/docs/concepts/configuration/secret/">Kubernetes Secrets</a>.</p>

<p>I&rsquo;ve created a secret locally as described in that document using a secret yaml:</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: kube-face-secret
type: Opaque
data:
  mysql_password: base64codehere
</code></pre>

<p>I created the  base64 code via the following command:</p>

<pre><code class="language-bash">echo -n &quot;ubersecurepassword&quot; | base64
</code></pre>

<p>And, this is what you&rsquo;ll see in my deployment yaml file:</p>

<pre><code class="language-yaml">...
- name: MYSQL_ROOT_PASSWORD
  valueFrom:
    secretKeyRef:
      name: kube-face-secret
      key: mysql_password
...
</code></pre>

<p>Another thing worth mentioning: It&rsquo;s using a volume to persist the database. The volume definition is as follows:</p>

<pre><code class="language-yaml">...
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
...
      volumes:
      - name: mysql-persistent-storage
        persistentVolumeClaim:
          claimName: mysql-pv-claim
...
</code></pre>

<p><code>presistentVolumeClain</code> is key here. This tells Kubernetes that this resource requires a persistent volume. How it&rsquo;s provided is abstracted away from the user. You can be sure that Kubernetes will provide a volume that will always be there. It is similar to Pods. To read up on the details, check out this document: <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes">Kubernetes Persistent Volumes</a>.</p>

<p>Deploying the mysql Service is done with the following command:</p>

<pre><code class="language-bash">kubectl apply -f mysql.yaml
</code></pre>

<p><code>apply</code> vs <code>create</code>. In short, <code>apply</code> is considered a declarative object configuration command while <code>create</code> is imperative. What this means for now is that ‘create’ is usually for a one of tasks, like running something or creating a deployment. While, when using apply, the user doesn&rsquo;t define the action to be taken. That will be defined by Kubernetes based on the current status of the cluster. Thus, when there is no service called <code>mysql</code> and I&rsquo;m calling <code>apply -f mysql.yaml</code> it will create the service. When running again, Kubernetes won&rsquo;t do anything. But if I would run <code>create</code> again it will throw an error saying the service is already created.</p>

<p>For more information, check out the following docs: <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/">Kubernetes Object Management</a>, <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/">Imperative Configuration</a>, <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/">Declarative Configuration</a>.</p>

<p>To see progress information, run:</p>

<pre><code class="language-bash"># Describes the whole process
kubectl describe deployment mysql
# Shows only the pod
kubectl get pods -l app=mysql
</code></pre>

<p>Output should be similar to this:</p>

<pre><code class="language-bash">...
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   mysql-55cd6b9f47 (1/1 replicas created)
...
</code></pre>

<p>Or in case of <code>get pods</code>:</p>

<pre><code class="language-bash">NAME                     READY     STATUS    RESTARTS   AGE
mysql-78dbbd9c49-k6sdv   1/1       Running   0          18s
</code></pre>

<p>To test the instance, run the following snippet:</p>

<pre><code class="language-bash">kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -pyourpasswordhere
</code></pre>

<p><strong>GOTCHA</strong>: If you change the password now, it&rsquo;s not enough to re-apply your yaml file to update the container. Since the DB is persisted, the password will not be changed. You have to delete the whole deployment with <code>kubectl delete -f mysql.yaml</code>.</p>

<p>You should see the following when running a <code>show databases</code>.</p>

<pre><code class="language-bash">If you don't see a command prompt, try pressing enter.
mysql&gt;
mysql&gt;
mysql&gt; show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| kube               |
| mysql              |
| performance_schema |
+--------------------+
4 rows in set (0.00 sec)

mysql&gt; exit
Bye
</code></pre>

<p>You&rsquo;ll also notice that I’ve mounted a file located here: <a href="https://github.com/Skarlso/kube-cluster-sample/blob/master/database_setup.sql">Database Setup SQL</a> into the container. MySQL container automatically executes these. That file will bootstrap some data and the schema I&rsquo;m going to use.</p>

<p>The volume definition is as follows:</p>

<pre><code class="language-yaml">  volumeMounts:
  - name: mysql-persistent-storage
    mountPath: /var/lib/mysql
  - name: bootstrap-script
    mountPath: /docker-entrypoint-initdb.d/database_setup.sql
volumes:
- name: mysql-persistent-storage
  persistentVolumeClaim:
    claimName: mysql-pv-claim
- name: bootstrap-script
  hostPath:
    path: /Users/hannibal/golang/src/github.com/Skarlso/kube-cluster-sample/database_setup.sql
    type: File
</code></pre>

<p>To check if the bootstrap script was successful, run this:</p>

<pre><code class="language-bash">~/golang/src/github.com/Skarlso/kube-cluster-sample/kube_files master*
❯ kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -uroot -pyourpasswordhere kube
If you don't see a command prompt, try pressing enter.

mysql&gt; show tables;
+----------------+
| Tables_in_kube |
+----------------+
| images         |
| person         |
| person_images  |
+----------------+
3 rows in set (0.00 sec)

mysql&gt;
</code></pre>

<p>This concludes the database service setup. Logs for this service can be viewed with the following command:</p>

<pre><code class="language-bash">kubectl logs deployment/mysql -f
</code></pre>

<h3 id="nsq-lookup">NSQ Lookup</h3>

<p>The NSQ Lookup will run as an internal service. It doesn&rsquo;t need access from the outside, so I&rsquo;m setting <code>clusterIP: None</code> which will tell Kubernetes that this service is a headless service. This means that it won&rsquo;t be load balanced, and it won&rsquo;t be a single IP service. The DNS will be based upon service selectors.</p>

<p>Our NSQ Lookup selector is:</p>

<pre><code class="language-yaml">  selector:
    matchLabels:
      app: nsqlookup
</code></pre>

<p>Thus, the internal DNS will look like this: <code>nsqlookup.default.svc.cluster.local</code>.</p>

<p>Headless services are described in detail here: <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services">Headless Service</a>.</p>

<p>Basically it&rsquo;s the same as MySQL just with slight modifications. As stated earlier, I&rsquo;m using NSQ&rsquo;s own Docker Image called <code>nsqio/nsq</code>. All nsq commands are there, so nsqd will also use this image just with a different command. For nsqlookupd the command is:</p>

<pre><code class="language-yaml">command: [&quot;/nsqlookupd&quot;]
args: [&quot;--broadcast-address=nsqlookup.default.svc.cluster.local&quot;]
</code></pre>

<p>What&rsquo;s the <code>--broadcast-address</code> for, you might ask? By default, nsqlookup will use the <code>hostname</code> as broadcast address. When the consumer runs a callback it will try to connect to something like <code>http://nsqlookup-234kf-asdf:4161/lookup?topics=image</code>. Note that <code>nsqlookup-234kf-asdf</code> is the hostname of the container. By setting the broadcast-address to the internal DNS that callback will be: <code>http://nsqlookup.default.svc.cluster.local:4161/lookup?topic=images</code>. Which will work as expected.</p>

<p>NSQ Lookup also requires two ports forwarded. One for broadcasting and one for nsqd callback. These are exposed in the Dockerfile and then utilized in the Kubernetes template like this:</p>

<p>In the container template:</p>

<pre><code class="language-yaml">        ports:
        - containerPort: 4160
          hostPort: 4160
        - containerPort: 4161
          hostPort: 4161
</code></pre>

<p>In the service template:</p>

<pre><code class="language-yaml">spec:
  ports:
  - name: tcp
    protocol: TCP
    port: 4160
    targetPort: 4160
  - name: http
    protocol: TCP
    port: 4161
    targetPort: 4161
</code></pre>

<p>Names are required by Kubernetes.</p>

<p>To create this service, I&rsquo;m using the same command as before:</p>

<pre><code class="language-bash">kubectl apply -f nsqlookup.yaml
</code></pre>

<p>This concludes nsqlookupd. Two of the major players are in the sack.</p>

<h3 id="receiver-1">Receiver</h3>

<p>This is a more complex one. The receiver will do three things.</p>

<ul>
<li>Create some deployments</li>
<li>Create the nsq daemon</li>
<li>Expose the service to the public</li>
</ul>

<h4 id="deployments-1">Deployments</h4>

<p>The first deployment it creates is it&rsquo;s own. The receiver’s container is <code>skarlso/kube-receiver-alpine</code>.</p>

<h4 id="nsq-daemon">Nsq Daemon</h4>

<p>The receiver starts an nsq daemon. Like stated earlier, the receiver runs an nsqd with it-self. It does that so talking to it can happen locally and not over the network. By making receiver do this, they will end up on the same node.</p>

<p>NSQ daemon also needs some adjustments and parameters.</p>

<pre><code class="language-yaml">        ports:
        - containerPort: 4150
          hostPort: 4150
        - containerPort: 4151
          hostPort: 4151
        env:
        - name: NSQLOOKUP_ADDRESS
          value: nsqlookup.default.svc.cluster.local
        - name: NSQ_BROADCAST_ADDRESS
          value: nsqd.default.svc.cluster.local
        command: [&quot;/nsqd&quot;]
        args: [&quot;--lookupd-tcp-address=$(NSQLOOKUP_ADDRESS):4160&quot;, &quot;--broadcast-address=$(NSQ_BROADCAST_ADDRESS)&quot;]

</code></pre>

<p>You can see that the lookup-tcp-address and the broadcast-address are set. Lookup tcp address is the DNS for the nsqlookupd service. And the broadcast address is necessary just like with nsqlookupd so the callbacks are working properly.</p>

<h4 id="public-facing">Public facing</h4>

<p>Now, this is the first time I&rsquo;m deploying a public facing service. There are two options. I could use a LoadBalancer because this API will be under heavy load. And if this would be deployed anywhere in production, then it should be using one.</p>

<p>I&rsquo;m doing this locally though- with one node- so something called a <code>NodePort</code> is enough. A <code>NodePort</code> exposes a service on each node&rsquo;s IP at a static port. If not specified, it will assign a random port on the host between 30000-32767. But it can also be configured to be a specific port, using <code>nodePort</code> in the template file. To reach this service, use <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code>. If more than one node is configured a LoadBalancer can multiplex them to a single IP.</p>

<p>For further information check out this document: <a href="https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services---service-types">Publishing Services</a>.</p>

<p>Putting this all together, we&rsquo;ll get a receiver-service for which the template is as follows:</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: receiver-service
spec:
  ports:
  - protocol: TCP
    port: 8000
    targetPort: 8000
  selector:
    app: receiver
  type: NodePort
</code></pre>

<p>For a fixed nodePort on 8000 a definition of <code>nodePort</code> must be provided:</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: receiver-service
spec:
  ports:
  - protocol: TCP
    port: 8000
    targetPort: 8000
  selector:
    app: receiver
  type: NodePort
  nodePort: 8000
</code></pre>

<h3 id="image-processor-1">Image processor</h3>

<p>The Image Processor is where I&rsquo;m handling passing off images to be identified. It should have access to nsqlookupd, mysql and the gRPC endpoint of the face recognition service. This is actually a boring service. In fact, it&rsquo;s not a service at all. It doesn&rsquo;t expose anything and thus it&rsquo;s the first deployment only component. For brevity, here is the whole template:</p>

<pre><code class="language-yaml">---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: image-processor-deployment
spec:
  selector:
    matchLabels:
      app: image-processor
  replicas: 1
  template:
    metadata:
      labels:
        app: image-processor
    spec:
      containers:
      - name: image-processor
        image: skarlso/kube-processor-alpine:latest
        env:
        - name: MYSQL_CONNECTION
          value: &quot;mysql.default.svc.cluster.local&quot;
        - name: MYSQL_USERPASSWORD
          valueFrom:
            secretKeyRef:
              name: kube-face-secret
              key: mysql_userpassword
        - name: MYSQL_PORT
          # TIL: If this is 3306 without &quot; kubectl throws an error.
          value: &quot;3306&quot;
        - name: MYSQL_DBNAME
          value: kube
        - name: NSQ_LOOKUP_ADDRESS
          value: &quot;nsqlookup.default.svc.cluster.local:4161&quot;
        - name: GRPC_ADDRESS
          value: &quot;face-recog.default.svc.cluster.local:50051&quot;

</code></pre>

<p>The only interesting points in this file are the multitude of environment properties that are used to configure the application. Note the nsqlookupd address and the grpc address.</p>

<p>To create this deployment, run:</p>

<pre><code class="language-bash">kubectl apply -f image_processor.yaml
</code></pre>

<h3 id="face-recognition-1">Face - Recognition</h3>

<p>The face recognition service does have a service. It&rsquo;s a simple one, only needed by image-processor. It&rsquo;s template is as follows:</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: face-recog
spec:
  ports:
  - protocol: TCP
    port: 50051
    targetPort: 50051
  selector:
    app: face-recog
  clusterIP: None
</code></pre>

<p>The more interesting part is that it requires two volumes. The two volumes are <code>known_people</code> and <code>unknown_people</code>. Can you guess what they will contain? Yep, images. The <code>known_people</code> volume contains all the images associated to the known people in the database. The <code>unknown_people</code> volume will contain all the new images. And that&rsquo;s the path we will need to use when sending images from the receiver. That is, wherever the mount point points too. Which in my case is <code>/unknown_people</code>. Basically the path needs to be one that the face recognition service can access.</p>

<p>Now, with Kubernetes and Docker this is easy. It could be a mounted S3 or some kind of nfs or a local mount from host to guest. The possibilities are endless (around a dozen or so). I&rsquo;m going to use a local mount for the sake of simplicity.</p>

<p>Mounting a volume has two parts. First, the Dockerfile has to specify volumes:</p>

<pre><code class="language-Dockerfile">VOLUME [ &quot;/unknown_people&quot;, &quot;/known_people&quot; ]
</code></pre>

<p>Second, the Kubernetes template needs add <code>volumeMounts</code> as seen in the MySQL service; the difference being <code>hostPath</code> instead of a claimed volume:</p>

<pre><code class="language-yaml">        volumeMounts:
        - name: known-people-storage
          mountPath: /known_people
        - name: unknown-people-storage
          mountPath: /unknown_people
      volumes:
      - name: known-people-storage
        hostPath:
          path: /Users/hannibal/Temp/known_people
          type: Directory
      - name: unknown-people-storage
        hostPath:
          path: /Users/hannibal/Temp/
          type: Directory
</code></pre>

<p>We also have to set the <code>known_people</code> folder config setting for face recognition. This is done via an environment property:</p>

<pre><code class="language-yaml">        env:
        - name: KNOWN_PEOPLE
          value: &quot;/known_people&quot;
</code></pre>

<p>Then the Python code will look up images like this:</p>

<pre><code class="language-python">        known_people = os.getenv('KNOWN_PEOPLE', 'known_people')
        print(&quot;Known people images location is: %s&quot; % known_people)
        images = self.image_files_in_folder(known_people)
</code></pre>

<p>Where <code>image_files_in_folder</code> is:</p>

<pre><code class="language-python">    def image_files_in_folder(self, folder):
        return [os.path.join(folder, f) for f in os.listdir(folder) if re.match(r'.*\.(jpg|jpeg|png)', f, flags=re.I)]
</code></pre>

<p>Neat.</p>

<p>Now, if the receiver receives a request (and sends it off further the line) similar to the one below&hellip;</p>

<pre><code class="language-bash">curl -d '{&quot;path&quot;:&quot;/unknown_people/unknown220.jpg&quot;}' http://192.168.99.100:30251/image/post
</code></pre>

<p>&hellip;it will look for an image called unknown220.jpg under <code>/unknown_people</code>, locate an image in the known_folder that corresponds to the person on the unknown image and return the name of the image that matched.</p>

<p>Looking at logs you should see something like this:</p>

<pre><code class="language-bash"># Receiver
❯ curl -d '{&quot;path&quot;:&quot;/unknown_people/unknown219.jpg&quot;}' http://192.168.99.100:30251/image/post
got path: {Path:/unknown_people/unknown219.jpg}
image saved with id: 4
image sent to nsq

# Image Processor
2018/03/26 18:11:21 INF    1 [images/ch] querying nsqlookupd http://nsqlookup.default.svc.cluster.local:4161/lookup?topic=images
2018/03/26 18:11:59 Got a message: 4
2018/03/26 18:11:59 Processing image id:  4
2018/03/26 18:12:00 got person:  Hannibal
2018/03/26 18:12:00 updating record with person id
2018/03/26 18:12:00 done
</code></pre>

<p>And that concludes all of the services that we need to deploy.</p>

<h3 id="frontend">Frontend</h3>

<p>Last, there is a small web-app which displays the information in the db for convenience. This is also a public facing service with the same parameters as the receiver.</p>

<p>It looks like this:</p>

<p><img src="https://skarlso.github.io/img/kube-frontend.png" alt="frontend" /></p>

<h3 id="recap">Recap</h3>

<p>So what is the situation so far? I deployed a bunch of services all over the place. A recap off the commands I used:</p>

<pre><code class="language-bash">kubectl apply -f mysql.yaml
kubectl apply -f nsqlookup.yaml
kubectl apply -f receiver.yaml
kubectl apply -f image_processor.yaml
kubectl apply -f face_recognition.yaml
kubectl apply -f frontend.yaml
</code></pre>

<p>These could be in any order because the application does not allocate connections on start. Except for image_processor&rsquo;s NSQ consumer. But that re-tries.</p>

<p>Query-ing kube for running pods with <code>kubectl get pods</code> should show something like this:</p>

<pre><code class="language-bash">❯ kubectl get pods
NAME                                          READY     STATUS    RESTARTS   AGE
face-recog-6bf449c6f-qg5tr                    1/1       Running   0          1m
image-processor-deployment-6467468c9d-cvx6m   1/1       Running   0          31s
mysql-7d667c75f4-bwghw                        1/1       Running   0          36s
nsqd-584954c44c-299dz                         1/1       Running   0          26s
nsqlookup-7f5bdfcb87-jkdl7                    1/1       Running   0          11s
receiver-deployment-5cb4797598-sf5ds          1/1       Running   0          26s
</code></pre>

<p>Running <code>minikube service list</code>:</p>

<pre><code class="language-bash">❯ minikube service list
|-------------|----------------------|-----------------------------|
|  NAMESPACE  |         NAME         |             URL             |
|-------------|----------------------|-----------------------------|
| default     | face-recog           | No node port                |
| default     | kubernetes           | No node port                |
| default     | mysql                | No node port                |
| default     | nsqd                 | No node port                |
| default     | nsqlookup            | No node port                |
| default     | receiver-service     | http://192.168.99.100:30251 |
| kube-system | kube-dns             | No node port                |
| kube-system | kubernetes-dashboard | http://192.168.99.100:30000 |
|-------------|----------------------|-----------------------------|
</code></pre>

<h3 id="rolling-update">Rolling update</h3>

<p>What happens during a rolling update?</p>

<p><img src="https://skarlso.github.io/img/kube_rotate.png" alt="kube rotate" /></p>

<p>As it happens during software development, change is requested/needed to some parts of the system. What happens to our cluster if I would like to change one of its components without breaking the others? And also whilst maintaining backwards compatibility with no disruption to user experience. Thankfully Kubernetes can help with that.</p>

<p>What I don&rsquo;t like is that the API only handles one image at a time. There is no option to bulk upload.</p>

<h4 id="code">Code</h4>

<p>Right now, we have the following code segment dealing with a single image:</p>

<pre><code class="language-go">// PostImage handles a post of an image. Saves it to the database
// and sends it to NSQ for further processing.
func PostImage(w http.ResponseWriter, r *http.Request) {
...
}

func main() {
    router := mux.NewRouter()
    router.HandleFunc(&quot;/image/post&quot;, PostImage).Methods(&quot;POST&quot;)
    log.Fatal(http.ListenAndServe(&quot;:8000&quot;, router))
}
</code></pre>

<p>We have two options. Add a new endpoint with <code>/images/post</code> and make the client use that, or modify the existing one.</p>

<p>The new client code has the advantage that it could fall back to submitting the old way if the new endpoint isn&rsquo;t available. The old client code though doesn&rsquo;t have this advantage so we can&rsquo;t change the way our code works right now. Consider this. You have 90 servers. You do a slow paced rolling update that will take out servers one step at a time doing an update. If an update lasts around a minute, the whole process will take around one and a half hours to complete (not counting any parallel updates).</p>

<p>During that time, some of your servers will run the new code and some will run the old one. Calls are load balanced, thus you have no control over what server is hit. If a client is trying to do a call the new way but hits an old server the client would fail. The client could try a fallback, but since you eliminated the old version it will not succeed unless it, by chance, hits a server with the new code (assuming no sticky sessions are set).</p>

<p>Also, once all your servers are updated, an old client will not be able to use your service any longer at all.</p>

<p>Now, you could argue that you don&rsquo;t want to keep old versions of your code forever. And that is true in some sense. That&rsquo;s why what we are going to do is modify the old code to simply call the new one with some slight augmentations. This way once all clients have been migrated, the code can simply be deleted without any problems.</p>

<h4 id="new-endpoint">New Endpoint</h4>

<p>Let&rsquo;s add a new route method:</p>

<pre><code class="language-go">...
router.HandleFunc(&quot;/images/post&quot;, PostImages).Methods(&quot;POST&quot;)
...
</code></pre>

<p>And updating the old one to call the new one with a modified body like this:</p>

<pre><code class="language-go">// PostImage handles a post of an image. Saves it to the database
// and sends it to NSQ for further processing.
func PostImage(w http.ResponseWriter, r *http.Request) {
    var p Path
    err := json.NewDecoder(r.Body).Decode(&amp;p)
    if err != nil {
      fmt.Fprintf(w, &quot;got error while decoding body: %s&quot;, err)
      return
    }
    fmt.Fprintf(w, &quot;got path: %+v\n&quot;, p)
    var ps Paths
    paths := make([]Path, 0)
    paths = append(paths, p)
    ps.Paths = paths
    var pathsJSON bytes.Buffer
    err = json.NewEncoder(&amp;pathsJSON).Encode(ps)
    if err != nil {
      fmt.Fprintf(w, &quot;failed to encode paths: %s&quot;, err)
      return
    }
    r.Body = ioutil.NopCloser(&amp;pathsJSON)
    r.ContentLength = int64(pathsJSON.Len())
    PostImages(w, r)
}
</code></pre>

<p>Well, the naming could be better, but you should get the basic idea. I&rsquo;m modifying the incoming single path by wrapping it into the new format and sending it over to the new endpoint handler. And that&rsquo;s it. There are a few more modifications, to check them out take a look at this PR: <a href="https://github.com/Skarlso/kube-cluster-sample/pull/1">Rolling Update Bulk Image Path PR</a>.</p>

<p>Now, we can call the receiver in two ways:</p>

<pre><code class="language-bash"># Single Path:
curl -d '{&quot;path&quot;:&quot;unknown4456.jpg&quot;}' http://127.0.0.1:8000/image/post

# Multiple Paths:
curl -d '{&quot;paths&quot;:[{&quot;path&quot;:&quot;unknown4456.jpg&quot;}]}' http://127.0.0.1:8000/images/post
</code></pre>

<p>Here, the client is curl. Normally, if the client would be a service, I would modify it that in case the new end-point throws a 404 it would try the old one next.</p>

<p>For brevity, I&rsquo;m not modifying NSQ and the others to handle bulk image processing. They will still receive it one - by - one. I&rsquo;ll leave that up to you as homework. ;)</p>

<h4 id="new-image">New Image</h4>

<p>To perform a rolling update, I must create a new image first from the receiver service.</p>

<pre><code class="language-bash">docker build -t skarlso/kube-receiver-alpine:v1.1 .
</code></pre>

<p>Once this is complete, we can begin rolling out the change.</p>

<h4 id="rolling-update-1">Rolling update</h4>

<p>In Kubernetes, you can configure your rolling update in multiple ways.</p>

<h5 id="manual-update">Manual Update</h5>

<p>If, say, I was using a container version in my config file called <code>v1.0</code> than doing an update is simply calling:</p>

<pre><code class="language-bash">kubectl rolling-update receiver --image:skarlso/kube-receiver-alpine:v1.1
</code></pre>

<p>If there is a problem during the rollout we can always rollback.</p>

<pre><code class="language-bash">kubectl rolling-update receiver --rollback
</code></pre>

<p>It will set back the previous version no fuss, no muss.</p>

<h5 id="apply-a-new-configuration-file">Apply a new configuration file</h5>

<p>The problem with by-hand updates is always that they aren&rsquo;t in source control.</p>

<p>Consider this. Something changed, a couple of servers got updated by hand to do a quick “patch fix”, but nobody witnessed it and it wasn’t documented. A new person comes along and does a change to the template and applies the template to the cluster. All the servers are updated, but suddenly, there is a service outage.</p>

<p>Long story short, the servers which got updated are whacked over because the template didn&rsquo;t reflect what has been done by hand. That is bad. Don&rsquo;t do that.</p>

<p>The recommended way is to change the template to use the new version and than apply the template with the <code>apply</code> command.</p>

<p>Kubernetes recommends that the Deployment handles the rollout with ReplicaSets. This means however, that there must be at least two replicates present for a rolling update. Otherwise the update won&rsquo;t work (unless <code>maxUnavailable</code> is set to 1). I&rsquo;m increasing the replica count in the yaml and I set the new image version for the receiver container.</p>

<pre><code class="language-yaml">  replicas: 2
...
    spec:
      containers:
      - name: receiver
        image: skarlso/kube-receiver-alpine:v1.1
...
</code></pre>

<p>Looking at the progress you should see something like this:</p>

<pre><code class="language-bash">❯ kubectl rollout status deployment/receiver-deployment
Waiting for rollout to finish: 1 out of 2 new replicas have been updated...
</code></pre>

<p>You can add in additional rollout configuration settings by specifying the <code>strategy</code> part of the template like this:</p>

<pre><code class="language-yaml">  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
</code></pre>

<p>Additional information on rolling update can be found in these documents: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-a-deployment">Deployment Rolling Update</a>, <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment">Updating a Deployment</a>, <a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#updating-your-application-without-a-service-outage">Manage Deployments</a>, <a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/">Rolling Update using ReplicaController</a>.</p>

<p><strong>NOTE MINIKUBE USERS</strong>: Since we are doing this on a local machine with one node and 1 replica of an application, we have to set <code>maxUnavailable</code> to <code>1</code>. Otherwise, Kubernetes won&rsquo;t allow the update to happen and the new version will always be in <code>Pending</code> state since we aren&rsquo;t allowing that at any given point in time there is a situation where no containers are present for <code>receiver</code> app.</p>

<h3 id="scaling-1">Scaling</h3>

<p>Scaling is dead easy with Kubernetes. Since it&rsquo;s managing the whole cluster, you basically, just have to put a number into the template of the desired replicas to use.</p>

<p>This has been a great post so far but it&rsquo;s getting too long. I&rsquo;m planning on writing a follow-up where I will be truly scaling things up on AWS with multiple nodes and replicas. Stay tuned.</p>

<h3 id="cleanup">Cleanup</h3>

<pre><code class="language-bash">kubectl delete deployments --all
kubectl delete services -all
</code></pre>

<h1 id="final-words">Final Words</h1>

<p>And that is it ladies and gentleman. We wrote, deployed, updated and scaled (well, not yet really) a distributed application with Kubernetes.</p>

<p>Any questions, please feel free to chat in the comments below, I&rsquo;m happy to answer.</p>

<p>I hope you enjoyed reading this. I know, it&rsquo;s quiet long and I was thinking of splitting it up, but having a cohesive, one page guide is sometimes useful and makes it easy to find something or save it for later read or even print as PDF.</p>

<p>Thank you for reading,
Gergely.</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">06 Feb 2018, 23:01</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://skarlso.github.io/2018/02/06/go-budapest-meetup/" class="post-title">Go Budapest Meetup</a>

                        <p class="post-meta">
                            
                                By <strong class="post-author">hannibal</strong>
                            
                            
                                under 
                                
                                <a class="post-category post-category-Go" href="https://skarlso.github.io//categories/go">Go</a><a class="post-category post-category-Meetup" href="https://skarlso.github.io//categories/meetup">Meetup</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        

<h1 id="intro">Intro</h1>

<p>So I was at <a href="https://www.meetup.com/go-budapest">Go Budapest Meetup</a> yesterday, where the brilliant <a href="https://jbrandhorst.com/">Johan Brandhorst</a>
gave a talk about his project based on <a href="https://grpc.io/">gRPC</a> using <a href="https://github.com/improbable-eng/grpc-web">gRPC-web</a> +
<a href="https://github.com/gopherjs/gopherjs">GopherJS</a> + <a href="https://github.com/google/protobuf">protobuf</a>. He also has some Go
contributions and check out his project here: <a href="https://github.com/johanbrandhorst/protobuf">Protobuf</a>. It&rsquo;s GopherJS Bindings for
ProtobufJS and gRPC-Web.</p>

<p>It was interesting to see where these projects could lead and I see the potential in them. I liked the usage of Protobuf and gRPC,
I don&rsquo;t have THAT much experience with them. However after yesterday, I&rsquo;m eager to find an excuse to do something with these libraries.
I used gRPC indirectly, well, the result of it, when dealing with Google Cloud Platform&rsquo;s API. Which is largely generated code through
gRPC and protobuf.</p>

<p>He also presented a bi-directional stream communication between the gRPC-web client and the server which was an interesting feat
to produce. It did involve the use of <a href="https://godoc.org/golang.org/x/sync/errgroup">errgroup</a>. Which is nice.</p>

<p>I didn&rsquo;t look THAT much into WebAssembly however, again, after yesterday, I will. He gave a shout out to WebAssembly developers
that he is ready to tackle the Go bindings for WASM!</p>

<p>It was a good change of pace to look at some Go code being written, I&rsquo;ll be sure to visit the meetup again, in about three months
when the next one will come.</p>

<p>Maybe, I&rsquo;ll even give a talk if they are looking for speakers. ;)</p>

<p>A huge thank you to <a href="https://www.emarsys.com/en/about-us/">Emarsys Budapest</a> for organizing the event and bringing Johan to us
for his talk.</p>

<p>Thanks,<br />
Gergely</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">06 Feb 2018, 23:01</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://skarlso.github.io/2018/02/06/go-budapest-meetup/" class="post-title">Go Budapest Meetup</a>

                        <p class="post-meta">
                            
                                By <strong class="post-author">hannibal</strong>
                            
                            
                                under 
                                
                                <a class="post-category post-category-Go" href="https://skarlso.github.io//categories/go">Go</a><a class="post-category post-category-Meetup" href="https://skarlso.github.io//categories/meetup">Meetup</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        

<h1 id="intro">Intro</h1>

<p>So I was at <a href="https://www.meetup.com/go-budapest">Go Budapest Meetup</a> yesterday, where the brilliant <a href="https://jbrandhorst.com/">Johan Brandhorst</a>
gave a talk about his project based on <a href="https://grpc.io/">gRPC</a> using <a href="https://github.com/improbable-eng/grpc-web">gRPC-web</a> +
<a href="https://github.com/gopherjs/gopherjs">GopherJS</a> + <a href="https://github.com/google/protobuf">protobuf</a>. He also has some Go
contributions and check out his project here: <a href="https://github.com/johanbrandhorst/protobuf">Protobuf</a>. It&rsquo;s GopherJS Bindings for
ProtobufJS and gRPC-Web.</p>

<p>It was interesting to see where these projects could lead and I see the potential in them. I liked the usage of Protobuf and gRPC,
I don&rsquo;t have THAT much experience with them. However after yesterday, I&rsquo;m eager to find an excuse to do something with these libraries.
I used gRPC indirectly, well, the result of it, when dealing with Google Cloud Platform&rsquo;s API. Which is largely generated code through
gRPC and protobuf.</p>

<p>He also presented a bi-directional stream communication between the gRPC-web client and the server which was an interesting feat
to produce. It did involve the use of <a href="https://godoc.org/golang.org/x/sync/errgroup">errgroup</a>. Which is nice.</p>

<p>I didn&rsquo;t look THAT much into WebAssembly however, again, after yesterday, I will. He gave a shout out to WebAssembly developers
that he is ready to tackle the Go bindings for WASM!</p>

<p>It was a good change of pace to look at some Go code being written, I&rsquo;ll be sure to visit the meetup again, in about three months
when the next one will come.</p>

<p>Maybe, I&rsquo;ll even give a talk if they are looking for speakers. ;)</p>

<p>A huge thank you to <a href="https://www.emarsys.com/en/about-us/">Emarsys Budapest</a> for organizing the event and bringing Johan to us
for his talk.</p>

<p>Thanks,<br />
Gergely</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">23 Jan 2018, 22:34</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://skarlso.github.io/2018/01/23/nginx-certbot-ansible/" class="post-title">Ansible &#43; Nginx &#43; LetsEncrypt &#43; Wiki &#43; Nagios</a>

                        <p class="post-meta">
                            
                                By <strong class="post-author">hannibal</strong>
                            
                            
                                under 
                                
                                <a class="post-category post-category-Ansible" href="https://skarlso.github.io//categories/ansible">Ansible</a><a class="post-category post-category-Nginx" href="https://skarlso.github.io//categories/nginx">Nginx</a><a class="post-category post-category-Certbot" href="https://skarlso.github.io//categories/certbot">Certbot</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        

<h1 id="intro">Intro</h1>

<p>Hi folks.</p>

<p>Today, I would like demonstrate how to use <a href="https://www.ansible.com/">Ansible</a> in order to construct a server hosting multiple HTTPS domains with <a href="https://www.nginx.com/">Nginx</a> and <a href="https://letsencrypt.org/">LetsEncrypt</a>. Are you ready? Let&rsquo;s dive in.</p>

<h2 id="tl-dr">TL;DR</h2>

<p><img src="https://skarlso.github.io/img/ansible.svg" alt="playbook" /></p>

<h2 id="what-you-will-need">What you will need</h2>

<p>There is really only one thing you need in order for this to work and that is Ansible. If you would like to run local tests without a remote server, than you will need <a href="https://www.vagrantup.com/">Vagrant</a> and <a href="https://www.virtualbox.org/wiki/Downloads">VirtualBox</a>. But those two are optional.</p>

<h2 id="what-we-are-going-to-set-up">What We Are Going To Set Up</h2>

<p>The setup is as follows:</p>

<h3 id="nagios">Nagios</h3>

<p>We are going to have a Nagios with a custom check for pending security updates. That will run under nagios.example.com.</p>

<h3 id="hugo-website">Hugo Website</h3>

<p>The main web site is going to be a basic <a href="https://gohugo.io/">Hugo</a> site. Hugo is a static Go based web site generator. This Blog is run by it.</p>

<p>We are also going to setup <a href="https://www.noip.com/">NoIP</a> which will provide the DNS for the sites.</p>

<h3 id="wiki">Wiki</h3>

<p>The wiki is a plain, basic <a href="https://www.dokuwiki.org/dokuwiki#">DokuWiki</a>.</p>

<h3 id="https-nginx">HTTPS + Nginx</h3>

<p>And all the above will be hosted by Nginx with HTTPS provided by letsencrypt. We are going to set all these up with Ansible on top so it will be idempotent.</p>

<h3 id="repository">Repository</h3>

<p>All of the playbooks and the whole thing together can be viewed here: <a href="https://github.com/Skarlso/ansible-server-setup">Github Ansible Server Setup</a>.</p>

<h2 id="ansible">Ansible</h2>

<p>I won&rsquo;t be writing everything down to the basics about Ansible. For that you will need to go and read its documentation. But I will provide ample of clarification for using what I&rsquo;ll be using.</p>

<h3 id="some-basics">Some Basics</h3>

<p>Ansible is a configuration management tool which, unlike chef or puppet, isn&rsquo;t master - slave based. It&rsquo;s using SSH to run a set of instructions on a target machine. The instructions are written in yaml files and look something like this:</p>

<pre><code class="language-yaml">---
# tasks file for ssh
- name: Copy sshd_config
  copy: content=&quot;{{sshd_config}}&quot; dest=/etc/ssh/sshd_config
  notify:
  - SSHD Restart
</code></pre>

<p>This is a basic Task which copies over an <code>sshd_config</code> file overwriting the one already being there. It can execute in priviliged mode if root password is provided or the user has sudo rights.</p>

<p>It works from so called <code>hosts</code> files where the server details are described. This is how a basic host file would look like:</p>

<pre><code class="language-bash">[local]
127.0.0.1

[webserver1]
1.23.4.5
</code></pre>

<p>Ansible will use these settings to try and access the server. To test if the connection is working, you can send a <code>ping</code> task like this:</p>

<pre><code class="language-bash">ansible all -m ping
</code></pre>

<p>Ansible uses <code>variables</code> for things that change. They are defined under each task&rsquo;s subfolder called <code>vars</code>. Please feel free to change the varialbes there to your liking.</p>

<h3 id="ssh-access">SSH Access</h3>

<p>You can either define SSH information per host or per group or globally. In this example I have it under the groups wars called
<code>webserver1</code> like this (vars.yaml):</p>

<pre><code class="language-yaml">---
# SSH sudo keys and pass
ansible_become_pass: '{{vault_ansible_become_pass}}'
ansible_ssh_port: '{{vault_ansible_ssh_port}}'
ansible_ssh_user: '{{vault_ansible_ssh_user}}'
ansible_ssh_private_key_file: '{{vault_ansible_ssh_private_key_file}}'
home_dir: /root
</code></pre>

<h3 id="further-reading">Further reading</h3>

<p>Further readings are:</p>

<ul>
<li><a href="https://serversforhackers.com/c/an-ansible-tutorial">Servers For Hackers</a></li>
<li><a href="http://docs.ansible.com/ansible/latest/intro_getting_started.html">Ansible docs</a></li>
</ul>

<h3 id="vault">Vault</h3>

<p>The vault is the place where we can keep secure information. This file is called <code>vault</code> and usually lives under either <code>group_vars</code> or <code>host_vars</code>. The preference is up to you.</p>

<p>This file is encrypted using a password you specify. You can have the vault password stored in the following ways:</p>

<ul>
<li>Store it on a secure drive which is encrypted and only mounted when the playbook is executed</li>
<li>Store it on <a href="https://keybase.io">Keybase</a></li>
<li>Store it on an encrypted S3 bucket</li>
<li>Store it in a file next to the playbook which is never commited into source control</li>
</ul>

<p>Either way, in the end, ansible will look for a file called <code>.vault_password</code> for when it&rsquo;s trying to decrypt the file. You can
define a different file in the <code>ansible.cfg</code> file using the <code>vault_password_file</code> option.</p>

<p>You can create a vault like this:</p>

<pre><code class="language-bash">ansible-vault create vault
</code></pre>

<p>If you are following along, you are going to need these variables in the vault:</p>

<pre><code class="language-yaml">vault_ansible_become_pass: &lt;your_sudo_password&gt; # if applicable
vault_ansible_ssh_user: &lt;ssh_user&gt;
vault_ansible_ssh_private_key_file: /Users/user/.ssh/ida_rsa
vault_nagios_password: supersecurenagiosadminpassword
vault_nagios_username: nagiosadmin
vault_noip_username: youruser@gmail.com
vault_noip_password: &quot;SuperSecureNoIPPassword&quot;
vault_nginx_user: &lt;localuser&gt;
</code></pre>

<p>You can always edit the vault later on with:</p>

<pre><code class="language-bash">ansible-vault edit group_vars/webserver1/vault --vault-password-file=.vault_pass
</code></pre>

<h3 id="tasks">Tasks</h3>

<p>The following are a collection of tasks which execute in order. The end task, which is letsencrypt, relies on all the hosts being present and configured under Nginx. Otherwise it will throw an error that the host you are trying to configure HTTPS for, isn&rsquo;t defined.</p>

<h4 id="no-ip">No-IP</h4>

<p>I&rsquo;m choosing No-ip as a DNS provider because it&rsquo;s cheap and the sync tool is easy to automate. To automate the CLI of No-IP, I&rsquo;m using a package called <code>expect</code>. This looks something like this:</p>

<pre><code class="language-bash">cd {{home_dir}}
wget http://www.no-ip.com/client/linux/noip-duc-linux.tar.gz
mkdir -p noip
tar zxf noip-duc-linux.tar.gz -C noip
cd noip/*
make

/usr/bin/expect &lt;&lt;END_SCRIPT
spawn make install
expect &quot;Please enter the login/email*&quot; { send &quot;{{noip_username}}\r&quot; }
expect &quot;Please enter the password for user*&quot; { send &quot;{{noip_password}}\r&quot; }
expect {
    &quot;Do you wish to have them all updated*&quot; {
        send &quot;y&quot;
        exp_continue
    }
}
expect &quot;Please enter an update interval*&quot; { send &quot;30\r&quot; }
expect &quot;Do you wish to run something at successful update*&quot; {send &quot;N&quot; }
END_SCRIPT
</code></pre>

<p>The interesting part is the command running expect. Basically, it&rsquo;s expecting some kind of output which is outlined there. And has canned answers for those which it <code>send</code>s to the waiting command.</p>

<h4 id="to-util-or-not-to-util">To Util or Not To Util</h4>

<p>So, there are small tasks, like installing vim and wget and such which could warrant the existance of a <code>utils</code> task. Utils task would install the packages that are used as convinience and don&rsquo;t really relate to a singe task.</p>

<p>Yet I settled for the following. Each of my tasks has a dependency part. The given tasks takes care of all the packages it needs so they can be executed on their own as well as in unison.</p>

<p>This looks like this:</p>

<pre><code class="language-yaml"># Install dependencies
- name: Install dependencies
  apt: pkg=&quot;{{item}}&quot; state=installed
  with_items:
    - &quot;{{deps}}&quot;
</code></pre>

<p>For which the <code>deps</code> variable is defined as follows:</p>

<pre><code class="language-yaml"># Defined dependencies for letsencrypt task.
deps: ['git', 'python-dev', 'build-essential', 'libpython-dev', 'libpython2.7', 'augeas-lenses', 'libaugeas0', 'libffi-dev', 'libssl-dev', 'python-virtualenv', 'python3-virtualenv', 'virtualenv']
</code></pre>

<p>This is much cleaner. And if a task is no longer needed, it&rsquo;s dependencies will no longer be needed either in most of the cases.</p>

<h4 id="nagios-1">Nagios</h4>

<p>I&rsquo;m using Nagios 4 which is a real pain in the butt to install. Luckily, thanks to Ansiblei, I only ever had to figure it out once. Now I have a script for that. Installing Nagios demands several, smaller components to be installed. Thus our task uses import from outside tasks like this:</p>

<pre><code class="language-yaml">- name: Install Nagios
  block:
    - include: create_users.yml # creates the Nagios user
    - include: install_dependencies.yml # installs Nagios dependencies
    - include: core_install.yml # Installs Nagios Core
    - include: plugin_install.yml # Installs Nagios Plugins
    - include: create_htpasswd.yml # Creates a password for Nagios' admin user
    - include: setup_custom_check.yml # Adds a custom check which is to check how many security updates are pending
  when: st.stat.exists == False
</code></pre>

<p>The <code>when</code> is a check for a variable created by a file check.</p>

<pre><code class="language-yaml">- stat:
    path: /usr/local/nagios/bin/nagios
  register: st
</code></pre>

<p>It checks if Nagios is installed or not. If yes, skip.</p>

<p>I&rsquo;m not going to paste in here all the subtasks because that would be huge. You can check those out in the repository under Nagios.</p>

<h4 id="hugo">Hugo</h4>

<p>Hugo is easy to install. Its sole requirement is Go. To install hugo you simply run <code>apt-get install hugo</code>. Setting up the
site for me was just checking out the git repo and than execute hugo from the root folder like this:</p>

<pre><code class="language-bash">hugo server --bind=127.0.0.1 --port=8080 --baseUrl=https://example.com --appendPort=false --logFile hugo.log --verboseLog --verbose -v &amp;
</code></pre>

<h4 id="wiki-1">Wiki</h4>

<p>I used DokuWiki because it&rsquo;s a file based wiki so installation is basically just downloading the archive, extracting it and done. The only thing that&rsquo;s needed for it, is php-fpm to run it and a few php modules which I&rsquo;ll outline in the ansible playbook.</p>

<p>The VHOST file for DokuWiki is provided by them and looks like this:</p>

<pre><code class="language-bash">server {
    server_name   {{ wiki_server_name }};
    root {{ wiki_root }};
    index index.php index.html index.htm;
    client_max_body_size 2M;
    client_body_buffer_size 128k;
    location / {
        index doku.php;
        try_files $uri $uri/ @dokuwiki;
    }
    location @dokuwiki {
        rewrite ^/_media/(.*) /lib/exe/fetch.php?media=$1 last;
        rewrite ^/_detail/(.*) /lib/exe/detail.php?media=$1 last;
        rewrite ^/_export/([^/]+)/(.*) /doku.php?do=export_$1&amp;id=$2 last;
        rewrite ^/(.*) /doku.php?id=$1 last;
    }
    location ~ \.php$ {
        try_files $uri =404;
        fastcgi_pass unix:/var/run/php5-fpm.sock;
        fastcgi_index index.php;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
        include fastcgi_params;
    }
    location ~ /\.ht {
        deny all;
    }
    location ~ /(data|conf|bin|inc)/ {
        deny all;
    }
}
</code></pre>

<h4 id="nginx">Nginx</h4>

<p>Nginx install is through apt as well. Here, however, there is a bit of magic going on with templates. The templates provide the
vhost files for the three hosts we will be running. This looks as follows:</p>

<pre><code class="language-yaml">- name: Install vhosts
  block:
    - template: src=01_example.com.j2 dest=/etc/nginx/vhosts/01_example.com
      notify:
      - Restart Nginx
    - template: src=02_wiki.example.com.j2 dest=/etc/nginx/vhosts/02_wiki_example.com
      notify:
      - Restart Nginx
    - template: src=03_nagios.example.com.j2 dest=/etc/nginx/vhosts/03_nagios.example.com
      notify:
      - Restart Nginx
</code></pre>

<p>Now, you might be wondering what <code>notify</code> is? It&rsquo;s basically a handler that gets notified to restart nginx. The great part about
it is that it does this only once, even if it was called multiple times. The handler looks like this:</p>

<pre><code class="language-yaml">- name: Restart Nginx
  service:
    name: nginx
    state: restarted
</code></pre>

<p>And lives under <code>handlers</code> sub-folder.</p>

<p>With this, Nginx is done and should be providing our sites under plain HTTP.</p>

<h4 id="letsencrypt">LetsEncrypt</h4>

<p>Now comes the part where we enable HTTPS for all these three domains. Which is as follows:</p>

<ul>
<li>example.com</li>
<li>wiki.example.com</li>
<li>nagios.example.com</li>
</ul>

<p>This is actually quiet simple now-a-days with <code>certbot-auto</code>. In fact, it will insert the configurations we need all by itself.
The only thing for us to do is to specify what domains we have and what our challenge would be. Also, we have to pass in some
variables for <code>certbot-auto</code> to run in a non-interactive mode. This looks as follows:</p>

<pre><code class="language-yaml">- name: Generate Certificate for Domains
  shell: ./certbot-auto --authenticator standalone --installer nginx -d '{{ domain_example }}' -d '{{ domain_wiki }}' -d '{{ domain_nagios }}' --email example@gmail.com --agree-tos -n --no-verify-ssl --pre-hook &quot;sudo systemctl stop nginx&quot; --post-hook &quot;sudo systemctl start nginx&quot; --redirect
  args:
    chdir: /opt/letsencrypt
</code></pre>

<p>And that&rsquo;s that. The interesting and required part here is the <code>pre-hook</code> and <code>post-hook</code>. Without those it wouldn&rsquo;t work because
the ports that certbot is performing the challenge on would be taken already. This stops nginx, performs the challenge and
generates the certs, and starts nginx again. Also note <code>--redirect</code>. This will force HTTPS on the sites and disables plain HTTP.</p>

<p>If all went well our sites should contain information like this:</p>

<pre><code class="language-bash">    listen 443 ssl; # managed by Certbot
    ssl_certificate /etc/letsencrypt/live/example.com-0001/fullchain.pem; # managed by Certbot
    ssl_certificate_key /etc/letsencrypt/live/example.com-0001/privkey.pem; # managed by Certbot
    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot
    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot
</code></pre>

<h3 id="test-run-using-vagrant">Test Run using Vagrant</h3>

<p>If you don&rsquo;t want to run all this on a live server to test out, you can do either of these two things:</p>

<ul>
<li>Use a remote dedicated test server</li>
<li>Use a local virtual machine with Vagrant</li>
</ul>

<p>Here, I&rsquo;m giving you an option for the later.</p>

<p>It&rsquo;s possible for most of the things to be tested on a local Vagrant machine. Most of the time a Vagrant box is enough to test out installing things. A sample Vagrant file looks like this:</p>

<pre><code class="language-ruby"># encoding: utf-8
# -*- mode: ruby -*-
# vi: set ft=ruby :
# Box / OS
VAGRANT_BOX = 'ubuntu/xenial64'

VM_NAME = 'ansible-practice'

Vagrant.configure(2) do |config|
  # Vagrant box from Hashicorp
  config.vm.box = VAGRANT_BOX
  # Actual machine name
  config.vm.hostname = VM_NAME
  # Set VM name in Virtualbox
  config.vm.provider 'virtualbox' do |v|
    v.name = VM_NAME
    v.memory = 2048
  end
  # Ansible provision
  config.vm.provision 'ansible_local' do |ansible|
    ansible.limit = 'all'
    ansible.inventory_path = 'hosts'
    ansible.playbook = 'local.yml'
  end
end
</code></pre>

<p>This interesting part here is the ansible provision section. It&rsquo;s running a version of Ansible that is called <code>ansible_local</code>. It&rsquo;s local, becuase it will be only on the VirtualBox. Meaning, you don&rsquo;t have to have Ansible installed to test it on a vagrant box. Neat, huh?</p>

<p>To test your playbook, simply run <code>vagrant up</code> and you should see the provisioning happening.</p>

<h2 id="room-for-improvement">Room for improvement</h2>

<p>And that should be all. Note that this setup isn&rsquo;t quiet enterprise ready. I would add the following things:</p>

<h3 id="tests-and-checks">Tests and Checks</h3>

<p>A ton of tests and checks if the commands that we are using are actually successful or not. If they aren&rsquo;t make them report the failure.</p>

<h3 id="multiple-domains">Multiple Domains</h3>

<p>If you happen to have a ton of domain names to set up, this will not be the most effective way. Right now letsencrypt creates a
single certificate file for those three domains with <code>-d</code> and that&rsquo;s not what you want with potentially hundreds of domains.</p>

<p>In that case, have a list to go through with <code>with_items</code>. Note that you&rsquo;ll have to restart nginx on each line, because you don&rsquo;t
want one of them fail and stop the process entirely. Rather have a few fail but the rest still work.</p>

<h1 id="conclusion">Conclusion</h1>

<p>That&rsquo;s it folks. Have fun setting up servers all over the place and enjoy the power of nginx and letsencrypt and not having to
worry about adding another server into the bunch.</p>

<p>Thank you for reading,
Gergely.</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">23 Jan 2018, 22:34</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://skarlso.github.io/2018/01/23/nginx-certbot-ansible/" class="post-title">Ansible &#43; Nginx &#43; LetsEncrypt &#43; Wiki &#43; Nagios</a>

                        <p class="post-meta">
                            
                                By <strong class="post-author">hannibal</strong>
                            
                            
                                under 
                                
                                <a class="post-category post-category-Ansible" href="https://skarlso.github.io//categories/ansible">Ansible</a><a class="post-category post-category-Nginx" href="https://skarlso.github.io//categories/nginx">Nginx</a><a class="post-category post-category-Certbot" href="https://skarlso.github.io//categories/certbot">Certbot</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        

<h1 id="intro">Intro</h1>

<p>Hi folks.</p>

<p>Today, I would like demonstrate how to use <a href="https://www.ansible.com/">Ansible</a> in order to construct a server hosting multiple HTTPS domains with <a href="https://www.nginx.com/">Nginx</a> and <a href="https://letsencrypt.org/">LetsEncrypt</a>. Are you ready? Let&rsquo;s dive in.</p>

<h2 id="tl-dr">TL;DR</h2>

<p><img src="https://skarlso.github.io/img/ansible.svg" alt="playbook" /></p>

<h2 id="what-you-will-need">What you will need</h2>

<p>There is really only one thing you need in order for this to work and that is Ansible. If you would like to run local tests without a remote server, than you will need <a href="https://www.vagrantup.com/">Vagrant</a> and <a href="https://www.virtualbox.org/wiki/Downloads">VirtualBox</a>. But those two are optional.</p>

<h2 id="what-we-are-going-to-set-up">What We Are Going To Set Up</h2>

<p>The setup is as follows:</p>

<h3 id="nagios">Nagios</h3>

<p>We are going to have a Nagios with a custom check for pending security updates. That will run under nagios.example.com.</p>

<h3 id="hugo-website">Hugo Website</h3>

<p>The main web site is going to be a basic <a href="https://gohugo.io/">Hugo</a> site. Hugo is a static Go based web site generator. This Blog is run by it.</p>

<p>We are also going to setup <a href="https://www.noip.com/">NoIP</a> which will provide the DNS for the sites.</p>

<h3 id="wiki">Wiki</h3>

<p>The wiki is a plain, basic <a href="https://www.dokuwiki.org/dokuwiki#">DokuWiki</a>.</p>

<h3 id="https-nginx">HTTPS + Nginx</h3>

<p>And all the above will be hosted by Nginx with HTTPS provided by letsencrypt. We are going to set all these up with Ansible on top so it will be idempotent.</p>

<h3 id="repository">Repository</h3>

<p>All of the playbooks and the whole thing together can be viewed here: <a href="https://github.com/Skarlso/ansible-server-setup">Github Ansible Server Setup</a>.</p>

<h2 id="ansible">Ansible</h2>

<p>I won&rsquo;t be writing everything down to the basics about Ansible. For that you will need to go and read its documentation. But I will provide ample of clarification for using what I&rsquo;ll be using.</p>

<h3 id="some-basics">Some Basics</h3>

<p>Ansible is a configuration management tool which, unlike chef or puppet, isn&rsquo;t master - slave based. It&rsquo;s using SSH to run a set of instructions on a target machine. The instructions are written in yaml files and look something like this:</p>

<pre><code class="language-yaml">---
# tasks file for ssh
- name: Copy sshd_config
  copy: content=&quot;{{sshd_config}}&quot; dest=/etc/ssh/sshd_config
  notify:
  - SSHD Restart
</code></pre>

<p>This is a basic Task which copies over an <code>sshd_config</code> file overwriting the one already being there. It can execute in priviliged mode if root password is provided or the user has sudo rights.</p>

<p>It works from so called <code>hosts</code> files where the server details are described. This is how a basic host file would look like:</p>

<pre><code class="language-bash">[local]
127.0.0.1

[webserver1]
1.23.4.5
</code></pre>

<p>Ansible will use these settings to try and access the server. To test if the connection is working, you can send a <code>ping</code> task like this:</p>

<pre><code class="language-bash">ansible all -m ping
</code></pre>

<p>Ansible uses <code>variables</code> for things that change. They are defined under each task&rsquo;s subfolder called <code>vars</code>. Please feel free to change the varialbes there to your liking.</p>

<h3 id="ssh-access">SSH Access</h3>

<p>You can either define SSH information per host or per group or globally. In this example I have it under the groups wars called
<code>webserver1</code> like this (vars.yaml):</p>

<pre><code class="language-yaml">---
# SSH sudo keys and pass
ansible_become_pass: '{{vault_ansible_become_pass}}'
ansible_ssh_port: '{{vault_ansible_ssh_port}}'
ansible_ssh_user: '{{vault_ansible_ssh_user}}'
ansible_ssh_private_key_file: '{{vault_ansible_ssh_private_key_file}}'
home_dir: /root
</code></pre>

<h3 id="further-reading">Further reading</h3>

<p>Further readings are:</p>

<ul>
<li><a href="https://serversforhackers.com/c/an-ansible-tutorial">Servers For Hackers</a></li>
<li><a href="http://docs.ansible.com/ansible/latest/intro_getting_started.html">Ansible docs</a></li>
</ul>

<h3 id="vault">Vault</h3>

<p>The vault is the place where we can keep secure information. This file is called <code>vault</code> and usually lives under either <code>group_vars</code> or <code>host_vars</code>. The preference is up to you.</p>

<p>This file is encrypted using a password you specify. You can have the vault password stored in the following ways:</p>

<ul>
<li>Store it on a secure drive which is encrypted and only mounted when the playbook is executed</li>
<li>Store it on <a href="https://keybase.io">Keybase</a></li>
<li>Store it on an encrypted S3 bucket</li>
<li>Store it in a file next to the playbook which is never commited into source control</li>
</ul>

<p>Either way, in the end, ansible will look for a file called <code>.vault_password</code> for when it&rsquo;s trying to decrypt the file. You can
define a different file in the <code>ansible.cfg</code> file using the <code>vault_password_file</code> option.</p>

<p>You can create a vault like this:</p>

<pre><code class="language-bash">ansible-vault create vault
</code></pre>

<p>If you are following along, you are going to need these variables in the vault:</p>

<pre><code class="language-yaml">vault_ansible_become_pass: &lt;your_sudo_password&gt; # if applicable
vault_ansible_ssh_user: &lt;ssh_user&gt;
vault_ansible_ssh_private_key_file: /Users/user/.ssh/ida_rsa
vault_nagios_password: supersecurenagiosadminpassword
vault_nagios_username: nagiosadmin
vault_noip_username: youruser@gmail.com
vault_noip_password: &quot;SuperSecureNoIPPassword&quot;
vault_nginx_user: &lt;localuser&gt;
</code></pre>

<p>You can always edit the vault later on with:</p>

<pre><code class="language-bash">ansible-vault edit group_vars/webserver1/vault --vault-password-file=.vault_pass
</code></pre>

<h3 id="tasks">Tasks</h3>

<p>The following are a collection of tasks which execute in order. The end task, which is letsencrypt, relies on all the hosts being present and configured under Nginx. Otherwise it will throw an error that the host you are trying to configure HTTPS for, isn&rsquo;t defined.</p>

<h4 id="no-ip">No-IP</h4>

<p>I&rsquo;m choosing No-ip as a DNS provider because it&rsquo;s cheap and the sync tool is easy to automate. To automate the CLI of No-IP, I&rsquo;m using a package called <code>expect</code>. This looks something like this:</p>

<pre><code class="language-bash">cd {{home_dir}}
wget http://www.no-ip.com/client/linux/noip-duc-linux.tar.gz
mkdir -p noip
tar zxf noip-duc-linux.tar.gz -C noip
cd noip/*
make

/usr/bin/expect &lt;&lt;END_SCRIPT
spawn make install
expect &quot;Please enter the login/email*&quot; { send &quot;{{noip_username}}\r&quot; }
expect &quot;Please enter the password for user*&quot; { send &quot;{{noip_password}}\r&quot; }
expect {
    &quot;Do you wish to have them all updated*&quot; {
        send &quot;y&quot;
        exp_continue
    }
}
expect &quot;Please enter an update interval*&quot; { send &quot;30\r&quot; }
expect &quot;Do you wish to run something at successful update*&quot; {send &quot;N&quot; }
END_SCRIPT
</code></pre>

<p>The interesting part is the command running expect. Basically, it&rsquo;s expecting some kind of output which is outlined there. And has canned answers for those which it <code>send</code>s to the waiting command.</p>

<h4 id="to-util-or-not-to-util">To Util or Not To Util</h4>

<p>So, there are small tasks, like installing vim and wget and such which could warrant the existance of a <code>utils</code> task. Utils task would install the packages that are used as convinience and don&rsquo;t really relate to a singe task.</p>

<p>Yet I settled for the following. Each of my tasks has a dependency part. The given tasks takes care of all the packages it needs so they can be executed on their own as well as in unison.</p>

<p>This looks like this:</p>

<pre><code class="language-yaml"># Install dependencies
- name: Install dependencies
  apt: pkg=&quot;{{item}}&quot; state=installed
  with_items:
    - &quot;{{deps}}&quot;
</code></pre>

<p>For which the <code>deps</code> variable is defined as follows:</p>

<pre><code class="language-yaml"># Defined dependencies for letsencrypt task.
deps: ['git', 'python-dev', 'build-essential', 'libpython-dev', 'libpython2.7', 'augeas-lenses', 'libaugeas0', 'libffi-dev', 'libssl-dev', 'python-virtualenv', 'python3-virtualenv', 'virtualenv']
</code></pre>

<p>This is much cleaner. And if a task is no longer needed, it&rsquo;s dependencies will no longer be needed either in most of the cases.</p>

<h4 id="nagios-1">Nagios</h4>

<p>I&rsquo;m using Nagios 4 which is a real pain in the butt to install. Luckily, thanks to Ansiblei, I only ever had to figure it out once. Now I have a script for that. Installing Nagios demands several, smaller components to be installed. Thus our task uses import from outside tasks like this:</p>

<pre><code class="language-yaml">- name: Install Nagios
  block:
    - include: create_users.yml # creates the Nagios user
    - include: install_dependencies.yml # installs Nagios dependencies
    - include: core_install.yml # Installs Nagios Core
    - include: plugin_install.yml # Installs Nagios Plugins
    - include: create_htpasswd.yml # Creates a password for Nagios' admin user
    - include: setup_custom_check.yml # Adds a custom check which is to check how many security updates are pending
  when: st.stat.exists == False
</code></pre>

<p>The <code>when</code> is a check for a variable created by a file check.</p>

<pre><code class="language-yaml">- stat:
    path: /usr/local/nagios/bin/nagios
  register: st
</code></pre>

<p>It checks if Nagios is installed or not. If yes, skip.</p>

<p>I&rsquo;m not going to paste in here all the subtasks because that would be huge. You can check those out in the repository under Nagios.</p>

<h4 id="hugo">Hugo</h4>

<p>Hugo is easy to install. Its sole requirement is Go. To install hugo you simply run <code>apt-get install hugo</code>. Setting up the
site for me was just checking out the git repo and than execute hugo from the root folder like this:</p>

<pre><code class="language-bash">hugo server --bind=127.0.0.1 --port=8080 --baseUrl=https://example.com --appendPort=false --logFile hugo.log --verboseLog --verbose -v &amp;
</code></pre>

<h4 id="wiki-1">Wiki</h4>

<p>I used DokuWiki because it&rsquo;s a file based wiki so installation is basically just downloading the archive, extracting it and done. The only thing that&rsquo;s needed for it, is php-fpm to run it and a few php modules which I&rsquo;ll outline in the ansible playbook.</p>

<p>The VHOST file for DokuWiki is provided by them and looks like this:</p>

<pre><code class="language-bash">server {
    server_name   {{ wiki_server_name }};
    root {{ wiki_root }};
    index index.php index.html index.htm;
    client_max_body_size 2M;
    client_body_buffer_size 128k;
    location / {
        index doku.php;
        try_files $uri $uri/ @dokuwiki;
    }
    location @dokuwiki {
        rewrite ^/_media/(.*) /lib/exe/fetch.php?media=$1 last;
        rewrite ^/_detail/(.*) /lib/exe/detail.php?media=$1 last;
        rewrite ^/_export/([^/]+)/(.*) /doku.php?do=export_$1&amp;id=$2 last;
        rewrite ^/(.*) /doku.php?id=$1 last;
    }
    location ~ \.php$ {
        try_files $uri =404;
        fastcgi_pass unix:/var/run/php5-fpm.sock;
        fastcgi_index index.php;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
        include fastcgi_params;
    }
    location ~ /\.ht {
        deny all;
    }
    location ~ /(data|conf|bin|inc)/ {
        deny all;
    }
}
</code></pre>

<h4 id="nginx">Nginx</h4>

<p>Nginx install is through apt as well. Here, however, there is a bit of magic going on with templates. The templates provide the
vhost files for the three hosts we will be running. This looks as follows:</p>

<pre><code class="language-yaml">- name: Install vhosts
  block:
    - template: src=01_example.com.j2 dest=/etc/nginx/vhosts/01_example.com
      notify:
      - Restart Nginx
    - template: src=02_wiki.example.com.j2 dest=/etc/nginx/vhosts/02_wiki_example.com
      notify:
      - Restart Nginx
    - template: src=03_nagios.example.com.j2 dest=/etc/nginx/vhosts/03_nagios.example.com
      notify:
      - Restart Nginx
</code></pre>

<p>Now, you might be wondering what <code>notify</code> is? It&rsquo;s basically a handler that gets notified to restart nginx. The great part about
it is that it does this only once, even if it was called multiple times. The handler looks like this:</p>

<pre><code class="language-yaml">- name: Restart Nginx
  service:
    name: nginx
    state: restarted
</code></pre>

<p>And lives under <code>handlers</code> sub-folder.</p>

<p>With this, Nginx is done and should be providing our sites under plain HTTP.</p>

<h4 id="letsencrypt">LetsEncrypt</h4>

<p>Now comes the part where we enable HTTPS for all these three domains. Which is as follows:</p>

<ul>
<li>example.com</li>
<li>wiki.example.com</li>
<li>nagios.example.com</li>
</ul>

<p>This is actually quiet simple now-a-days with <code>certbot-auto</code>. In fact, it will insert the configurations we need all by itself.
The only thing for us to do is to specify what domains we have and what our challenge would be. Also, we have to pass in some
variables for <code>certbot-auto</code> to run in a non-interactive mode. This looks as follows:</p>

<pre><code class="language-yaml">- name: Generate Certificate for Domains
  shell: ./certbot-auto --authenticator standalone --installer nginx -d '{{ domain_example }}' -d '{{ domain_wiki }}' -d '{{ domain_nagios }}' --email example@gmail.com --agree-tos -n --no-verify-ssl --pre-hook &quot;sudo systemctl stop nginx&quot; --post-hook &quot;sudo systemctl start nginx&quot; --redirect
  args:
    chdir: /opt/letsencrypt
</code></pre>

<p>And that&rsquo;s that. The interesting and required part here is the <code>pre-hook</code> and <code>post-hook</code>. Without those it wouldn&rsquo;t work because
the ports that certbot is performing the challenge on would be taken already. This stops nginx, performs the challenge and
generates the certs, and starts nginx again. Also note <code>--redirect</code>. This will force HTTPS on the sites and disables plain HTTP.</p>

<p>If all went well our sites should contain information like this:</p>

<pre><code class="language-bash">    listen 443 ssl; # managed by Certbot
    ssl_certificate /etc/letsencrypt/live/example.com-0001/fullchain.pem; # managed by Certbot
    ssl_certificate_key /etc/letsencrypt/live/example.com-0001/privkey.pem; # managed by Certbot
    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot
    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot
</code></pre>

<h3 id="test-run-using-vagrant">Test Run using Vagrant</h3>

<p>If you don&rsquo;t want to run all this on a live server to test out, you can do either of these two things:</p>

<ul>
<li>Use a remote dedicated test server</li>
<li>Use a local virtual machine with Vagrant</li>
</ul>

<p>Here, I&rsquo;m giving you an option for the later.</p>

<p>It&rsquo;s possible for most of the things to be tested on a local Vagrant machine. Most of the time a Vagrant box is enough to test out installing things. A sample Vagrant file looks like this:</p>

<pre><code class="language-ruby"># encoding: utf-8
# -*- mode: ruby -*-
# vi: set ft=ruby :
# Box / OS
VAGRANT_BOX = 'ubuntu/xenial64'

VM_NAME = 'ansible-practice'

Vagrant.configure(2) do |config|
  # Vagrant box from Hashicorp
  config.vm.box = VAGRANT_BOX
  # Actual machine name
  config.vm.hostname = VM_NAME
  # Set VM name in Virtualbox
  config.vm.provider 'virtualbox' do |v|
    v.name = VM_NAME
    v.memory = 2048
  end
  # Ansible provision
  config.vm.provision 'ansible_local' do |ansible|
    ansible.limit = 'all'
    ansible.inventory_path = 'hosts'
    ansible.playbook = 'local.yml'
  end
end
</code></pre>

<p>This interesting part here is the ansible provision section. It&rsquo;s running a version of Ansible that is called <code>ansible_local</code>. It&rsquo;s local, becuase it will be only on the VirtualBox. Meaning, you don&rsquo;t have to have Ansible installed to test it on a vagrant box. Neat, huh?</p>

<p>To test your playbook, simply run <code>vagrant up</code> and you should see the provisioning happening.</p>

<h2 id="room-for-improvement">Room for improvement</h2>

<p>And that should be all. Note that this setup isn&rsquo;t quiet enterprise ready. I would add the following things:</p>

<h3 id="tests-and-checks">Tests and Checks</h3>

<p>A ton of tests and checks if the commands that we are using are actually successful or not. If they aren&rsquo;t make them report the failure.</p>

<h3 id="multiple-domains">Multiple Domains</h3>

<p>If you happen to have a ton of domain names to set up, this will not be the most effective way. Right now letsencrypt creates a
single certificate file for those three domains with <code>-d</code> and that&rsquo;s not what you want with potentially hundreds of domains.</p>

<p>In that case, have a list to go through with <code>with_items</code>. Note that you&rsquo;ll have to restart nginx on each line, because you don&rsquo;t
want one of them fail and stop the process entirely. Rather have a few fail but the rest still work.</p>

<h1 id="conclusion">Conclusion</h1>

<p>That&rsquo;s it folks. Have fun setting up servers all over the place and enjoy the power of nginx and letsencrypt and not having to
worry about adding another server into the bunch.</p>

<p>Thank you for reading,
Gergely.</p>

                    </div>
                </section>
                
            </div>
            
<div class="pagination">
  <nav role="pagination" class="post-list-pagination">
      
    <span class="post-list-pagination-item post-list-pagination-item-current">Page 1 of 25</span>
    
      <a href="https://skarlso.github.io/page/2/" class="post-list-pagination-item pure-button post-list-pagination-item-next">
        Older&nbsp;<i class="fa fa-angle-double-right"></i>
      </a>
    
  </nav>
</div>


            <div class="footer">
    <div class="pure-menu pure-menu-horizontal pure-menu-open">
        <ul>
            <li>Powered by <a class="hugo" href="https://gohugo.io/" target="_blank">hugo</a></li>
        </ul>
    </div>
</div>
<script src="https://skarlso.github.io//js/all.min.js"></script>

        </div>
    </div>
</div>


<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', '', 'auto');
ga('send', 'pageview');

</script>

</body>
</html>
